[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probabilistic dynamic semantics",
    "section": "",
    "text": "Outline\nThe recent advent of linguistic datasets and their associated statistical models have given rise to two major kinds of questions bearing on linguistic theory and methodology:\n\nHow can semanticists use such datasets; i.e., how can the statistical properties of a dataset inform semantic theory directly, and what guiding principles regulate the link between such properties and semantic theory?\nHow should semantic theories themselves be modified so that they may characterize not only informally collected acceptability and inference judgments, but statistical generalizations observed from datasets?\n\nThis course brings the compositional, algebraic view of meaning employed by semanticists into contact with linguistic datasets by introducing and applying the framework of Probabilistic Dynamic Semantics (Grove and White 2024a, 2025, 2024b). PDS seamlessly integrates theories of semantic competence with accounts of linguistic behavior in experimental settings by taking a modular approach: given a dataset involving some semantic phenomenon, and which exhibits certain statistical properties, this course offers an approach to developing both (a) theories of the meanings assigned to the expressions present in the dataset, and (b) linking hypotheses that directly relate these theories to linguistic behavior.\n\n\nExisting probabilistic approaches to meaning\nThe ideas developed in this course build on and respond to existing probabilistic approaches to semantics and pragmatics, including those which use computational modeling to characterize inference (Zeevat and Schmitz 2015; Franke and Jäger 2016; Brasoveanu and Dotlačil 2020; Bernardy et al. 2022; Goodman et al. n.d.). Such models are motivated, in part, by the observation that linguistic inference tends to display substantial gradience, giving rise to quantitative patterns that traditional semantic theory has difficulty capturing. Meanwhile, they often aim to explain Gricean linguistic behavior (Grice 1975) by regarding humans as Bayesian reasoners. Indeed, due to this emphasis on pragmatic principles, much modeling work blurs the semantics/pragmatics distinction, rendering the connection to traditional semantic theory somewhat opaque.\nTo take a paradigm case, models within the Rational Speech Act (RSA) framework consider human interpreters as inferring meanings for an utterance which maximize the utterance’s utility relative to a set of possible alternative utterances (Frank and Goodman 2012; Lassiter 2011; Goodman and Stuhlmüller 2013; Goodman and Frank 2016; Lassiter and Goodman 2017; Degen 2023). Probabilistic models of linguistic inference, including RSA, tend to encode Bayesian principles of probabilistic update in terms of Bayes’ theorem, which states that the posterior probability of an event given an observation is proportional to the prior probability of the event, multiplied by the likelihood of the observation given the event. RSA models give an explicit operational interpretation to Bayes’ theorem by assuming that prior distributions over inferences encode world knowledge, and that likelihoods represent the utility-maximizing behavior of a pragmatic speaker.\nDespite their success in modeling a wide variety of semantic and pragmatic phenomena, probabilistic models of linguistic data remain largely divorced from semantic and pragmatic practice, both in theory and in implementation. RSA models, for example, regard the semantic interpretations which humans pragmatically reason about as being provided by a literal listener that determines a distribution over inferences, given an utterance (Degen 2023). But aside from the constraint that the literal listener’s posterior distribution is proportional to its prior distribution (i.e., it acts as a mere filter), the semantic components of RSA models are generally designed by researchers on an ad hoc basis: on the one hand, the space (I) of possible inferences must be decided by individual researchers in a way that depends on the task being modeled; on the other hand, the relation (⟦·⟧) between utterances and inferences is typically assumed without a justified connection to any semantic theory using, e.g., an explicit grammar fragment in the style of .\n\n\nPDS as a bridge between probabilistic models and semantic theory\nGiven this background, this course introduces students to a novel approach to probabilistic meaning which integrates traditional Montague semantics, as well as ideas in compositional dynamic semantics, with probabilistic computational models in a completely seamless fashion. The theoretical framework and methodology we introduce retain the beneficial features of both kinds of approach to meaning: PDS may be used to construct probabilistic models of human inference data, and it is in principle compatible with existing probabilistic modeling paradigms such as RSA; meanwhile, it seamlessly connects probabilistic models to compositional dynamic semantics in the Montagovian tradition by providing a setting to write full-fledged grammar fragments.\nPDS additionally provides a theory of dynamic discourse update, integrating aspects of discourse such as the common ground, the question under discussion (Ginzburg 1996; Roberts 2012; Farkas and Bruce 2010), and uncertainty about lexical meaning. Crucially, given a semantic theory of some discourse phenomenon couched with PDS, one may obtain a probabilistic model of some linguistic dataset, given a particular response function (Grove and White 2024a, 2025, 2024b). We introduce PDS in the context empirical datasets studying factivity, vagueness, and the QUD.\n\n\n\n\n\n\n\nReferences\n\nBernardy, Jean-Philippe, Rasmus Blanck, Stergios Chatzikyriakidis, and Aleksandre Maskharashvili. 2022. “Bayesian Natural Language Semantics and Pragmatics.” In Probabilistic Approaches to Linguistic Theory, edited by Jean-Philippe Bernardy, Rasmus Blanck, Stergios Chatzikyriakidis, Shalom Lappin, and Aleksandre Maskharashvili. CSLI Publications.\n\n\nBrasoveanu, Adrian, and Jakub Dotlačil. 2020. Computational Cognitive Modeling and Linguistic Theory. Vol. 6. Language, Cognition, and Mind. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-31846-8.\n\n\nDegen, Judith. 2023. “The Rational Speech Act Framework.” Annual Review of Linguistics 9 (1): 519–40. https://doi.org/10.1037/rev0000186.\n\n\nFarkas, Donka F., and Kim B. Bruce. 2010. “On Reacting to Assertions and Polar Questions.” Journal of Semantics 27 (1): 81–118. https://doi.org/10.1093/jos/ffp010.\n\n\nFrank, Michael C., and Noah D. Goodman. 2012. “Predicting Pragmatic Reasoning in Language Games.” Science 336 (6084): 998–98. https://doi.org/10.1126/science.1218633.\n\n\nFranke, Michael, and Gerhard Jäger. 2016. “Probabilistic Pragmatics, or Why Bayes’ Rule Is Probably Important for Pragmatics.” Zeitschrift Für Sprachwissenschaft 35 (1): 3–44. https://doi.org/10.1515/zfs-2016-0002.\n\n\nGinzburg, Jonathan. 1996. “Dynamics and the Semantics of Dialogue.” In Logic, Language, and Computation, edited by Jerry Seligman and Dag Westerståhl, 1:221–37. Stanford: CSLI Publications.\n\n\nGoodman, Noah D., and Michael C. Frank. 2016. “Pragmatic Language Interpretation as Probabilistic Inference.” Trends in Cognitive Sciences 20 (11): 818–29. https://doi.org/10.1016/j.tics.2016.08.005.\n\n\nGoodman, Noah D., and Andreas Stuhlmüller. 2013. “Knowledge and Implicature: Modeling Language Understanding as Social Cognition.” Topics in Cognitive Science 5 (1): 173–84. https://doi.org/10.1111/tops.12007.\n\n\nGoodman, Noah D., Joshua B. Tenenbaum, Daphna Buchsbaum, Joshua Hartshorne, Robert Hawkins, Timothy O’Donnell, and Michael Henry Tessler. n.d. “Probabilistic Models of Cognition - 2nd Edition.” Probabilistic Models of Cognition. Accessed June 30, 2024. http://probmods.org/.\n\n\nGrice, H. Paul. 1975. “Logic and Conversation.” In Syntax and Semantics, edited by Peter Cole and Jerry L. Morgan, 3, Speech Acts:41–58. New York: Academic Press.\n\n\nGrove, Julian, and Aaron Steven White. 2024a. “Factivity, Presupposition Projection, and the Role of Discrete Knowlege in Gradient Inference Judgments.” LingBuzz. https://ling.auf.net/lingbuzz/007450.\n\n\n———. 2024b. “Probabilistic Dynamic Semantics.” University of Rochester. https://ling.auf.net/lingbuzz/008478.\n\n\n———. 2025. “Modeling the Prompt in Inference Judgment Tasks.” Experiments in Linguistic Meaning 3 (January): 176–87. https://doi.org/10.3765/elm.3.5857.\n\n\nLassiter, Daniel. 2011. “Vagueness as Probabilistic Linguistic Knowledge.” In Vagueness in Communication, edited by Rick Nouwen, Robert van Rooij, Uli Sauerland, and Hans-Christian Schmitz, 127–50. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer. https://doi.org/10.1007/978-3-642-18446-8_8.\n\n\nLassiter, Daniel, and Noah D. Goodman. 2017. “Adjectival Vagueness in a Bayesian Model of Interpretation.” Synthese 194 (10): 3801–36. https://doi.org/10.1007/s11229-015-0786-1.\n\n\nRoberts, Craige. 2012. “Information Structure: Towards an Integrated Formal Theory of Pragmatics.” Semantics and Pragmatics 5 (December): 6:1–69. https://doi.org/10.3765/sp.5.6.\n\n\nZeevat, Henk, and Hans-Christian Schmitz, eds. 2015. Bayesian Natural Language Semantics and Pragmatics. Vol. 2. Language, Cognition, and Mind. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-17064-0.",
    "crumbs": [
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "background/background.html",
    "href": "background/background.html",
    "title": "Probabilistic dynamic semantics",
    "section": "",
    "text": "Compositionality: explains how lexical knowledge gives rise to\n\n\n\n\nInference judgments arise from multiple causal factors, only one of which is semantic knowledge.\nTraditional methodologies focus on small areas of the lexicon to support a given analysis (on the order of tens of examples), jeopardizing generalizations’ validity.\n\n\n\n\n\n\nMethodologies like inference judgment tasks provide a way of studying larger areas of the lexicon, potentially improving the validity of semantic generalizations.\n\n\n\n\nGenerally, unclear how modeling constructs relate to semantic theory; i.e., what lexical semantic property is implicated and how it interacts with language-external factors.\nIn the setting of entire datasets, this question becomes one about gradience: what theoretical constructs underlie the relevant distributions of judgments?-\n\nThus little progress on the first challenge for the traditional view.\n\n\n\n\n\n\n\nAims to distinguish semantic and pragmatic causes of inference behavior (i.e., L0 vs. L1/S1).\n\n\n\n\nUnclear what role compositionality can play (no theory of L0).",
    "crumbs": [
      "Background",
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "pds-intro/pds-intro.html",
    "href": "pds-intro/pds-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Test.",
    "crumbs": [
      "Introduction to probabilistic dynamic semantics",
      "Introduction"
    ]
  },
  {
    "objectID": "qud/qud.html",
    "href": "qud/qud.html",
    "title": "Probabilistic dynamic semantics",
    "section": "",
    "text": "Outline\nThe recent advent of linguistic datasets and their associated statistical models have given rise to two major kinds of questions bearing on linguistic theory and methodology:\n\nHow can semanticists use such datasets; i.e., how can the statistical properties of a dataset inform semantic theory directly, and what guiding principles regulate the link between such properties and semantic theory?\nHow should semantic theories themselves be modified so that they may characterize not only informally collected acceptability and inference judgments, but statistical generalizations observed from datasets?\n\nThis course brings the compositional, algebraic view of meaning employed by semanticists into contact with linguistic datasets by introducing and applying the framework of Probabilistic Dynamic Semantics (Grove and White 2024a, 2025, 2024b). PDS seamlessly integrates theories of semantic competence with accounts of linguistic behavior in experimental settings by taking a modular approach: given a dataset involving some semantic phenomenon, and which exhibits certain statistical properties, this course offers an approach to developing both (a) theories of the meanings assigned to the expressions present in the dataset, and (b) linking hypotheses that directly relate these theories to linguistic behavior.\n\n\nExisting probabilistic approaches to meaning\nThe ideas developed in this course build on and respond to existing probabilistic approaches to semantics and pragmatics, including those which use computational modeling to characterize inference (Zeevat and Schmitz 2015; Franke and Jäger 2016; Brasoveanu and Dotlačil 2020; Bernardy et al. 2022; Goodman et al. n.d.). Such models are motivated, in part, by the observation that linguistic inference tends to display substantial gradience, giving rise to quantitative patterns that traditional semantic theory has difficulty capturing. Meanwhile, they often aim to explain Gricean linguistic behavior (Grice 1975) by regarding humans as Bayesian reasoners. Indeed, due to this emphasis on pragmatic principles, much modeling work blurs the semantics/pragmatics distinction, rendering the connection to traditional semantic theory somewhat opaque.\nTo take a paradigm case, models within the Rational Speech Act (RSA) framework consider human interpreters as inferring meanings for an utterance which maximize the utterance’s utility relative to a set of possible alternative utterances (Frank and Goodman 2012; Lassiter 2011; Goodman and Stuhlmüller 2013; Goodman and Frank 2016; Lassiter and Goodman 2017; Degen 2023). Probabilistic models of linguistic inference, including RSA, tend to encode Bayesian principles of probabilistic update in terms of Bayes’ theorem, which states that the posterior probability of an event given an observation is proportional to the prior probability of the event, multiplied by the likelihood of the observation given the event. RSA models give an explicit operational interpretation to Bayes’ theorem by assuming that prior distributions over inferences encode world knowledge, and that likelihoods represent the utility-maximizing behavior of a pragmatic speaker.\nDespite their success in modeling a wide variety of semantic and pragmatic phenomena, probabilistic models of linguistic data remain largely divorced from semantic and pragmatic practice, both in theory and in implementation. RSA models, for example, regard the semantic interpretations which humans pragmatically reason about as being provided by a literal listener that determines a distribution over inferences, given an utterance (Degen 2023). But aside from the constraint that the literal listener’s posterior distribution is proportional to its prior distribution (i.e., it acts as a mere filter), the semantic components of RSA models are generally designed by researchers on an ad hoc basis: on the one hand, the space (I) of possible inferences must be decided by individual researchers in a way that depends on the task being modeled; on the other hand, the relation (⟦·⟧) between utterances and inferences is typically assumed without a justified connection to any semantic theory using, e.g., an explicit grammar fragment in the style of .\n\n\nPDS as a bridge between probabilistic models and semantic theory\nGiven this background, this course introduces students to a novel approach to probabilistic meaning which integrates traditional Montague semantics, as well as ideas in compositional dynamic semantics, with probabilistic computational models in a completely seamless fashion. The theoretical framework and methodology we introduce retain the beneficial features of both kinds of approach to meaning: PDS may be used to construct probabilistic models of human inference data, and it is in principle compatible with existing probabilistic modeling paradigms such as RSA; meanwhile, it seamlessly connects probabilistic models to compositional dynamic semantics in the Montagovian tradition by providing a setting to write full-fledged grammar fragments.\nPDS additionally provides a theory of dynamic discourse update, integrating aspects of discourse such as the common ground, the question under discussion (Ginzburg 1996; Roberts 2012; Farkas and Bruce 2010), and uncertainty about lexical meaning. Crucially, given a semantic theory of some discourse phenomenon couched with PDS, one may obtain a probabilistic model of some linguistic dataset, given a particular response function (Grove and White 2024a, 2025, 2024b). We introduce PDS in the context empirical datasets studying factivity, vagueness, and the QUD.\n\n\n\n\n\n\n\nReferences\n\nBernardy, Jean-Philippe, Rasmus Blanck, Stergios Chatzikyriakidis, and Aleksandre Maskharashvili. 2022. “Bayesian Natural Language Semantics and Pragmatics.” In Probabilistic Approaches to Linguistic Theory, edited by Jean-Philippe Bernardy, Rasmus Blanck, Stergios Chatzikyriakidis, Shalom Lappin, and Aleksandre Maskharashvili. CSLI Publications.\n\n\nBrasoveanu, Adrian, and Jakub Dotlačil. 2020. Computational Cognitive Modeling and Linguistic Theory. Vol. 6. Language, Cognition, and Mind. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-31846-8.\n\n\nDegen, Judith. 2023. “The Rational Speech Act Framework.” Annual Review of Linguistics 9 (1): 519–40. https://doi.org/10.1037/rev0000186.\n\n\nFarkas, Donka F., and Kim B. Bruce. 2010. “On Reacting to Assertions and Polar Questions.” Journal of Semantics 27 (1): 81–118. https://doi.org/10.1093/jos/ffp010.\n\n\nFrank, Michael C., and Noah D. Goodman. 2012. “Predicting Pragmatic Reasoning in Language Games.” Science 336 (6084): 998–98. https://doi.org/10.1126/science.1218633.\n\n\nFranke, Michael, and Gerhard Jäger. 2016. “Probabilistic Pragmatics, or Why Bayes’ Rule Is Probably Important for Pragmatics.” Zeitschrift Für Sprachwissenschaft 35 (1): 3–44. https://doi.org/10.1515/zfs-2016-0002.\n\n\nGinzburg, Jonathan. 1996. “Dynamics and the Semantics of Dialogue.” In Logic, Language, and Computation, edited by Jerry Seligman and Dag Westerståhl, 1:221–37. Stanford: CSLI Publications.\n\n\nGoodman, Noah D., and Michael C. Frank. 2016. “Pragmatic Language Interpretation as Probabilistic Inference.” Trends in Cognitive Sciences 20 (11): 818–29. https://doi.org/10.1016/j.tics.2016.08.005.\n\n\nGoodman, Noah D., and Andreas Stuhlmüller. 2013. “Knowledge and Implicature: Modeling Language Understanding as Social Cognition.” Topics in Cognitive Science 5 (1): 173–84. https://doi.org/10.1111/tops.12007.\n\n\nGoodman, Noah D., Joshua B. Tenenbaum, Daphna Buchsbaum, Joshua Hartshorne, Robert Hawkins, Timothy O’Donnell, and Michael Henry Tessler. n.d. “Probabilistic Models of Cognition - 2nd Edition.” Probabilistic Models of Cognition. Accessed June 30, 2024. http://probmods.org/.\n\n\nGrice, H. Paul. 1975. “Logic and Conversation.” In Syntax and Semantics, edited by Peter Cole and Jerry L. Morgan, 3, Speech Acts:41–58. New York: Academic Press.\n\n\nGrove, Julian, and Aaron Steven White. 2024a. “Factivity, Presupposition Projection, and the Role of Discrete Knowlege in Gradient Inference Judgments.” LingBuzz. https://ling.auf.net/lingbuzz/007450.\n\n\n———. 2024b. “Probabilistic Dynamic Semantics.” University of Rochester. https://ling.auf.net/lingbuzz/008478.\n\n\n———. 2025. “Modeling the Prompt in Inference Judgment Tasks.” Experiments in Linguistic Meaning 3 (January): 176–87. https://doi.org/10.3765/elm.3.5857.\n\n\nLassiter, Daniel. 2011. “Vagueness as Probabilistic Linguistic Knowledge.” In Vagueness in Communication, edited by Rick Nouwen, Robert van Rooij, Uli Sauerland, and Hans-Christian Schmitz, 127–50. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer. https://doi.org/10.1007/978-3-642-18446-8_8.\n\n\nLassiter, Daniel, and Noah D. Goodman. 2017. “Adjectival Vagueness in a Bayesian Model of Interpretation.” Synthese 194 (10): 3801–36. https://doi.org/10.1007/s11229-015-0786-1.\n\n\nRoberts, Craige. 2012. “Information Structure: Towards an Integrated Formal Theory of Pragmatics.” Semantics and Pragmatics 5 (December): 6:1–69. https://doi.org/10.3765/sp.5.6.\n\n\nZeevat, Henk, and Hans-Christian Schmitz, eds. 2015. Bayesian Natural Language Semantics and Pragmatics. Vol. 2. Language, Cognition, and Mind. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-17064-0.",
    "crumbs": [
      "Modeling the question prompt",
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "index.html#challenges-to-the-classic-view",
    "href": "index.html#challenges-to-the-classic-view",
    "title": "Probabilistic dynamic semantics",
    "section": "",
    "text": "Inference judgments arise from multiple causal factors, only one of which is semantic knowledge.",
    "crumbs": [
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "index.html#challenges-to-the-traditional-view",
    "href": "index.html#challenges-to-the-traditional-view",
    "title": "Probabilistic dynamic semantics",
    "section": "",
    "text": "Inference judgments arise from multiple causal factors, only one of which is semantic knowledge.\nTraditional methodologies focus on small areas of the lexicon to support a given analysis (on the order of tens of examples), limiting empirical coverage.",
    "crumbs": [
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "index.html#the-traditional-view-of-meaning",
    "href": "index.html#the-traditional-view-of-meaning",
    "title": "Probabilistic dynamic semantics",
    "section": "The traditional view of meaning",
    "text": "The traditional view of meaning\n\nCompositionality: explains how lexical knowledge gives rise to\n\n\nChallenges\n\nInference judgments arise from multiple causal factors, only one of which is semantic knowledge.\nTraditional methodologies focus on small areas of the lexicon to support a given analysis (on the order of tens of examples), jeopardizing generalizations’ validity.",
    "crumbs": [
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "index.html#experimental-semantics-and-pragmatics",
    "href": "index.html#experimental-semantics-and-pragmatics",
    "title": "Probabilistic dynamic semantics",
    "section": "Experimental semantics and pragmatics",
    "text": "Experimental semantics and pragmatics\n\nMethodologies like inference judgment tasks provide a way of studying larger areas of the lexicon, potentially improving the validity of semantic generalizations.\n\n\nChallenges\n\nGenerally, unclear how modeling constructs relate to semantic theory; i.e., what lexical semantic property is implicated and how it interacts with language-external factors.\nIn the setting of entire datasets, this question becomes one about gradience: what theoretical constructs underlie the relevant distributions of judgments?-\n\nThus little progress on the first challenge for the traditional view.",
    "crumbs": [
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "index.html#a-theoretically-oriented-approach-rsa",
    "href": "index.html#a-theoretically-oriented-approach-rsa",
    "title": "Probabilistic dynamic semantics",
    "section": "A theoretically-oriented approach: RSA",
    "text": "A theoretically-oriented approach: RSA\n\nAims to distinguish semantic and pragmatic causes of inference behavior (i.e., L0 vs. L1/S1).\n\n\nChallenges\n\nUnclear what role compositionality can play (no theory of L0).",
    "crumbs": [
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "index.html#compositionality-for-models-of-linguistic-datasets",
    "href": "index.html#compositionality-for-models-of-linguistic-datasets",
    "title": "Probabilistic dynamic semantics",
    "section": "Compositionality for models of linguistic datasets",
    "text": "Compositionality for models of linguistic datasets\n\nSeamless integration between semantic theory and models of probabilistic uncertainty.",
    "crumbs": [
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "index.html#modularity",
    "href": "index.html#modularity",
    "title": "Probabilistic dynamic semantics",
    "section": "Modularity",
    "text": "Modularity\n\nFactors affecting inference judgments may be theorized about independently and combined.\n\nLexical and compositional semantics.\nWorld knowledge; etc.\nResponse behavior.",
    "crumbs": [
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "index.html#abstraction",
    "href": "index.html#abstraction",
    "title": "Probabilistic dynamic semantics",
    "section": "Abstraction",
    "text": "Abstraction",
    "crumbs": [
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "background/background.html#the-traditional-view-of-meaning",
    "href": "background/background.html#the-traditional-view-of-meaning",
    "title": "Probabilistic dynamic semantics",
    "section": "",
    "text": "Compositionality: explains how lexical knowledge gives rise to\n\n\n\n\nInference judgments arise from multiple causal factors, only one of which is semantic knowledge.\nTraditional methodologies focus on small areas of the lexicon to support a given analysis (on the order of tens of examples), jeopardizing generalizations’ validity.",
    "crumbs": [
      "Background",
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "background/background.html#experimental-semantics-and-pragmatics",
    "href": "background/background.html#experimental-semantics-and-pragmatics",
    "title": "Probabilistic dynamic semantics",
    "section": "",
    "text": "Methodologies like inference judgment tasks provide a way of studying larger areas of the lexicon, potentially improving the validity of semantic generalizations.\n\n\n\n\nGenerally, unclear how modeling constructs relate to semantic theory; i.e., what lexical semantic property is implicated and how it interacts with language-external factors.\nIn the setting of entire datasets, this question becomes one about gradience: what theoretical constructs underlie the relevant distributions of judgments?-\n\nThus little progress on the first challenge for the traditional view.",
    "crumbs": [
      "Background",
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "background/background.html#a-theoretically-oriented-approach-rsa",
    "href": "background/background.html#a-theoretically-oriented-approach-rsa",
    "title": "Probabilistic dynamic semantics",
    "section": "",
    "text": "Aims to distinguish semantic and pragmatic causes of inference behavior (i.e., L0 vs. L1/S1).\n\n\n\n\nUnclear what role compositionality can play (no theory of L0).",
    "crumbs": [
      "Background",
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "background/background.html#compositionality-for-models-of-linguistic-datasets",
    "href": "background/background.html#compositionality-for-models-of-linguistic-datasets",
    "title": "Probabilistic dynamic semantics",
    "section": "Compositionality for models of linguistic datasets",
    "text": "Compositionality for models of linguistic datasets\n\nSeamless integration between semantic theory and models of probabilistic uncertainty.",
    "crumbs": [
      "Background",
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "background/background.html#modularity",
    "href": "background/background.html#modularity",
    "title": "Probabilistic dynamic semantics",
    "section": "Modularity",
    "text": "Modularity\n\nFactors affecting inference judgments may be theorized about independently and combined.\n\nLexical and compositional semantics.\nWorld knowledge; etc.\nResponse behavior.\n\nOne potential benefit of modularity is that PDS may have different uses; e.g., one may swap out a response function for a function relating a semantic representation to likely utterances (perhaps, S1).",
    "crumbs": [
      "Background",
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "background/background.html#abstraction",
    "href": "background/background.html#abstraction",
    "title": "Probabilistic dynamic semantics",
    "section": "Abstraction",
    "text": "Abstraction\n\nWe should be able to state models of inference judgment data abstractly, i.e., by describing probability distributions without worrying about, e.g., how they are computed.\n\nConsequence: separation between theory and model.\n\nAbstraction allows PDS to be flexible (in principle) in its implementation.",
    "crumbs": [
      "Background",
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "background/background.html#experimental-semantics-and-pragmatics-aaron",
    "href": "background/background.html#experimental-semantics-and-pragmatics-aaron",
    "title": "Probabilistic dynamic semantics",
    "section": "",
    "text": "Methodologies like inference judgment tasks provide a way of studying larger areas of the lexicon, potentially improving the validity of semantic generalizations.\n\n\n\n\nGenerally, unclear how modeling constructs relate to semantic theory; i.e., what lexical semantic property is implicated and how it interacts with language-external factors.\nIn the setting of entire datasets, this question becomes one about gradience: what theoretical constructs underlie the relevant distributions of judgments?-\n\nThus little progress on the first challenge for the traditional view.",
    "crumbs": [
      "Background",
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "background/background.html#the-traditional-view-of-meaning-aaron",
    "href": "background/background.html#the-traditional-view-of-meaning-aaron",
    "title": "Probabilistic dynamic semantics",
    "section": "",
    "text": "Compositionality: explains how lexical knowledge gives rise to\n\n\n\n\nInference judgments arise from multiple causal factors, only one of which is semantic knowledge.\nTraditional methodologies focus on small areas of the lexicon to support a given analysis (on the order of tens of examples), jeopardizing generalizations’ validity.",
    "crumbs": [
      "Background",
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "pds-intro/pds-intro.html#a-monadic-interface-for-probabilistic-programs",
    "href": "pds-intro/pds-intro.html#a-monadic-interface-for-probabilistic-programs",
    "title": "Introduction",
    "section": "A monadic interface for probabilistic programs",
    "text": "A monadic interface for probabilistic programs",
    "crumbs": [
      "Introduction to probabilistic dynamic semantics",
      "Introduction"
    ]
  },
  {
    "objectID": "pds-intro/pds-intro.html#adding-discourse-constructs",
    "href": "pds-intro/pds-intro.html#adding-discourse-constructs",
    "title": "Introduction",
    "section": "Adding discourse constructs",
    "text": "Adding discourse constructs\n\nCommon grounds\n\nPossible worlds\n\n\n\nQuestions under discussion",
    "crumbs": [
      "Introduction to probabilistic dynamic semantics",
      "Introduction"
    ]
  },
  {
    "objectID": "pds-intro/pds-intro.html#parameterized-monads",
    "href": "pds-intro/pds-intro.html#parameterized-monads",
    "title": "Introduction",
    "section": "Parameterized monads",
    "text": "Parameterized monads\n\nStates",
    "crumbs": [
      "Introduction to probabilistic dynamic semantics",
      "Introduction"
    ]
  },
  {
    "objectID": "pds-intro/pds-intro.html#types-terms-and-reduction-rules",
    "href": "pds-intro/pds-intro.html#types-terms-and-reduction-rules",
    "title": "Introduction",
    "section": "Types, terms, and reduction rules",
    "text": "Types, terms, and reduction rules",
    "crumbs": [
      "Introduction to probabilistic dynamic semantics",
      "Introduction"
    ]
  },
  {
    "objectID": "pds-intro/pds-intro.html#delta-rules",
    "href": "pds-intro/pds-intro.html#delta-rules",
    "title": "Introduction",
    "section": "Delta rules",
    "text": "Delta rules",
    "crumbs": [
      "Introduction to probabilistic dynamic semantics",
      "Introduction"
    ]
  },
  {
    "objectID": "pds-intro/pds-intro.html#probabilistic-programs-in-stan",
    "href": "pds-intro/pds-intro.html#probabilistic-programs-in-stan",
    "title": "Introduction",
    "section": "Probabilistic programs in Stan",
    "text": "Probabilistic programs in Stan",
    "crumbs": [
      "Introduction to probabilistic dynamic semantics",
      "Introduction"
    ]
  },
  {
    "objectID": "vagueness/vagueness.html",
    "href": "vagueness/vagueness.html",
    "title": "Probabilistic dynamic semantics",
    "section": "",
    "text": "Outline\nThe recent advent of linguistic datasets and their associated statistical models have given rise to two major kinds of questions bearing on linguistic theory and methodology:\n\nHow can semanticists use such datasets; i.e., how can the statistical properties of a dataset inform semantic theory directly, and what guiding principles regulate the link between such properties and semantic theory?\nHow should semantic theories themselves be modified so that they may characterize not only informally collected acceptability and inference judgments, but statistical generalizations observed from datasets?\n\nThis course brings the compositional, algebraic view of meaning employed by semanticists into contact with linguistic datasets by introducing and applying the framework of Probabilistic Dynamic Semantics (Grove and White 2024a, 2025, 2024b). PDS seamlessly integrates theories of semantic competence with accounts of linguistic behavior in experimental settings by taking a modular approach: given a dataset involving some semantic phenomenon, and which exhibits certain statistical properties, this course offers an approach to developing both (a) theories of the meanings assigned to the expressions present in the dataset, and (b) linking hypotheses that directly relate these theories to linguistic behavior.\n\n\nExisting probabilistic approaches to meaning\nThe ideas developed in this course build on and respond to existing probabilistic approaches to semantics and pragmatics, including those which use computational modeling to characterize inference (Zeevat and Schmitz 2015; Franke and Jäger 2016; Brasoveanu and Dotlačil 2020; Bernardy et al. 2022; Goodman et al. n.d.). Such models are motivated, in part, by the observation that linguistic inference tends to display substantial gradience, giving rise to quantitative patterns that traditional semantic theory has difficulty capturing. Meanwhile, they often aim to explain Gricean linguistic behavior (Grice 1975) by regarding humans as Bayesian reasoners. Indeed, due to this emphasis on pragmatic principles, much modeling work blurs the semantics/pragmatics distinction, rendering the connection to traditional semantic theory somewhat opaque.\nTo take a paradigm case, models within the Rational Speech Act (RSA) framework consider human interpreters as inferring meanings for an utterance which maximize the utterance’s utility relative to a set of possible alternative utterances (Frank and Goodman 2012; Lassiter 2011; Goodman and Stuhlmüller 2013; Goodman and Frank 2016; Lassiter and Goodman 2017; Degen 2023). Probabilistic models of linguistic inference, including RSA, tend to encode Bayesian principles of probabilistic update in terms of Bayes’ theorem, which states that the posterior probability of an event given an observation is proportional to the prior probability of the event, multiplied by the likelihood of the observation given the event. RSA models give an explicit operational interpretation to Bayes’ theorem by assuming that prior distributions over inferences encode world knowledge, and that likelihoods represent the utility-maximizing behavior of a pragmatic speaker.\nDespite their success in modeling a wide variety of semantic and pragmatic phenomena, probabilistic models of linguistic data remain largely divorced from semantic and pragmatic practice, both in theory and in implementation. RSA models, for example, regard the semantic interpretations which humans pragmatically reason about as being provided by a literal listener that determines a distribution over inferences, given an utterance (Degen 2023). But aside from the constraint that the literal listener’s posterior distribution is proportional to its prior distribution (i.e., it acts as a mere filter), the semantic components of RSA models are generally designed by researchers on an ad hoc basis: on the one hand, the space (I) of possible inferences must be decided by individual researchers in a way that depends on the task being modeled; on the other hand, the relation (⟦·⟧) between utterances and inferences is typically assumed without a justified connection to any semantic theory using, e.g., an explicit grammar fragment in the style of .\n\n\nPDS as a bridge between probabilistic models and semantic theory\nGiven this background, this course introduces students to a novel approach to probabilistic meaning which integrates traditional Montague semantics, as well as ideas in compositional dynamic semantics, with probabilistic computational models in a completely seamless fashion. The theoretical framework and methodology we introduce retain the beneficial features of both kinds of approach to meaning: PDS may be used to construct probabilistic models of human inference data, and it is in principle compatible with existing probabilistic modeling paradigms such as RSA; meanwhile, it seamlessly connects probabilistic models to compositional dynamic semantics in the Montagovian tradition by providing a setting to write full-fledged grammar fragments.\nPDS additionally provides a theory of dynamic discourse update, integrating aspects of discourse such as the common ground, the question under discussion (Ginzburg 1996; Roberts 2012; Farkas and Bruce 2010), and uncertainty about lexical meaning. Crucially, given a semantic theory of some discourse phenomenon couched with PDS, one may obtain a probabilistic model of some linguistic dataset, given a particular response function (Grove and White 2024a, 2025, 2024b). We introduce PDS in the context empirical datasets studying factivity, vagueness, and the QUD.\n\n\n\n\n\n\n\nReferences\n\nBernardy, Jean-Philippe, Rasmus Blanck, Stergios Chatzikyriakidis, and Aleksandre Maskharashvili. 2022. “Bayesian Natural Language Semantics and Pragmatics.” In Probabilistic Approaches to Linguistic Theory, edited by Jean-Philippe Bernardy, Rasmus Blanck, Stergios Chatzikyriakidis, Shalom Lappin, and Aleksandre Maskharashvili. CSLI Publications.\n\n\nBrasoveanu, Adrian, and Jakub Dotlačil. 2020. Computational Cognitive Modeling and Linguistic Theory. Vol. 6. Language, Cognition, and Mind. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-31846-8.\n\n\nDegen, Judith. 2023. “The Rational Speech Act Framework.” Annual Review of Linguistics 9 (1): 519–40. https://doi.org/10.1037/rev0000186.\n\n\nFarkas, Donka F., and Kim B. Bruce. 2010. “On Reacting to Assertions and Polar Questions.” Journal of Semantics 27 (1): 81–118. https://doi.org/10.1093/jos/ffp010.\n\n\nFrank, Michael C., and Noah D. Goodman. 2012. “Predicting Pragmatic Reasoning in Language Games.” Science 336 (6084): 998–98. https://doi.org/10.1126/science.1218633.\n\n\nFranke, Michael, and Gerhard Jäger. 2016. “Probabilistic Pragmatics, or Why Bayes’ Rule Is Probably Important for Pragmatics.” Zeitschrift Für Sprachwissenschaft 35 (1): 3–44. https://doi.org/10.1515/zfs-2016-0002.\n\n\nGinzburg, Jonathan. 1996. “Dynamics and the Semantics of Dialogue.” In Logic, Language, and Computation, edited by Jerry Seligman and Dag Westerståhl, 1:221–37. Stanford: CSLI Publications.\n\n\nGoodman, Noah D., and Michael C. Frank. 2016. “Pragmatic Language Interpretation as Probabilistic Inference.” Trends in Cognitive Sciences 20 (11): 818–29. https://doi.org/10.1016/j.tics.2016.08.005.\n\n\nGoodman, Noah D., and Andreas Stuhlmüller. 2013. “Knowledge and Implicature: Modeling Language Understanding as Social Cognition.” Topics in Cognitive Science 5 (1): 173–84. https://doi.org/10.1111/tops.12007.\n\n\nGoodman, Noah D., Joshua B. Tenenbaum, Daphna Buchsbaum, Joshua Hartshorne, Robert Hawkins, Timothy O’Donnell, and Michael Henry Tessler. n.d. “Probabilistic Models of Cognition - 2nd Edition.” Probabilistic Models of Cognition. Accessed June 30, 2024. http://probmods.org/.\n\n\nGrice, H. Paul. 1975. “Logic and Conversation.” In Syntax and Semantics, edited by Peter Cole and Jerry L. Morgan, 3, Speech Acts:41–58. New York: Academic Press.\n\n\nGrove, Julian, and Aaron Steven White. 2024a. “Factivity, Presupposition Projection, and the Role of Discrete Knowlege in Gradient Inference Judgments.” LingBuzz. https://ling.auf.net/lingbuzz/007450.\n\n\n———. 2024b. “Probabilistic Dynamic Semantics.” University of Rochester. https://ling.auf.net/lingbuzz/008478.\n\n\n———. 2025. “Modeling the Prompt in Inference Judgment Tasks.” Experiments in Linguistic Meaning 3 (January): 176–87. https://doi.org/10.3765/elm.3.5857.\n\n\nLassiter, Daniel. 2011. “Vagueness as Probabilistic Linguistic Knowledge.” In Vagueness in Communication, edited by Rick Nouwen, Robert van Rooij, Uli Sauerland, and Hans-Christian Schmitz, 127–50. Lecture Notes in Computer Science. Berlin, Heidelberg: Springer. https://doi.org/10.1007/978-3-642-18446-8_8.\n\n\nLassiter, Daniel, and Noah D. Goodman. 2017. “Adjectival Vagueness in a Bayesian Model of Interpretation.” Synthese 194 (10): 3801–36. https://doi.org/10.1007/s11229-015-0786-1.\n\n\nRoberts, Craige. 2012. “Information Structure: Towards an Integrated Formal Theory of Pragmatics.” Semantics and Pragmatics 5 (December): 6:1–69. https://doi.org/10.3765/sp.5.6.\n\n\nZeevat, Henk, and Hans-Christian Schmitz, eds. 2015. Bayesian Natural Language Semantics and Pragmatics. Vol. 2. Language, Cognition, and Mind. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-17064-0.",
    "crumbs": [
      "Vagueness and imprecision",
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "vagueness/vagueness.html#scale-norming",
    "href": "vagueness/vagueness.html#scale-norming",
    "title": "Probabilistic dynamic semantics",
    "section": "Scale norming",
    "text": "Scale norming",
    "crumbs": [
      "Vagueness and imprecision",
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "vagueness/vagueness.html#likelihood-inferences",
    "href": "vagueness/vagueness.html#likelihood-inferences",
    "title": "Probabilistic dynamic semantics",
    "section": "Likelihood inferences",
    "text": "Likelihood inferences",
    "crumbs": [
      "Vagueness and imprecision",
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "vagueness/vagueness.html#gradable-adjectives",
    "href": "vagueness/vagueness.html#gradable-adjectives",
    "title": "Probabilistic dynamic semantics",
    "section": "Gradable adjectives",
    "text": "Gradable adjectives",
    "crumbs": [
      "Vagueness and imprecision",
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "vagueness/vagueness.html#scale-norming-prompts",
    "href": "vagueness/vagueness.html#scale-norming-prompts",
    "title": "Probabilistic dynamic semantics",
    "section": "Scale-norming prompts",
    "text": "Scale-norming prompts",
    "crumbs": [
      "Vagueness and imprecision",
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "vagueness/vagueness.html#likelihood-prompts",
    "href": "vagueness/vagueness.html#likelihood-prompts",
    "title": "Probabilistic dynamic semantics",
    "section": "Likelihood prompts",
    "text": "Likelihood prompts",
    "crumbs": [
      "Vagueness and imprecision",
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "vagueness/vagueness.html#delta-rules",
    "href": "vagueness/vagueness.html#delta-rules",
    "title": "Probabilistic dynamic semantics",
    "section": "Delta rules",
    "text": "Delta rules",
    "crumbs": [
      "Vagueness and imprecision",
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "factivity/factivity.html",
    "href": "factivity/factivity.html",
    "title": "Probabilistic dynamic semantics",
    "section": "",
    "text": "Factivity, presupposition projection, and the role of discrete knowledge in gradient inference judgments (Aaron)\n\n\nSome implementation details",
    "crumbs": [
      "Factivity inferences",
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "factivity/factivity.html#delta-rules",
    "href": "factivity/factivity.html#delta-rules",
    "title": "Probabilistic dynamic semantics",
    "section": "Delta rules",
    "text": "Delta rules",
    "crumbs": [
      "Factivity inferences",
      "Probabilistic dynamic semantics"
    ]
  },
  {
    "objectID": "qud/qud.html#delta-rules",
    "href": "qud/qud.html#delta-rules",
    "title": "Probabilistic dynamic semantics",
    "section": "Delta rules",
    "text": "Delta rules",
    "crumbs": [
      "Modeling the question prompt",
      "Probabilistic dynamic semantics"
    ]
  }
]