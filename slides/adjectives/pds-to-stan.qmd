---
title: "From PDS to Stan"
bibliography: ../../pds.bib
format:
  revealjs:
    css: ../styles.css
    html-math-method: mathjax
    mathjax-config:
      loader: {load: ['[tex]/bussproofs','[tex]/bbox','[tex]/colorbox']}
      tex:
        packages: {'[+]': ['bussproofs','bbox','colorbox']}
---

::: {.hidden}
$$
\newcommand{\expr}[3]{\begin{array}{c}
#1 \\
\bbox[lightblue,5px]{#2}
\end{array} ⊢ #3}
\newcommand{\ct}[1]{\bbox[font-size: 0.8em]{\mathsf{#1}}}
\newcommand{\updct}[1]{\ct{upd\_#1}}
\newcommand{\abbr}[1]{\bbox[transform: scale(0.95)]{\mathtt{#1}}}
\newcommand{\pure}[1]{\bbox[border: 1px solid orange]{\bbox[border: 4px solid transparent]{#1}}}
\newcommand{\return}[1]{\bbox[border: 1px solid black]{\bbox[border: 4px solid transparent]{#1}}}
\def\P{\mathtt{P}}
\def\Q{\mathtt{Q}}
\def\True{\ct{T}}
\def\False{\ct{F}}
\def\ite{\ct{if\_then\_else}}
\def\Do{\abbr{do}}
$$
:::

# Probabilistic programs in Stan

## From PDS to Stan

Having seen how gradable adjectives work in compositional semantics, we now face a practical challenge:

- How do we test semantic theories against experimental data?
- How do we translate abstract λ-terms to statistical models?

::: {.fragment}
### Our approach

PDS automates the translation from compositional semantics to statistical kernels, which we implement in Stan.
:::

---

## The compilation challenge

::: {.incremental}
- **Abstract semantic theory**: λ-terms, type theory, compositional rules
- **Concrete statistical models**: Parameters, likelihoods, inference
- **The gap**: Need to make linking hypotheses explicit
:::

::: {.fragment}
PDS bridges this gap by automating the core translation while leaving room for statistical expertise
:::

---

## What PDS produces

PDS outputs a **kernel model**—the semantic core from lexical/compositional semantics

::: {.fragment}
### Example: Kernel vs Full Model

::: {.columns}
::: {.column width="45%"}
**PDS Kernel:**
```stan
model {
  w ~ normal(0.0, 1.0);
  target += normal_lpdf(y | w, sigma);
}
```
:::

::: {.column width="45%"}
**Full Model:**
```stan {code-line-numbers="2,5"}
model {
  w ~ normal(0.0, 1.0);          // PDS
  sigma_e ~ beta(2, 10);         // Analyst
  z_eps ~ std_normal();          // Analyst
  target += normal_lpdf(y | w + eps, sigma_e);
}
```
:::
:::
:::

---

## Why Stan?

Stan is a probabilistic programming language designed for statistical inference

::: {.incremental}
- **Declarative**: We specify probability models, not procedures
- **Specialized**: Designed for Bayesian inference via HMC
- **Parallel to semantics**: Both declare "what" not "how"
  - Semantics: truth conditions
  - Stan: probabilistic relationships
:::

---

## Stan's block structure - Part 1

```stan
data {
  // What we observe from experiments
  int<lower=1> N;           // number of observations
  vector[N] y;              // responses
}

parameters {
  // What we want to learn
  real mu;                  // mean parameter
  real<lower=0> sigma;      // standard deviation
}
```

::: {.fragment}
Clear separation between what's given and what's inferred
:::

---

## Stan's block structure - Part 2

```stan
transformed parameters {
  // Derived from parameters
  real precision = 1 / sigma^2;  // for convenience
}

model {
  // How data relates to parameters
  mu ~ normal(0, 10);      // prior
  sigma ~ exponential(1);   // prior
  y ~ normal(mu, sigma);    // likelihood
}
```

---

## Why this structure matters

::: {.incremental}
- **Separates observed from inferred**: Makes assumptions explicit
- **Enables optimization**: Stan knows what can be precomputed
- **Forces clarity**: Must specify every relationship
- **Matches PDS philosophy**: Declarative, not procedural
:::

---

# Case Study: Norming Model

## Our first example

::: {.incremental}
- **Question**: "How tall is Jo?"
- **Task**: Participants report degrees on scales
- **Goal**: Infer the "true" degree for each item
:::

::: {.fragment}
This seemingly simple task will reveal the complexity of the PDS-to-Stan pipeline
:::

---

## The experimental data

```{style="font-size: 85%;"}
| participant | item | adjective | condition | response |
|-------------|------|-----------|-----------|----------|
| 1 | tall_high | tall | high | 0.82 |
| 1 | wide_low | wide | low | 0.34 |
| 1 | expensive_mid | expensive | mid | 0.62 |
| 1 | full_high | full | high | 1.00 |
```

::: {.fragment}
- **Items** = adjective × condition pairs
- **Responses** on 0-1 scale (slider)
- **Challenge**: Handle boundary responses (0s and 1s)
:::

---

## Building the data block - counts

```stan
data {
  int<lower=1> N_item;        // number of items
  int<lower=1> N_participant; // number of participants  
  int<lower=1> N_data;        // number of data points
```

::: {.fragment}
### Why constraints?

- `<lower=1>` ensures valid data
- Helps Stan optimize algorithms
- Catches errors early
:::

---

## Handling boundary responses

```stan
  // Censoring at boundaries
  int<lower=1> N_0;           // number of 0s
  int<lower=1> N_1;           // number of 1s
```

::: {.fragment}
### Why separate these?

- Slider endpoints are special
- 0 might mean "even less than 0"
- 1 might mean "even more than 1"
- Censored data needs special treatment
:::

---

## Response data and indexing

```stan
  vector<lower=0, upper=1>[N_data] y; // responses in (0,1)
  
  // Which item/participant for each response
  array[N_data] int<lower=1, upper=N_item> item;
  array[N_data] int<lower=1, upper=N_participant> participant;
}
```

::: {.fragment}
These arrays are lookup tables: `item[5] = 3` means the 5th response is about item #3
:::

---

## Parameters: What we want to learn

```stan
parameters {
  // Semantic parameters - what PDS cares about
  vector[N_item] mu_guess;    // degree for each item
```

::: {.fragment}
This is the core semantic content: each item has a "true" degree on its scale
:::

---

## Random effects structure

```stan
  // How much people vary
  real<lower=0> sigma_epsilon_guess;
  
  // Each person's deviation (z-scores)
  vector[N_participant] z_epsilon_guess;
```

::: {.fragment}
### Non-centered parameterization

- Separate scale (`sigma`) from standardized deviations (`z`)
- Helps Stan's algorithms converge
- Mathematically equivalent but computationally superior
:::

---

## Measurement and censoring

```stan
  real<lower=0,upper=1> sigma_e;  // response noise
  
  // True values for censored data
  array[N_0] real<upper=0> y_0;    // true values for 0s
  array[N_1] real<lower=1> y_1;    // true values for 1s
}
```

::: {.fragment}
We infer what the "true" values might have been beyond scale boundaries
:::

---

## Transformed parameters

```stan
transformed parameters {
  // Convert z-scores to actual effects
  vector[N_participant] epsilon_guess = 
    sigma_epsilon_guess * z_epsilon_guess;
```

::: {.fragment}
If `sigma = 0.2` and `z[3] = 1.5`, then participant 3 gives responses 0.3 units higher than average
:::

---

## Computing predictions

```stan
  vector[N_data] guess;
  for (i in 1:N_data) {
    guess[i] = mu_guess[item[i]] + 
               epsilon_guess[participant[i]];
  }
}
```

::: {.fragment}
### Example trace
- Response i: item 5, participant 3
- `mu_guess[5] = 0.7` (item's degree)
- `epsilon_guess[3] = 0.1` (participant's adjustment)
- `guess[i] = 0.7 + 0.1 = 0.8`
:::

---

## The model block

```stan
model {
  // Priors
  sigma_epsilon_guess ~ exponential(1);
  z_epsilon_guess ~ std_normal();
  
  // Likelihood
  for (i in 1:N_data) {
    y[i] ~ normal(guess[i], sigma_e);
  }
}
```

---

# PDS to Stan Compilation

## PDS code structure

```haskell
-- Example using actual PDS functions
-- assert, ask, (>>>) from Lambda.Convenience
-- ty from Lambda.Types
discourse' = ty tau $ assert s1' >>> ask q1'
```

::: {.fragment}
This seemingly simple code triggers a complex compilation process
:::

---

## The 'how' operator

```haskell
-- From Grammar.Lexica.SynSem.Adjectives
"how" -> [ SynSem {
  syn = Qdeg :/: (S :/: AP) :/: (AP :\: Deg),
  sem = ty tau (purePP (lam x (lam y (lam z (y @@ (x @@ z))))))
  }
, SynSem {
  syn = Qdeg :/: (S :\: Deg),
  sem = ty tau (purePP (lam x x))
  } ]
```

::: {.fragment}
These entries handle degree questions in the compositional semantics
:::

---

## Initial semantic representation

The compositional semantics for degree questions involves extracting degree information from the discourse state.

::: {.fragment}
The delta rules will transform these complex semantic representations into simpler forms
:::

---

## Delta rules: What are they?

Delta rules are partial functions that simplify terms while preserving meaning

::: {.incremental}
- Type: `Term -> Maybe Term`
- Preserve semantic equivalence
- Enable computational tractability
- Defined in [`Lambda.Delta`](https://juliangrove.github.io/pds/Lambda-Delta.html)
:::

---

## The indices delta rule

```haskell
-- From Lambda.Delta module
indices :: DeltaRule
indices = \case
  Height (UpdHeight p _) -> Just p
  Height (UpdSocPla _ i) -> Just (Height i)
  SocPla (UpdSocPla p _) -> Just p
  SocPla (UpdHeight _ i) -> Just (SocPla i)
  Ling   (UpdLing p _)   -> Just p
  Ling   (UpdEpi _ i)    -> Just (Ling i)
  Ling   (UpdSocPla _ i) -> Just (Ling i)
  _                      -> Nothing
```

::: {.fragment}
Extracts the height value from the discourse state for entity j
:::

---

## Delta reduction process

The indices rule extracts values from the discourse state:

```haskell
-- From Lambda.Delta
indices :: DeltaRule
indices = \case
  Height (UpdHeight p _) -> Just p
  -- ... other cases
```

::: {.fragment}
Through a series of delta reductions, complex semantic terms are simplified to forms that can be compiled to Stan parameters
:::

---

## Key transformations

::: {.incremental}
1. **Degree questions → Parameter inference**
   - Questions about degrees become Stan parameters to infer

2. **Functions → Arrays**
   - $\ct{height} : \iota \to e \to r$ becomes `height[person]`

3. **Monadic structure → Target**
   - `Return` values become parameters or data
   - Bind operations become likelihood statements
:::

---

## PDS kernel output

```stan
model {
  w ~ normal(0.0, 1.0);
  target += normal_lpdf(y | w, sigma);
}
```

::: {.fragment}
::: {style="font-size: 85%;"}
Actual PDS output: `model { w ~ normal(0.0, 1.0); target += normal_lpdf(y | w, sigma); }`
:::

- `w` = degree parameter
- Prior on degrees + likelihood of observations
:::

---

## From kernel to full model

```stan {code-line-numbers="2,6"}
model {
  mu_guess ~ normal(0.0, 1.0);      // PDS kernel
  sigma_epsilon_guess ~ exponential(1);  // Analyst
  sigma_e ~ beta(2, 10);            // Analyst
  z_epsilon_guess ~ std_normal();   // Analyst
  y ~ normal(mu_guess[item] + epsilon_guess[participant], 
             sigma_e);              // PDS kernel + modifications
}
```

---

# Gradable Adjectives and Vagueness

## From degrees to judgments

New question: "Is Jo tall?"

::: {.incremental}
- No longer asking for a degree
- Binary judgment with vagueness
- Requires comparing degree to threshold
- **Vagueness** = threshold uncertainty
:::

---

## PDS entry for 'tall'

```haskell
-- From Grammar.Lexica.SynSem.Adjectives
"tall" -> [ SynSem {
  syn = AP :\: Deg,
  sem = ty tau (purePP (lam d (lam x (lam i (sCon "(≥)" @@ (sCon "height" @@ i @@ x) @@ d)))))
  }
, SynSem {
  syn = AP,
  sem = ty tau (lam s (purePP (lam x (lam i (sCon "(≥)" @@ (sCon "height" @@ i @@ x) @@ (sCon "d_tall" @@ s)))) @@ s))
  } ]
```

::: {.fragment}
Different from degree-question version!
:::

---

## Semantic components

::: {.incremental}
- $\ct{height}$ : $\iota \to e \to r$
  - Maps entities to their heights
- $\ct{d\_tall}$ : $\sigma \to r$  
  - Extracts threshold from discourse state
- $\ct{(≥)}$ : $r \to r \to t$
  - Comparison operator
:::

---

## Initial representation

"Jo is tall" parses to:

$$\ct{(≥)}(\ct{height}(i)(j))(\ct{d\_tall}(s))$$

::: {.fragment}
We need to reduce this step by step using delta rules
:::

---

## The states delta rule

```haskell
-- From Lambda.Delta module
states :: DeltaRule
states = \case
  DTall   (UpdDTall d _)   -> Just d
  DTall   (UpdCG _ s)      -> Just (DTall s)
  DTall   (UpdQUD _ s)     -> Just (DTall s)
  CG      (UpdCG cg _)     -> Just cg
  CG      (UpdDTall _ s)   -> Just (CG s)
  CG      (UpdQUD _ s)     -> Just (CG s)
  _                        -> Nothing
```

::: {.fragment}
Extracts the threshold for "tall" from the discourse state
:::

---

## Step-by-step reduction

Extract threshold:

$$\ct{(≥)}(\ct{height}(i)(j))(\ct{d\_tall}(s))$$

::: {.fragment}
$$\downarrow \text{ states}$$

$$\ct{(≥)}(\ct{height}(i)(j))(d)$$

where $d$ is the contextual threshold for tallness
:::

---

## Continue reduction

Extract height:

$$\ct{(≥)}(\ct{height}(i)(j))(d)$$

::: {.fragment}
$$\downarrow \text{ indices}$$

$$\ct{(≥)}(h)(d)$$

where $h$ is Jo's height
:::

---

## Arithmetic rule

```haskell
-- From Lambda.Delta module  
arithmetic :: DeltaRule
arithmetic = \case
  Add t u      -> case t of
                    Zero -> Just u
                    x@(DCon _) -> case u of
                                    Zero       -> Just x
                                    y@(DCon _) -> Just (x + y)
                                    _          -> Nothing
                    t'         -> case u of
                                    Zero -> Just t'
                                    _    -> Nothing
  Mult t u     -> case t of
                     Zero       -> Just Zero
                     One        -> Just u
                     x@(DCon _) -> case u of
                                     Zero       -> Just Zero
                                     One        -> Just x
                                     y@(DCon _) -> Just (x * y)
                     t'         -> case u of
                                     Zero -> Just Zero
                                     One  -> Just t'
                                     _    -> Nothing
  Neg (DCon x) -> Just (dCon (-x))
  _            -> Nothing
```

::: {.fragment}
Note: This rule handles arithmetic operations, not comparisons like GE
:::

---

## Probabilistic structure

Since we can't reduce symbolically, wrap in distributions:

```haskell
d ~ Normal(mu, sigma)      -- threshold distribution
h ~ Normal(mu_h, sigma_h)  -- height distribution
Return((≥)(h, d))          -- comparison
```

::: {.fragment}
This captures vagueness through threshold uncertainty
:::

---

## Monadic compilation

The bind operator sequences computations:

```stan
target += normal_lpdf(d | mu, sigma);
target += normal_lpdf(h | mu_h, sigma_h);
target += bernoulli_lpmf(y | h >= d);
```

::: {.fragment}
Each line adds to the log probability
:::

---

## PDS compilation details

**Input PDS:**
```haskell
-- From Grammar.Parser and Grammar.Lexica.SynSem.Adjectives
expr = ["jo", "is", "tall"]
parses = interpretations @Adjectives expr 0
s1 = sem (head (getList parses))
discourse = ty tau $ assert s1
```

::: {.fragment}
**Kernel output:**
```stan
model {
  mu ~ normal(0.5, 0.2);      // threshold location
  sigma ~ exponential(5);      // vagueness
  target += normal_cdf_log(d | mu, sigma);
}
```
:::

---

# Likelihood Judgments

## Metalinguistic judgments

"How likely is Jo tall?"

::: {.incremental}
- Not binary true/false
- Gradient likelihood judgment
- Requires probabilistic reasoning
- Marginalizes over uncertainty
:::

---

## The likely operator

```haskell
-- From Grammar.Lexica.SynSem.Adjectives
"likely" -> [ SynSem {
  syn = S :\: Deg :/: S,
  sem = ty tau (lam s (purePP (lam p (lam d (lam _' (sCon "(≥)" @@ (Pr (let' i (CG s) (Return (p @@ i)))) @@ d)))) @@ s))
} ]
```

::: {.fragment}
Compares probability of proposition to a degree threshold
:::

---

## Probabilistic delta rules

```haskell
-- From Lambda.Delta module
probabilities :: DeltaRule
probabilities = \case
  Pr (Return Tr)                                             -> Just 1
  Pr (Return Fa)                                             -> Just 0
  Pr (Bern x)                                                -> Just x
  Pr (Disj x t u)                                            -> Just (x * Pr t + (1 - x) * Pr u)
  Pr (Let v (Normal x y) (Return (GE t (Var v')))) | v' == v -> Just (NormalCDF x y t)
  Pr (Let v (Normal x y) (Return (GE (Var v') t))) | v' == v -> Just (NormalCDF (- x) y t)
  _                                                          -> Nothing
```

---

## How probabilities handles marginalization

The `probabilities` rule can compute $P(X \geq y)$ when $X \sim N(\mu, \sigma)$:

```haskell
Pr (Let v (Normal x y) (Return (GE t (Var v')))) | v' == v -> Just (NormalCDF x y t)
```

This evaluates to the CDF of a normal distribution, which gives us the probability that a normally distributed variable exceeds a threshold.

---

## Compiled to Stan

```stan
real p = normal_cdf((mu_h - mu_d) / 
                    sqrt(sigma_h^2 + sigma_d^2) | 0, 1);
target += normal_lpdf(y | p, 0.1);
```

::: {.fragment}
The probability of "tall" becomes a parameter itself
:::

---

# Relative vs Absolute Standards

## Theoretical distinction

::: {.columns}
::: {.column width="45%"}
**Relative adjectives**
- "tall": varies by context
- Basketball player vs child
- Standard shifts dramatically
:::

::: {.column width="45%"}
**Absolute adjectives**
- "full": anchored to endpoint
- Glass is full or not
- Standard relatively fixed
:::
:::

---

## Relative adjective entry

```haskell
-- Theoretical extension: context-dependent thresholds
-- (not in current Adjectives.hs implementation)
"tall" -> [ SynSem {
  sem = ... (sCon "d_tall" @@ s @@ 
            (sCon "context" @@ i)) ...
} ]
```

::: {.fragment}
Threshold depends on context - extra argument!
:::

---

## Absolute adjective entry

```haskell
-- Example of absolute adjective (not in Adjectives.hs)
-- Would have fixed threshold independent of context
"full" -> [ SynSem {
  sem = ... (sCon "d_full" @@ s) ...
} ]
```

::: {.fragment}
Fixed threshold - no context dependence
:::

---

---

## Mixture model insight

Discrete semantics → gradient behavior

$$P(\text{response}) = \pi \cdot P(\text{relative}) + (1-\pi) \cdot P(\text{absolute})$$

::: {.fragment}
- $\pi$ = probability of relative interpretation
- Learn which interpretation dominates
- Different adjectives may have different $\pi$
:::

---

# Summary

## The complete pipeline

```{style="text-align: center;"}
Semantic theory (λ-terms)
     ↓ Delta rules
Simplified form
     ↓ Compilation
Kernel model (Stan)
     ↓ Augmentation
Full statistical model
```

---

## Key insights

::: {.incremental}
- **Delta rules** bridge theory and computation
- **Kernel models** isolate semantic content
- **Augmentation** adds statistical machinery
- **Compilation** embodies linking hypotheses
:::

---

## What we've covered

::: {.incremental}
- **Degree inference** from questions
- **Vagueness** as threshold uncertainty
- **Likelihood** via probabilistic computation
- **Relative/absolute** via context dependence
:::

---

## Future directions

::: {.incremental}
- **Automated augmentation**: Add random effects automatically
- **Verified compilation**: Prove delta rules preserve semantics
- **Richer kernels**: Output mixture models directly
- **Community growth**: Contribute lexical entries, delta rules
:::

::: {.fragment}
Next: How this approach handles factivity—where gradience poses even deeper puzzles
:::