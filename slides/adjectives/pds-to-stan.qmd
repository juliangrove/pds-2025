---
title: "From PDS to Stan"
bibliography: ../../pds.bib
format:
  revealjs:
    css: ../styles.css
    html-math-method: mathjax
    mathjax-config:
      loader: {load: ['[tex]/bussproofs','[tex]/bbox','[tex]/colorbox']}
      tex:
        packages: {'[+]': ['bussproofs','bbox','colorbox']}
---

::: {.hidden}
$$
\newcommand{\expr}[3]{\begin{array}{c}
#1 \\
\bbox[lightblue,5px]{#2}
\end{array} ⊢ #3}
\newcommand{\ct}[1]{\bbox[font-size: 0.8em]{\mathsf{#1}}}
\newcommand{\updct}[1]{\ct{upd\_#1}}
\newcommand{\abbr}[1]{\bbox[transform: scale(0.95)]{\mathtt{#1}}}
\newcommand{\pure}[1]{\bbox[border: 1px solid orange]{\bbox[border: 4px solid transparent]{#1}}}
\newcommand{\return}[1]{\bbox[border: 1px solid black]{\bbox[border: 4px solid transparent]{#1}}}
\def\P{\mathtt{P}}
\def\Q{\mathtt{Q}}
\def\True{\ct{T}}
\def\False{\ct{F}}
\def\ite{\ct{if\_then\_else}}
\def\Do{\abbr{do}}
$$
:::

# From PDS to Stan

## The compilation challenge

::: {.incremental}
- **Semantic theory**: λ-terms, compositional rules
- **Statistical models**: Parameters, likelihoods, inference  
- **The gap**: Need explicit linking hypotheses
:::

::: {.fragment}
PDS automates the core translation while leaving room for statistical expertise
:::

---

## What PDS produces

PDS outputs a **kernel model**—the semantic core from lexical/compositional semantics

::: {.fragment}
### Example: Kernel vs Full Model

::: {.columns}
::: {.column width="45%"}
**PDS Kernel:**
```stan
model {
  w ~ normal(0.0, 1.0);
  target += normal_lpdf(y | w, sigma);
}
```
:::

::: {.column width="45%"}
**Full Model:**
```stan {code-line-numbers="2,5"}
model {
  w ~ normal(0.0, 1.0);          // PDS
  sigma_e ~ beta(2, 10);         // Analyst
  z_eps ~ std_normal();          // Analyst
  target += normal_lpdf(y | w + eps, sigma_e);
}
```
:::
:::
:::

---

## Why Stan?

Stan is a probabilistic programming language designed for statistical inference

::: {.incremental}
- **Declarative**: We specify probability models, not procedures
- **Specialized**: Designed for Bayesian inference via HMC
- **Parallel to semantics**: Both declare "what" not "how"
  - Semantics: truth conditions
  - Stan: probabilistic relationships
:::

---

## Stan's block structure

```stan {style="font-size: 85%;"}
data {
  int<lower=1> N;           // observed
  vector[N] y;              
}
parameters {
  real mu;                  // inferred
  real<lower=0> sigma;      
}
model {
  mu ~ normal(0, 10);       // prior
  y ~ normal(mu, sigma);    // likelihood
}
```

::: {.fragment}
**Key insight**: Declarative structure parallels compositional semantics
:::

---

# Case Study: Norming Model

## Our first example

::: {.incremental}
- **Question**: "How tall is Jo?"
- **Task**: Participants report degrees on scales
- **Goal**: Infer the "true" degree for each item
:::

::: {.fragment}
This seemingly simple task will reveal the complexity of the PDS-to-Stan pipeline
:::

---

## The experimental data

```{style="font-size: 85%;"}
| participant | item | adjective | condition | response |
|-------------|------|-----------|-----------|----------|
| 1 | tall_high | tall | high | 0.82 |
| 1 | wide_low | wide | low | 0.34 |
| 1 | expensive_mid | expensive | mid | 0.62 |
| 1 | full_high | full | high | 1.00 |
```

::: {.fragment}
- **Items** = adjective × condition pairs
- **Responses** on 0-1 scale (slider)
- **Challenge**: Handle boundary responses (0s and 1s)
:::

---

## Building the data block - counts

```stan
data {
  int<lower=1> N_item;        // number of items
  int<lower=1> N_participant; // number of participants  
  int<lower=1> N_data;        // number of data points
```

::: {.fragment}
### Why constraints?

- `<lower=1>` ensures valid data
- Helps Stan optimize algorithms
- Catches errors early
:::

---

## Handling boundary responses

```stan
  // Censoring at boundaries
  int<lower=1> N_0;           // number of 0s
  int<lower=1> N_1;           // number of 1s
```

::: {.fragment}
### Why separate these?

- Slider endpoints are special
- 0 might mean "even less than 0"
- 1 might mean "even more than 1"
- Censored data needs special treatment
:::

---

## Response data and indexing

```stan
  vector<lower=0, upper=1>[N_data] y; // responses in (0,1)
  
  // Which item/participant for each response
  array[N_data] int<lower=1, upper=N_item> item;
  array[N_data] int<lower=1, upper=N_participant> participant;
}
```

::: {.fragment}
These arrays are lookup tables: `item[5] = 3` means the 5th response is about item #3
:::

---

## Parameters: What we want to learn

```stan
parameters {
  // Semantic parameters - what PDS cares about
  vector[N_item] mu_guess;    // degree for each item
```

::: {.fragment}
This is the core semantic content: each item has a "true" degree on its scale
:::

---

## Random effects structure

```stan
  // How much people vary
  real<lower=0> sigma_epsilon_guess;
  
  // Each person's deviation (z-scores)
  vector[N_participant] z_epsilon_guess;
```

::: {.fragment}
### Non-centered parameterization

- Separate scale (`sigma`) from standardized deviations (`z`)
- Helps Stan's algorithms converge
- Mathematically equivalent but computationally superior
:::

---

## Measurement and censoring

```stan
  real<lower=0,upper=1> sigma_e;  // response noise
  
  // True values for censored data
  array[N_0] real<upper=0> y_0;    // true values for 0s
  array[N_1] real<lower=1> y_1;    // true values for 1s
}
```

::: {.fragment}
We infer what the "true" values might have been beyond scale boundaries
:::

---

## Transformed parameters

```stan
transformed parameters {
  // Convert z-scores to actual effects
  vector[N_participant] epsilon_guess = 
    sigma_epsilon_guess * z_epsilon_guess;
```

::: {.fragment}
If `sigma = 0.2` and `z[3] = 1.5`, then participant 3 gives responses 0.3 units higher than average
:::

---

## Computing predictions

```stan
  vector[N_data] guess;
  for (i in 1:N_data) {
    guess[i] = mu_guess[item[i]] + 
               epsilon_guess[participant[i]];
  }
}
```

::: {.fragment}
### Example trace
- Response i: item 5, participant 3
- `mu_guess[5] = 0.7` (item's degree)
- `epsilon_guess[3] = 0.1` (participant's adjustment)
- `guess[i] = 0.7 + 0.1 = 0.8`
:::

---

## The model block

```stan
model {
  // Priors
  sigma_epsilon_guess ~ exponential(1);
  z_epsilon_guess ~ std_normal();
  
  // Likelihood
  for (i in 1:N_data) {
    y[i] ~ normal(guess[i], sigma_e);
  }
}
```

---

# PDS to Stan Compilation

## The PDS implementation

```haskell
-- From sources/pds/src/
s1'        = termOf $ getSemantics @Adjectives 1 
             ["jo", "is", "a", "soccer player"]
q1'        = termOf $ getSemantics @Adjectives 0 
             ["how", "tall", "jo", "is"]
discourse' = ty tau $ assert s1' >>> ask q1'
scaleNormingExample = asTyped tau 
  (betaDeltaNormal deltaRules . 
   adjectivesRespond scaleNormingPrior) discourse'
```

::: {.fragment}
This establishes context and asks for Jo's height
:::

---

## Lexical entries for degree questions

```haskell {style="font-size: 80%;"}
-- From Grammar.Lexica.SynSem.Adjectives
"tall" -> [ SynSem {
  syn = AP :\: Deg,
  sem = ty tau (purePP (lam d (lam x (lam i 
    (sCon "(≥)" @@ (sCon "height" @@ i @@ x) @@ d)))))
  }, ... ]

"how" -> [ SynSem {
  syn = Qdeg :/: (S :/: AP) :/: (AP :\: Deg),
  sem = ty tau (purePP (lam x (lam y (lam z 
    (y @@ (x @@ z))))))
  }, ... ]
```

::: {.fragment}
The degree-argument version of adjectives enables degree questions
:::

---

## Compositional semantics

For "how tall is Jo?", composition yields:

$$\ct{Return}(\iota(\lambda d[\ct{height}(i)(j) = d]))$$

::: {.fragment}
This term:
- Extracts Jo's height at index $i$
- Uses $\iota$ to get the unique degree
- Wraps in $\ct{Return}$ for monadic structure
:::

---

## Delta rules: What are they?

Delta rules are partial functions that simplify terms while preserving meaning

::: {.incremental}
- Type: `Term -> Maybe Term`
- Preserve semantic equivalence
- Enable computational tractability
- Defined in [`Lambda.Delta`](https://juliangrove.github.io/pds/Lambda-Delta.html)
:::

---

## The indices delta rule

```haskell {style="font-size: 80%;"}
-- From Lambda.Delta module
indices :: DeltaRule
indices = \case
  Height (UpdHeight p _) -> Just p
  Height (UpdSocPla _ i) -> Just (Height i)
  SocPla (UpdSocPla p _) -> Just p
  SocPla (UpdHeight _ i) -> Just (SocPla i)
  -- ... other cases for Ling, Epi
  _                      -> Nothing
```

::: {.fragment}
Extracts values from the discourse state
:::

---

## Delta reduction walkthrough

Starting term:
$$\ct{Return}(\iota(\lambda d[\ct{height}(i)(j) = d]))$$

::: {.fragment}
Apply `indices` rule to extract height:
$$\ct{Return}(\iota(\lambda d[h = d]))$$
where $h$ is Jo's actual height
:::

::: {.fragment}
Apply iota extraction:
$$\ct{Return}(h)$$
:::

---

## Key transformations

::: {.incremental}
1. **Degree questions → Parameter inference**
   - Questions about degrees become Stan parameters to infer

2. **Functions → Arrays**
   - $\ct{height} : \iota \to e \to r$ becomes `height[person]`

3. **Monadic structure → Target**
   - `Return` values become parameters or data
   - Bind operations become likelihood statements
:::

---

## PDS kernel output

```stan
model {
  w ~ normal(0.0, 1.0);
  target += normal_lpdf(y | w, sigma);
}
```

::: {.fragment}
::: {style="font-size: 85%;"}
Actual PDS output: `model { w ~ normal(0.0, 1.0); target += normal_lpdf(y | w, sigma); }`
:::

- `w` = degree parameter
- Prior on degrees + likelihood of observations
:::

---

## From kernel to full model

```stan {code-line-numbers="2,7"}
model {
  mu_guess ~ normal(0.0, 1.0);           // PDS kernel
  sigma_epsilon_guess ~ exponential(1);  // Analyst
  sigma_e ~ beta(2, 10);                 // Analyst
  z_epsilon_guess ~ std_normal();        // Analyst
  
  y ~ normal(mu_guess[item] +            // PDS kernel
             epsilon_guess[participant], // + analyst
             sigma_e);                   
}
```

::: {.fragment}
The kernel captures semantic content; augmentations handle experimental realities
:::

---

# Modeling Vagueness

## From degrees to likelihood judgments

New question: "How likely is it that Jo is tall?"

::: {.incremental}
- Not asking for a degree
- Metalinguistic judgment about adjective application
- Requires probabilistic reasoning
- **Key**: Threshold uncertainty creates vagueness
:::

---

## The PDS implementation

```haskell {style="font-size: 85%;"}
-- From sources/pds/src/
expr1 = ["jo", "is", "a", "soccer", "player"]
expr2 = ["how", "likely", "that", "jo", "is", "tall"]
s1 = getSemantics @Adjectives 0 expr1
q1 = getSemantics @Adjectives 0 expr2
discourse = ty tau $ assert s1 >>> ask q1
likelihoodExample = asTyped tau 
  (betaDeltaNormal deltaRules . 
   adjectivesRespond likelihoodPrior) discourse
```

---

## Lexical entries (Part 1)

```haskell {style="font-size: 75%;"}
-- From Grammar.Lexica.SynSem.Adjectives
"tall" -> [ SynSem {
    syn = AP,
    sem = ty tau (lam s (purePP (lam x (lam i 
      (sCon "(≥)" @@ (sCon "height" @@ i @@ x) @@ 
       (sCon "d_tall" @@ s)))) @@ s))
    } ]
```

::: {.fragment}
Key semantic components:
- `height`: $\iota \to e \to r$ (entity heights)
- `d_tall`: $\sigma \to r$ (contextual threshold)
- `(≥)`: $r \to r \to t$ (comparison)
:::

---

## Lexical entries (Part 2)

```haskell {style="font-size: 75%;"}
"likely" -> [ SynSem {
    syn = S :\: Deg :/: S,
    sem = ty tau (lam s (purePP (lam p (lam d (lam _' 
      (sCon "(≥)" @@ 
       (Pr (let' i (CG s) (Return (p @@ i)))) @@ 
       d)))) @@ s))
    } ]
```

::: {.fragment}
Compares probability of embedded proposition to a degree
:::

---

## Delta reduction for vagueness

Starting with "Jo is tall":
$$\ct{(≥)}(\ct{height}(i)(j))(\ct{d\_tall}(s))$$

::: {.fragment}
Apply `states` rule to extract threshold:
$$\ct{(≥)}(\ct{height}(i)(j))(d)$$
:::

::: {.fragment}
Apply `indices` rule to extract height:
$$\ct{(≥)}(h)(d)$$
:::

---

## The states delta rule

```haskell {style="font-size: 75%;"}
-- From Lambda.Delta (lines 167-183)
states :: DeltaRule
states = \case
  CG      (UpdCG cg _)     -> Just cg
  CG      (UpdDTall _ s)   -> Just (CG s)
  DTall   (UpdDTall d _)   -> Just d
  DTall   (UpdCG _ s)      -> Just (DTall s)
  QUD     (UpdQUD q _)     -> Just q
  QUD     (UpdCG _ s)      -> Just (QUD s)
  TauKnow (UpdTauKnow b _) -> Just b
  TauKnow (UpdCG _ s)      -> Just (TauKnow s)
  _                        -> Nothing
```

---

## The probabilities delta rule

```haskell {style="font-size: 75%;"}
-- From Lambda.Delta (lines 156-164)
probabilities :: DeltaRule
probabilities = \case
  Pr (Return Tr)         -> Just 1
  Pr (Return Fa)         -> Just 0
  Pr (Bern x)            -> Just x
  Pr (Disj x t u)        -> Just (x * Pr t + (1 - x) * Pr u)
  Pr (Let v (Normal x y) (Return (GE t (Var v')))) 
    | v' == v -> Just (NormalCDF x y t)
  Pr (Let v (Normal x y) (Return (GE (Var v') t))) 
    | v' == v -> Just (NormalCDF (- x) y t)
  _                      -> Nothing
```

---

## Probabilistic computation

The system computes $P(h \geq d)$ when both are uncertain:

::: {.incremental}
- $h \sim \mathcal{N}(\mu_h, \sigma_h)$ (height)
- $d \sim \mathcal{N}(\mu_d, \sigma_d)$ (threshold)
- Result: `normal_cdf` computation in Stan
:::

::: {.fragment}
```stan
response = 1 - normal_cdf(d[item[i]] | 
                         mu_guess[i], 
                         sigma_guess);
```
:::

---

## PDS kernel for vagueness

```stan
model {
  v ~ normal(0.0, 1.0);
  target += normal_lpdf(y | 
              normal_cdf(v, -0.0, 1.0), 
              sigma);
}
```

::: {.fragment}
The kernel captures likelihood via `normal_cdf`
:::

---

## Full vagueness model

```stan {style="font-size: 75%;" code-line-numbers="8,11"}
transformed parameters {
  for (i in 1:N_data) {
    // Add participant adjustment
    real threshold_logit = mu_guess0[item[i]] + 
                          epsilon_mu_guess[participant[i]];
    mu_guess[i] = inv_logit(threshold_logit);
    
    // PDS KERNEL: P(adjective applies)
    response_rel[i] = 1 - normal_cdf(d[item[i]] | 
                                     mu_guess[i], 
                                     sigma_guess);
  }
}
```

---




# Summary

## The complete pipeline

```{style="text-align: center;"}
Semantic theory (λ-terms)
     ↓ Delta rules
Simplified form
     ↓ Compilation
Kernel model (Stan)
     ↓ Augmentation
Full statistical model
```

---

## Key insights

::: {.incremental}
- **Delta rules** bridge theory and computation
- **Kernel models** isolate semantic content
- **Augmentation** adds statistical machinery
- **Compilation** embodies linking hypotheses
:::

---

## What we've covered

::: {.incremental}
- **Degree inference** from norming questions
- **Vagueness** as threshold uncertainty
- **Likelihood judgments** via probabilistic computation
- **Full delta rules** when applying transformations
:::

---

# Future Directions

## Automated augmentation

Currently analysts manually add:
- Random effects
- Hierarchical priors
- Measurement models

::: {.fragment}
**Goal**: Automate common statistical patterns
:::

---

## Formally verified compilation

::: {.incremental}
- Delta rules with Agda/Coq proofs
- Ensure semantic preservation
- Enable generic transformations
- Build trust in compilation
:::

---

## Richer kernel models

Extend PDS to directly output:
- Mixture models
- Censoring mechanisms
- Complex statistical structures

::: {.fragment}
**Key**: Encode more semantic theory in kernels
:::

---

## Community contributions

The modular PDS design invites:
- New lexical entries
- Domain-specific delta rules
- Alternative compilation strategies
- Cross-linguistic extensions

---

## Conclusion

::: {.incremental}
- PDS bridges formal semantics and statistical modeling
- Delta rules enable computational tractability
- Kernel models preserve theoretical clarity
- This approach scales to complex phenomena
:::

::: {.fragment}
Next: How this handles factivity—where gradience poses even deeper theoretical puzzles
:::