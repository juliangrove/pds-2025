---
title: "From PDS to Stan"
bibliography: ../../pds.bib
format:
  revealjs:
    css: ../styles.css
    html-math-method: mathjax
    mathjax-config:
      loader: {load: ['[tex]/bussproofs','[tex]/bbox','[tex]/colorbox']}
      tex:
        packages: {'[+]': ['bussproofs','bbox','colorbox']}
---

::: {.hidden}
$$
\newcommand{\expr}[3]{\begin{array}{c}
#1 \\
\bbox[lightblue,5px]{#2}
\end{array} ⊢ #3}
\newcommand{\ct}[1]{\bbox[font-size: 0.8em]{\mathsf{#1}}}
\newcommand{\updct}[1]{\ct{upd\_#1}}
\newcommand{\abbr}[1]{\bbox[transform: scale(0.95)]{\mathtt{#1}}}
\newcommand{\pure}[1]{\bbox[border: 1px solid orange]{\bbox[border: 4px solid transparent]{#1}}}
\newcommand{\return}[1]{\bbox[border: 1px solid black]{\bbox[border: 4px solid transparent]{#1}}}
\def\P{\mathtt{P}}
\def\Q{\mathtt{Q}}
\def\True{\ct{T}}
\def\False{\ct{F}}
\def\ite{\ct{if\_then\_else}}
\def\Do{\abbr{do}}
$$
:::

# Probabilistic programs in Stan

## From PDS to Stan

Having seen how gradable adjectives work in compositional semantics, we now face a practical challenge:

- How do we test semantic theories against experimental data?
- How do we translate abstract λ-terms to statistical models?

::: {.fragment}
### Our approach

PDS automates the translation from compositional semantics to statistical kernels, which we implement in Stan.
:::

---

## The compilation challenge

::: {.incremental}
- **Abstract semantic theory**: λ-terms, type theory, compositional rules
- **Concrete statistical models**: Parameters, likelihoods, inference
- **The gap**: Need to make linking hypotheses explicit
:::

::: {.fragment}
PDS bridges this gap by automating the core translation while leaving room for statistical expertise
:::

---

## What PDS produces

PDS outputs a **kernel model**—the semantic core from lexical/compositional semantics

::: {.fragment}
### Example: Kernel vs Full Model

::: {.columns}
::: {.column width="45%"}
**PDS Kernel:**
```stan
model {
  w ~ normal(0.0, 1.0);
  target += normal_lpdf(y | w, sigma);
}
```
:::

::: {.column width="45%"}
**Full Model:**
```stan {code-line-numbers="2,5"}
model {
  w ~ normal(0.0, 1.0);          // PDS
  sigma_e ~ beta(2, 10);         // Analyst
  z_eps ~ std_normal();          // Analyst
  target += normal_lpdf(y | w + eps, sigma_e);
}
```
:::
:::
:::

---

## Why Stan?

Stan is a probabilistic programming language designed for statistical inference

::: {.incremental}
- **Declarative**: We specify probability models, not procedures
- **Specialized**: Designed for Bayesian inference via HMC
- **Parallel to semantics**: Both declare "what" not "how"
  - Semantics: truth conditions
  - Stan: probabilistic relationships
:::

---

## Stan's block structure - Part 1

```stan
data {
  // What we observe from experiments
  int<lower=1> N;           // number of observations
  vector[N] y;              // responses
}

parameters {
  // What we want to learn
  real mu;                  // mean parameter
  real<lower=0> sigma;      // standard deviation
}
```

::: {.fragment}
Clear separation between what's given and what's inferred
:::

---

## Stan's block structure - Part 2

```stan
transformed parameters {
  // Derived from parameters
  real precision = 1 / sigma^2;  // for convenience
}

model {
  // How data relates to parameters
  mu ~ normal(0, 10);      // prior
  sigma ~ exponential(1);   // prior
  y ~ normal(mu, sigma);    // likelihood
}
```

---

## Why this structure matters

::: {.incremental}
- **Separates observed from inferred**: Makes assumptions explicit
- **Enables optimization**: Stan knows what can be precomputed
- **Forces clarity**: Must specify every relationship
- **Matches PDS philosophy**: Declarative, not procedural
:::

---

# Case Study: Norming Model

## Our first example

::: {.incremental}
- **Question**: "How tall is Jo?"
- **Task**: Participants report degrees on scales
- **Goal**: Infer the "true" degree for each item
:::

::: {.fragment}
This seemingly simple task will reveal the complexity of the PDS-to-Stan pipeline
:::

---

## The experimental data

```{style="font-size: 85%;"}
| participant | item | adjective | condition | response |
|-------------|------|-----------|-----------|----------|
| 1 | tall_high | tall | high | 0.82 |
| 1 | wide_low | wide | low | 0.34 |
| 1 | expensive_mid | expensive | mid | 0.62 |
| 1 | full_high | full | high | 1.00 |
```

::: {.fragment}
- **Items** = adjective × condition pairs
- **Responses** on 0-1 scale (slider)
- **Challenge**: Handle boundary responses (0s and 1s)
:::

---

## Building the data block - counts

```stan
data {
  int<lower=1> N_item;        // number of items
  int<lower=1> N_participant; // number of participants  
  int<lower=1> N_data;        // number of data points
```

::: {.fragment}
### Why constraints?

- `<lower=1>` ensures valid data
- Helps Stan optimize algorithms
- Catches errors early
:::

---

## Handling boundary responses

```stan
  // Censoring at boundaries
  int<lower=1> N_0;           // number of 0s
  int<lower=1> N_1;           // number of 1s
```

::: {.fragment}
### Why separate these?

- Slider endpoints are special
- 0 might mean "even less than 0"
- 1 might mean "even more than 1"
- Censored data needs special treatment
:::

---

## Response data and indexing

```stan
  vector<lower=0, upper=1>[N_data] y; // responses in (0,1)
  
  // Which item/participant for each response
  array[N_data] int<lower=1, upper=N_item> item;
  array[N_data] int<lower=1, upper=N_participant> participant;
}
```

::: {.fragment}
These arrays are lookup tables: `item[5] = 3` means the 5th response is about item #3
:::

---

## Parameters: What we want to learn

```stan
parameters {
  // Semantic parameters - what PDS cares about
  vector[N_item] mu_guess;    // degree for each item
```

::: {.fragment}
This is the core semantic content: each item has a "true" degree on its scale
:::

---

## Random effects structure

```stan
  // How much people vary
  real<lower=0> sigma_epsilon_guess;
  
  // Each person's deviation (z-scores)
  vector[N_participant] z_epsilon_guess;
```

::: {.fragment}
### Non-centered parameterization

- Separate scale (`sigma`) from standardized deviations (`z`)
- Helps Stan's algorithms converge
- Mathematically equivalent but computationally superior
:::

---

## Measurement and censoring

```stan
  real<lower=0,upper=1> sigma_e;  // response noise
  
  // True values for censored data
  array[N_0] real<upper=0> y_0;    // true values for 0s
  array[N_1] real<lower=1> y_1;    // true values for 1s
}
```

::: {.fragment}
We infer what the "true" values might have been beyond scale boundaries
:::

---

## Transformed parameters

```stan
transformed parameters {
  // Convert z-scores to actual effects
  vector[N_participant] epsilon_guess = 
    sigma_epsilon_guess * z_epsilon_guess;
```

::: {.fragment}
If `sigma = 0.2` and `z[3] = 1.5`, then participant 3 gives responses 0.3 units higher than average
:::

---

## Computing predictions

```stan
  vector[N_data] guess;
  for (i in 1:N_data) {
    guess[i] = mu_guess[item[i]] + 
               epsilon_guess[participant[i]];
  }
}
```

::: {.fragment}
### Example trace
- Response i: item 5, participant 3
- `mu_guess[5] = 0.7` (item's degree)
- `epsilon_guess[3] = 0.1` (participant's adjustment)
- `guess[i] = 0.7 + 0.1 = 0.8`
:::

---

## The model block

```stan
model {
  // Priors
  sigma_epsilon_guess ~ exponential(1);
  z_epsilon_guess ~ std_normal();
  
  // Likelihood
  for (i in 1:N_data) {
    y[i] ~ normal(guess[i], sigma_e);
  }
}
```

---

# PDS to Stan Compilation

## PDS code for degree questions

```haskell
s1' = termOf $ getSemantics @Adjectives 1 
      ["jo", "is", "a", "soccer player"]
q1' = termOf $ getSemantics @Adjectives 0 
      ["how", "tall", "jo", "is"]
discourse' = ty tau $ assert s1' >>> ask q1'
```

::: {.fragment}
This seemingly simple code triggers a complex compilation process
:::

---

## Special lexical entry for degrees

```haskell
"tall" -> [ SynSem {
  syn = (S :\: NP) :/: (S :/: NP :/: Deg),
  sem = ty tau (lam s (purePP (lam f (lam x (lam i 
    (f @@ (sCon "height" @@ i @@ x) @@ x @@ i)))))
} ]
```

::: {.fragment}
- Takes a degree-to-proposition function `f`
- Applies it to the entity's height
- Special syntax for degree questions
:::

---

## The 'how' operator

```haskell
"how" -> [ SynSem {
  syn = S :/: (S :\: Deg),
  sem = ty tau (purePP (lam p (lam i 
    (Return (iota (lam d (p @@ d @@ i)))))))
} ]
```

::: {.fragment}
The $\iota$ (iota) operator extracts the unique degree satisfying the predicate
:::

---

## Initial semantic representation

After parsing "how tall is Jo?":

$$\ct{Return}(\iota(\lambda d[\ct{height}(i)(j) = d]))$$

::: {.fragment}
This complex λ-term needs simplification before it can become Stan code
:::

---

## Delta rules: What are they?

Delta rules are partial functions that simplify terms while preserving meaning

::: {.incremental}
- Type: `Term -> Maybe Term`
- Preserve semantic equivalence
- Enable computational tractability
- Defined in [`Lambda.Delta`](https://juliangrove.github.io/pds/Lambda-Delta.html)
:::

---

## The indices delta rule

```haskell
indices :: DeltaRule
indices = \case
  Height (UpdHeight f _) -> Just (f @@ j)
  Height i               -> Just (lookupHeight i j)
  _                      -> Nothing
```

::: {.fragment}
Extracts the height value from the discourse state for entity j
:::

---

## First reduction step

Apply the indices rule:

$$\ct{Return}(\iota(\lambda d[\ct{height}(i)(j) = d]))$$

::: {.fragment}
$$\downarrow \text{ indices}$$

$$\ct{Return}(\iota(\lambda d[h = d]))$$

where $h$ is Jo's actual height at index $i$
:::

---

## Iota extraction

The $\iota$ operator finds the unique value satisfying the equation:

$$\ct{Return}(\iota(\lambda d[h = d]))$$

::: {.fragment}
$$\downarrow \text{ iota}$$

$$\ct{Return}(h)$$

Final form: just the height value wrapped in Return
:::

---

## Key transformations

::: {.incremental}
1. **Degree extraction → Parameter inference**
   - $\iota(\lambda d[...])$ becomes a Stan parameter to infer

2. **Functions → Arrays**
   - $\ct{height} : \iota \to e \to r$ becomes `height[person]`

3. **Monadic structure → Target**
   - `Return` values become parameters or data
   - Bind operations become likelihood statements
:::

---

## PDS kernel output

```stan
model {
  w ~ normal(0.0, 1.0);
  target += normal_lpdf(y | w, sigma);
}
```

::: {.fragment}
::: {style="font-size: 85%;"}
Actual PDS output: `model { w ~ normal(0.0, 1.0); target += normal_lpdf(y | w, sigma); }`
:::

- `w` = degree parameter
- Prior on degrees + likelihood of observations
:::

---

## From kernel to full model

```stan {code-line-numbers="2,6"}
model {
  mu_guess ~ normal(0.0, 1.0);      // PDS kernel
  sigma_epsilon_guess ~ exponential(1);  // Analyst
  sigma_e ~ beta(2, 10);            // Analyst
  z_epsilon_guess ~ std_normal();   // Analyst
  y ~ normal(mu_guess[item] + epsilon_guess[participant], 
             sigma_e);              // PDS kernel + modifications
}
```

---

# Gradable Adjectives and Vagueness

## From degrees to judgments

New question: "Is Jo tall?"

::: {.incremental}
- No longer asking for a degree
- Binary judgment with vagueness
- Requires comparing degree to threshold
- **Vagueness** = threshold uncertainty
:::

---

## PDS entry for 'tall'

```haskell
"tall" -> [ SynSem {
  syn = AP,
  sem = ty tau (lam s (purePP (lam x (lam i 
    (sCon "(≥)" @@ (sCon "height" @@ i @@ x) @@ 
     (sCon "d_tall" @@ s))))) @@ s))
} ]
```

::: {.fragment}
Different from degree-question version!
:::

---

## Semantic components

::: {.incremental}
- $\ct{height}$ : $\iota \to e \to r$
  - Maps entities to their heights
- $\ct{d\_tall}$ : $\sigma \to r$  
  - Extracts threshold from discourse state
- $\ct{(≥)}$ : $r \to r \to t$
  - Comparison operator
:::

---

## Initial representation

"Jo is tall" parses to:

$$\ct{(≥)}(\ct{height}(i)(j))(\ct{d\_tall}(s))$$

::: {.fragment}
We need to reduce this step by step using delta rules
:::

---

## The states delta rule

```haskell
states :: DeltaRule
states = \case
  D_tall (UpdD_tall d _) -> Just d
  D_tall s               -> Just (lookupD_tall s)
  _                      -> Nothing
```

::: {.fragment}
Extracts the threshold for "tall" from the discourse state
:::

---

## Step-by-step reduction

Extract threshold:

$$\ct{(≥)}(\ct{height}(i)(j))(\ct{d\_tall}(s))$$

::: {.fragment}
$$\downarrow \text{ states}$$

$$\ct{(≥)}(\ct{height}(i)(j))(d)$$

where $d$ is the contextual threshold for tallness
:::

---

## Continue reduction

Extract height:

$$\ct{(≥)}(\ct{height}(i)(j))(d)$$

::: {.fragment}
$$\downarrow \text{ indices}$$

$$\ct{(≥)}(h)(d)$$

where $h$ is Jo's height
:::

---

## Arithmetic rule

```haskell
arithmetic :: DeltaRule
arithmetic = \case
  GE (DCon x) (DCon y) -> Just (if x >= y then Tr else Fa)
  GE x y               -> Nothing  -- can't reduce symbolic
  _                    -> Nothing
```

::: {.fragment}
Without concrete values, the comparison stays symbolic
:::

---

## Probabilistic structure

Since we can't reduce symbolically, wrap in distributions:

```haskell
d ~ Normal(mu, sigma)      -- threshold distribution
h ~ Normal(mu_h, sigma_h)  -- height distribution
Return((≥)(h, d))          -- comparison
```

::: {.fragment}
This captures vagueness through threshold uncertainty
:::

---

## Monadic compilation

The bind operator sequences computations:

```stan
target += normal_lpdf(d | mu, sigma);
target += normal_lpdf(h | mu_h, sigma_h);
target += bernoulli_lpmf(y | h >= d);
```

::: {.fragment}
Each line adds to the log probability
:::

---

## PDS compilation details

**Input PDS:**
```haskell
s1 = termOf $ getSemantics @Adjectives 1 ["jo", "is", "tall"]
discourse = ty tau $ assert s1
```

::: {.fragment}
**Kernel output:**
```stan
model {
  mu ~ normal(0.5, 0.2);      // threshold location
  sigma ~ exponential(5);      // vagueness
  target += normal_cdf_log(d | mu, sigma);
}
```
:::

---

# Likelihood Judgments

## Metalinguistic judgments

"How likely is Jo tall?"

::: {.incremental}
- Not binary true/false
- Gradient likelihood judgment
- Requires probabilistic reasoning
- Marginalizes over uncertainty
:::

---

## The likely operator

```haskell
"likely" -> [ SynSem {
  syn = S :/: S,
  sem = ty tau (lam s (purePP (lam p (lam i 
    (Return (normalCDF (p @@ i) 0.5 0.1))))))
} ]
```

::: {.fragment}
Maps propositions to probabilities via `normalCDF`
:::

---

## Probabilistic delta rules

```haskell
probabilities :: DeltaRule
probabilities = \case
  NormalCDF Tr mu sigma  -> Just (normalCDF 1.0 mu sigma)
  NormalCDF Fa mu sigma  -> Just (normalCDF 0.0 mu sigma)
  NormalCDF (GE x y) _   -> Nothing  -- stays symbolic
  _                      -> Nothing
```

---

## Marginalization insight

Key idea: marginalize over uncertainty

::: {.incremental}
- If $h \sim N(\mu_h, \sigma_h)$ (height)
- And $d \sim N(\mu_d, \sigma_d)$ (threshold)
- Then $P(h \geq d)$ has a closed form!
:::

---

## Marginalization delta rule

```haskell
marginalize :: DeltaRule
marginalize = \case
  Let x (Normal mu_x sigma_x) 
    (Let y (Normal mu_y sigma_y) 
      (Return (GE (Var x) (Var y)))) ->
        Just (Return (NormalCDF 0 (mu_x - mu_y) 
                       (sqrt (sigma_x^2 + sigma_y^2))))
  _ -> Nothing
```

---

## Compiled to Stan

```stan
real p = normal_cdf((mu_h - mu_d) / 
                    sqrt(sigma_h^2 + sigma_d^2) | 0, 1);
target += normal_lpdf(y | p, 0.1);
```

::: {.fragment}
The probability of "tall" becomes a parameter itself
:::

---

# Relative vs Absolute Standards

## Theoretical distinction

::: {.columns}
::: {.column width="45%"}
**Relative adjectives**
- "tall": varies by context
- Basketball player vs child
- Standard shifts dramatically
:::

::: {.column width="45%"}
**Absolute adjectives**
- "full": anchored to endpoint
- Glass is full or not
- Standard relatively fixed
:::
:::

---

## Relative adjective entry

```haskell
"tall" -> [ SynSem {
  sem = ... (sCon "d_tall" @@ s @@ 
            (sCon "context" @@ i)) ...
} ]
```

::: {.fragment}
Threshold depends on context - extra argument!
:::

---

## Absolute adjective entry

```haskell
"full" -> [ SynSem {
  sem = ... (sCon "d_full" @@ s) ...
} ]
```

::: {.fragment}
Fixed threshold - no context dependence
:::

---

## Different delta rules

```haskell
-- Context-dependent (relative)
contextual = \case
  D_tall ctx (UpdD_tall f s) -> Just (f @@ ctx)
  
-- Fixed (absolute)
fixed = \case
  D_full (UpdD_full d s) -> Just d
```

---

## Mixture model insight

Discrete semantics → gradient behavior

$$P(\text{response}) = \pi \cdot P(\text{relative}) + (1-\pi) \cdot P(\text{absolute})$$

::: {.fragment}
- $\pi$ = probability of relative interpretation
- Learn which interpretation dominates
- Different adjectives may have different $\pi$
:::

---

# Summary

## The complete pipeline

```{style="text-align: center;"}
Semantic theory (λ-terms)
     ↓ Delta rules
Simplified form
     ↓ Compilation
Kernel model (Stan)
     ↓ Augmentation
Full statistical model
```

---

## Key insights

::: {.incremental}
- **Delta rules** bridge theory and computation
- **Kernel models** isolate semantic content
- **Augmentation** adds statistical machinery
- **Compilation** embodies linking hypotheses
:::

---

## What we've covered

::: {.incremental}
- **Degree inference** via iota operator
- **Vagueness** as threshold uncertainty
- **Likelihood** via marginalization
- **Relative/absolute** via context dependence
:::

---

## Future directions

::: {.incremental}
- **Automated augmentation**: Add random effects automatically
- **Verified compilation**: Prove delta rules preserve semantics
- **Richer kernels**: Output mixture models directly
- **Community growth**: Contribute lexical entries, delta rules
:::

::: {.fragment}
Next: How this approach handles factivity—where gradience poses even deeper puzzles
:::