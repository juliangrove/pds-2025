---
title: "Stan/Adjectives"
bibliography: ../../pds.bib
format:
  revealjs:
    css: ../styles.css
    html-math-method: mathjax
    mathjax-config:
      loader: {load: ['[tex]/bussproofs','[tex]/bbox','[tex]/colorbox']}
      tex:
        packages: {'[+]': ['bussproofs','bbox','colorbox']}
---

# Probabilistic programs in Stan

## From PDS to Stan

Having seen how gradable adjectives work in compositional semantics, we now face a practical challenge:

- How do we test semantic theories against experimental data?
- How do we translate abstract λ-terms to statistical models?

::: {.fragment}
### Our approach

PDS automates the translation from compositional semantics to statistical kernels, which we implement in Stan.
:::

---

## PDS outputs and kernel models

PDS produces a **kernel model**—the semantic core from lexical/compositional semantics

::: {.fragment}
### In practice

::: {style="font-size: 85%;"}
- **Highlighted lines** = kernel model from PDS (semantic core)
- **Non-highlighted lines** = analyst additions (random effects, priors)
:::

```stan {.line-numbers highlight="2,5"}
model {
  mu_guess ~ normal(0.0, 1.0);      // PDS kernel
  sigma_e ~ beta(2, 10);            // Analyst addition
  z_epsilon_guess ~ std_normal();   // Analyst addition
  y ~ normal(mu_guess, sigma_e);    // PDS kernel (modified)
}
```
:::

---

## Why Stan?

Stan is a probabilistic programming language for statistical inference

::: {.incremental}
- **Declarative**: We specify probability models, not procedures
- **Specialized**: Designed for Bayesian inference via HMC
- **Parallel to semantics**: Both declare "what" not "how"
  - Semantics: truth conditions
  - Stan: probabilistic relationships
:::

---

## The PDS implementation

From `Grammar.Lexica.SynSem.Adjectives`:

```haskell
"tall" -> [ SynSem {
    syn = AP,
    sem = ty tau (lam s (purePP (lam x (lam i 
      (sCon "(≥)" @@ (sCon "height" @@ i @@ x) @@ 
       (sCon "d_tall" @@ s))))) @@ s))
} ]
```

::: {.fragment}
### Key components

- `sCon "height"`: measure function
- `sCon "d_tall"`: contextual threshold
- `sCon "(≥)"`: comparison relation
- Type: $(e → (i → t))$
:::

---

## Theoretical insights encoded

The lexical entry captures:

1. **Syntactic type**: `AP` (adjective phrase)
2. **Semantic computation**:
   - Takes discourse state `s` (with thresholds)
   - Returns function from entities to propositions
   - True when entity's measure exceeds threshold

::: {.fragment}
This implements degree-based semantics (@kennedy_vagueness_2007)
:::

---

## Structure of a Stan program

Stan programs have a specific architecture:

```stan
data {
  // What we observe
}
parameters {
  // What we want to infer
}
transformed parameters {
  // Derived quantities
}
model {
  // Priors and likelihood
}
generated quantities {
  // Post-inference computations
}
```

---

## Mapping lambda-terms to Stan code

### The challenge

Several theoretical issues arise in translation:

::: {.incremental}
1. **Higher-order functions**: Degree questions involve functions over functions
2. **Dynamic effects**: Free variables resolved dynamically
3. **Anaphora resolution**: Dependencies create static compilation challenges
:::

::: {.fragment}
Each approach has trade-offs—marginalization, fixing resolution, or parameter expansion.
:::

---

## Case study: Scale norming

Let's implement a baseline model for "How tall is Jo?"

### The experiment

- Participants see items with adjectives
- Provide slider responses (0 to 1)
- We model: What degree does each item have?

::: {.fragment}
```csv
participant,item,response
1,"tall_high",0.82
1,"wide_low",0.34
```
:::

---

## The data block

```stan
data {
  int<lower=1> N_item;        // number of items
  int<lower=1> N_participant; // number of participants  
  int<lower=1> N_data;        // number of data points
  
  // Boundary responses (censored)
  int<lower=1> N_0;           // responses at 0
  int<lower=1> N_1;           // responses at 1
  
  // Actual responses
  vector<lower=0, upper=1>[N_data] y;
  
  // Indexing arrays
  array[N_data] int<lower=1, upper=N_item> item;
  array[N_data] int<lower=1, upper=N_participant> participant;
}
```

---

## The parameters block

```stan
parameters {
  // SEMANTIC PARAMETERS
  vector<lower=0, upper=1>[N_item] mu_guess;  // degree for each item
  
  // RESPONSE NOISE
  real<lower=0> sigma_e;                       // measurement error
  
  // RANDOM EFFECTS
  real<lower=0> sigma_epsilon_guess;          // participant variation
  vector[N_participant] z_epsilon_guess;       // z-scores
}
```

::: {.fragment}
Note the separation: semantic parameters vs. statistical machinery
:::

---

## The model block

```stan {.line-numbers highlight="6,13"}
model {
  // PRIORS (analyst-added)
  sigma_epsilon_guess ~ exponential(1);
  sigma_e ~ beta(2, 10);
  
  // FIXED EFFECTS (PDS kernel)
  mu_guess ~ normal(0.0, 1.0);
  
  // RANDOM EFFECTS (analyst-added)
  z_epsilon_guess ~ std_normal();
  
  // LIKELIHOOD (PDS kernel with modifications)
  y[i] ~ normal(mu_guess[item[i]] + epsilon_guess[participant[i]], 
                sigma_e);
}
```

---

## PDS compilation example

PDS code for scale norming:

```haskell
s1' = termOf $ getSemantics @Adjectives 1 
      ["jo", "is", "a", "soccer player"]
q1' = termOf $ getSemantics @Adjectives 0 
      ["how", "tall", "jo", "is"]
discourse' = ty tau $ assert s1' >>> ask q1'
```

::: {.fragment}
This generates the kernel:

```stan
model {
  w ~ normal(0.0, 1.0);          // degree parameter
  target += normal_lpdf(y | w, sigma);  // likelihood
}
```
:::

---

## Degree questions in PDS

Special lexical entry for degree questions:

```haskell
-- Degree-question version
"tall" -> [ SynSem {
    syn = (S :\: NP) :/: (S :/: NP :/: Deg),
    sem = ty tau (lam s (purePP (lam f (lam x (lam i 
      (f @@ (sCon "height" @@ i @@ x) @@ x @@ i)))))
} ]
```

::: {.fragment}
"How" provides the degree-extracting function:

```haskell
"how" -> [ SynSem {
    syn = S :/: (S :\: Deg),
    sem = ty tau (purePP (lam p (lam i 
      (Return (iota (lam d (p @@ d @@ i)))))))
} ]
```
:::

---

## Delta rules and simplification

PDS applies delta rules to simplify complex λ-terms:

- Preserve semantic equivalence
- Enable different computational strategies
- Critical for tractable Stan code

::: {.fragment}
Example: From nested lambdas to simple degree extraction

$$\text{how}(\text{tall}(jo)) \xrightarrow{\delta} \text{height}(jo)$$
:::

---

## Model 2: Vagueness and imprecision

More sophisticated model capturing theoretical distinctions:

- **Vagueness**: uncertainty about standards
- **Imprecision**: tolerance around thresholds

::: {.fragment}
### New parameters

```stan
parameters {
  vector<lower=0, upper=1>[N_item] d;    // item degrees
  real<lower=0> epsilon;                 // global vagueness
  vector<lower=0>[N_adjective] epsilon_adj;  // adj-specific
}
```
:::

---

## The complete pipeline

::: {style="text-align: center;"}
```
Compositional Semantics (Haskell)
           ↓
      PDS Compilation
           ↓
    Kernel Model (Stan)
           ↓
   Analyst Augmentation
           ↓
    Full Statistical Model
           ↓
      Inference & Testing
```
:::

---

## Key insights

### Theoretical

- Maintains compositionality while adding probabilistic structure
- Separates semantic core from statistical machinery
- Forces explicit linking hypotheses

::: {.fragment}
### Practical

- PDS automates theory-to-kernel translation
- Analysts add domain-specific structure
- Gap between kernel and implementation = active research
:::

---

## Summary

- Stan provides the computational framework for testing semantic theories
- PDS translates λ-terms to statistical kernels
- The translation forces us to be explicit about:
  - Semantic competence vs. performance
  - Linking hypotheses
  - Statistical assumptions

::: {.fragment}
Next: How this approach handles a more challenging phenomenon—factivity
:::