---
title: "From PDS to Stan"
bibliography: ../../pds.bib
format:
  revealjs:
    css: ../styles.css
    html-math-method: mathjax
    mathjax-config:
      loader: {load: ['[tex]/bussproofs','[tex]/bbox','[tex]/colorbox']}
      tex:
        packages: {'[+]': ['bussproofs','bbox','colorbox']}
---

::: {.hidden}
$$
\newcommand{\expr}[3]{\begin{array}{c}
#1 \\
\bbox[lightblue,5px]{#2}
\end{array} ⊢ #3}
\newcommand{\ct}[1]{\bbox[font-size: 0.8em]{\mathsf{#1}}}
\newcommand{\updct}[1]{\ct{upd\_#1}}
\newcommand{\abbr}[1]{\bbox[transform: scale(0.95)]{\mathtt{#1}}}
\newcommand{\pure}[1]{\bbox[border: 1px solid orange]{\bbox[border: 4px solid transparent]{#1}}}
\newcommand{\return}[1]{\bbox[border: 1px solid black]{\bbox[border: 4px solid transparent]{#1}}}
\def\P{\mathtt{P}}
\def\Q{\mathtt{Q}}
\def\True{\ct{T}}
\def\False{\ct{F}}
\def\ite{\ct{if\_then\_else}}
\def\Do{\abbr{do}}
$$
:::

## The compilation challenge

::: {.incremental}
- **Semantic theory**: λ-terms, compositional rules
- **Statistical models**: Parameters, likelihoods, inference  
- **The gap**: Need explicit linking hypotheses
:::

::: {.fragment}
PDS automates the core translation while leaving room for statistical expertise
:::

---

## What PDS produces

- PDS outputs what we term a *kernel model*—the semantic core that corresponds directly to the lexical and compositional semantics
- This kernel can in principle be augmented with other components:
  - Random effects
  - Hierarchical priors  
  - Other statistical machinery
- The current implementation focuses on producing just the semantic kernel

---

## Why Stan?

- Stan is a probabilistic programming language designed for statistical inference
- Unlike general-purpose programming languages, Stan is specialized for stating distributional assumptions
- These assumptions are translated to a C++ backend that performs statistical inference
- Usually uses a form of Markov Chain Monte Carlo (MCMC)

---

## Why Stan?

::: {.incremental}
- **Declarative(-ish)**: We specify probability models, not procedures
- **Specialized**: Designed for Bayesian inference
- **Parallel to semantics**: Both declare "what" not "how"
  - Semantics: truth conditions
  - Stan: probabilistic relationships
:::

---

## Stan's block structure

```stan {style="font-size: 85%;"}
data {
  int<lower=1> N;           // observed
  vector[N] y;              
}
parameters {
  real mu;                  // inferred
  real<lower=0> sigma;      
}
model {
  mu ~ normal(0, 10);       // prior
  y ~ normal(mu, sigma);    // likelihood
}
```

::: {.fragment}
**Key idea**: This approach aligns well with our goals:

- Formal semantic analyses declare some representation of the semantic content
- Stan declares probabilistic relationships rather than sampling algorithms
:::

---

# Case Study: Norming Model

## Our first example

::: {.incremental}
- **Question**: "How tall is Jo?"
- **Task**: Participants report degrees on scales
- **Goal**: Infer the "true" degree for each item
:::

::: {.fragment}
This seemingly simple task will reveal the complexity of the PDS-to-Stan pipeline
:::

---

## The experimental data

:::{style="font-size: 85%;"}
| participant | item | adjective | condition | response |
|-------------|------|-----------|-----------|----------|
| 1 | tall_high | tall | high | 0.82 |
| 1 | wide_low | wide | low | 0.34 |
| 1 | expensive_mid | expensive | mid | 0.62 |
| 1 | full_high | full | high | 1.00 |
:::

::: {.fragment}
- **Items** = adjective × condition pairs
- **Responses** on 0-1 scale (slider)
- **Challenge**: Handle boundary responses (0s and 1s)
:::

---

## Building the data block - counts

```stan
data {
  int<lower=1> N_item;        // number of items
  int<lower=1> N_participant; // number of participants  
  int<lower=1> N_data;        // number of data points
```

::: {.fragment}
### Why constraints?

- `<lower=1>` ensures valid data
- Helps Stan optimize algorithms
- Catches errors early
:::

---

## Handling boundary responses

```stan
  // Censoring at boundaries
  int<lower=1> N_0;           // number of 0s
  int<lower=1> N_1;           // number of 1s
```

::: {.fragment}
### Why separate these?

- Slider endpoints are special
- 0 might mean "even less than 0"
- 1 might mean "even more than 1"
- Censored data needs special treatment
:::

---

## Response data and indexing

```stan
  vector<lower=0, upper=1>[N_data] y; // responses in (0,1)
  
  // Which item/participant for each response
  array[N_data] int<lower=1, upper=N_item> item;
  array[N_data] int<lower=1, upper=N_participant> participant;
}
```

::: {.fragment}
These arrays are lookup tables: `item[5] = 3` means the 5th response is about item #3
:::

---

## Parameters: What we want to learn

```stan
parameters {
  // Semantic parameters - what PDS cares about
  vector[N_item] mu_guess;    // degree for each item
```

::: {.fragment}
- These means represent our best guess, as researchers, about each item's true degree on its scale
- They capture the theoretical degrees that the semantic analysis posits
- Crucially, they can also be viewed as representing subjects' uncertainty about these degrees
- They reflect what unresolved uncertainty subjects maintain when they make these guesses
:::

---

## Random effects structure

```stan
  // How much people vary
  real<lower=0> sigma_epsilon_guess;
  
  // Each person's deviation (z-scores)
  vector[N_participant] z_epsilon_guess;
```

::: {.fragment}
### Non-centered parameterization

- Separate scale (`sigma`) from standardized deviations (`z`)
- Helps Stan's algorithms converge
- Mathematically equivalent but computationally superior
:::

---

## Measurement and censoring

```stan
  real<lower=0,upper=1> sigma_e;  // response noise
  
  // True values for censored data
  array[N_0] real<upper=0> y_0;    // true values for 0s
  array[N_1] real<lower=1> y_1;    // true values for 1s
}
```

::: {.fragment}
We infer what the "true" values might have been beyond scale boundaries
:::

---

## Transformed parameters

```stan
transformed parameters {
  // Convert z-scores to actual effects
  vector[N_participant] epsilon_guess = 
    sigma_epsilon_guess * z_epsilon_guess;
```

::: {.fragment}
If `sigma = 0.2` and `z[3] = 1.5`, then participant 3 gives responses 0.3 units higher than average
:::

---

## Computing predictions

```stan
  vector[N_data] guess;
  for (i in 1:N_data) {
    guess[i] = mu_guess[item[i]] + 
               epsilon_guess[participant[i]];
  }
}
```

::: {.fragment}
### Example trace
- Response i: item 5, participant 3
- `mu_guess[5] = 0.7` (item's degree)
- `epsilon_guess[3] = 0.1` (participant's adjustment)
- `guess[i] = 0.7 + 0.1 = 0.8`
:::

---

## The model block

```stan
model {
  // Priors
  sigma_epsilon_guess ~ exponential(1);
  z_epsilon_guess ~ std_normal();
  
  // Likelihood
  for (i in 1:N_data) {
    y[i] ~ normal(guess[i], sigma_e);
  }
}
```

---

# PDS to Stan Compilation

## The PDS implementation

```haskell
-- From sources/pds/src/
s1'        = termOf $ getSemantics @Adjectives 1 
             ["jo", "is", "a", "soccer player"]
q1'        = termOf $ getSemantics @Adjectives 0 
             ["how", "tall", "jo", "is"]
discourse' = ty tau $ assert s1' >>> ask q1'
scaleNormingExample = asTyped tau 
  (betaDeltaNormal deltaRules . 
   adjectivesRespond scaleNormingPrior) discourse'
```

::: {.fragment}
This establishes context and asks for Jo's height
:::

::: {.fragment style="font-size: 85%;"}
Note the use of certain convenience functions:

- `getSemantics` retrieves one of the meanings (in the λ-calculus) for the expression
- It takes a string of strings as input
- Uses the parser implemented at [`Grammar.Parser`](https://juliangrove.github.io/pds/Grammar-Parser.html)
:::

---

## Lexical entries for degree questions

```haskell {style="font-size: 80%;"}
-- From Grammar.Lexica.SynSem.Adjectives
"tall" -> [ SynSem {
  syn = AP :\: Deg,
  sem = ty tau (purePP (lam d (lam x (lam i 
    (sCon "(≥)" @@ (sCon "height" @@ i @@ x) @@ d)))))
  }, ... ]

"how" -> [ SynSem {
  syn = Qdeg :/: (S :/: AP) :/: (AP :\: Deg),
  sem = ty tau (purePP (lam x (lam y (lam z 
    (y @@ (x @@ z))))))
  }, ... ]
```

::: {.fragment}
The degree-argument version of adjectives enables degree questions
:::

---

## Compositional semantics

For *how tall is Jo?*, composition yields:

$$λd, i.\ct{height}(i)(j) ≥ d$$

::: {.fragment}
When $\abbr{respond}$ comes into the picture, some index $i^{\prime}$ is sampled from the common ground, and the maximal answer to the question is determined to be:

$$\ct{max}(λd.\ct{height}(i^{\prime})(j) ≥ d)$$
:::

---

## delta-rules: What are they?

- delta-rules enable different semantic computations while preserving semantic equivalence
- Formalism is strongly normalizing and confluent, so:
  - Order of rule application doesn't affect the final result
  - Our semantic theory remains consistent

::: {.incremental}
- Type: `Term -> Maybe Term`
- Preserve semantic equivalence
- Enable computational tractability
- Defined in [`Lambda.Delta`](https://juliangrove.github.io/pds/Lambda-Delta.html)
:::

---

## The `indices` and `maxes` delta-rules

```haskell {style="font-size: 80%;"}
-- From Lambda.Delta module
indices :: DeltaRule
indices = \case
  Height (UpdHeight p _) -> Just p
  Height (UpdSocPla _ i) -> Just (Height i)
  SocPla (UpdSocPla p _) -> Just p
  SocPla (UpdHeight _ i) -> Just (SocPla i)
  -- ... other cases for Ling, Epi
  _                      -> Nothing
```

::: {.fragment}
Extracts values from the discourse state
:::

```haskell
maxes :: DeltaRule
maxes = \case
   Max (Lam y (GE x (Var y'))) | y' == y -> Just x
   _                                     -> Nothing  
```

::: {.fragment}
Extracts the unique degree satisfying the inequality
:::

---

## delta-reduction walkthrough

For degree questions like *how tall is Jo?*, the compositional semantics produces:
$$\ct{max}(λd.\ct{height}(i^{\prime})(j) ≥ d)$$

::: {.fragment}
Apply `indices` rule to extract height:
$$\ct{max}(λd.h ≥ d)$$
where $h$ represents Jo's actual height at index $i$
:::

::: {.fragment}
Apply max extraction using the following delta-rule:
$$h$$
This final form directly corresponds to the Stan parameter we need to infer—the degree on the height scale.
:::

---

## Key transformations

- **Challenge:** translating abstract semantic computations into Stan's parameter space
- Translation embodies (some of) our linking hypothesis between semantic competence and performance

::: {.incremental}
1. **Degree extraction becomes parameter inference**
   - $\ct{max}(λd.\ct{height}(i)(j) ≥ d)$ → Infer parameter `height_jo`
   - The unique degree satisfying the equation becomes a parameter to estimate

:::

---

## Key transformations

::: {.fragment}
2. **Functions become arrays**
   - $\ct{height} : \iota \to e \to r$ → Array `height[person]`
   - Function application → Array indexing

3. **Propositions become probabilities**
   - Truth values → Real numbers in [0,1]
   - Logical operations → Probabilistic operations

4. **The monad becomes Stan's target**
   - The $\Do$-notation structures sequential computation
   - This determines Stan's log probability
:::

---

## PDS kernel output

The PDS system outputs the following kernel model:

```stan
model {
  // FIXED EFFECTS
  w ~ normal(0.0, 1.0);
  
  // LIKELIHOOD
  target += normal_lpdf(y | w, sigma);
}
```

::: {.fragment}
::: {style="font-size: 85%;"}
Actual PDS output: `model { w ~ normal(0.0, 1.0); target += normal_lpdf(y | w, sigma); }`
:::

Captures the essential degree-based semantics where `w` represents the degree on the height scale.
:::

---

## From kernel to full model

The full model with analyst augmentations looks like:

```stan {code-line-numbers="7,13"}
model {
  // PRIORS (analyst-added)
  sigma_epsilon_guess ~ exponential(1);
  sigma_e ~ beta(2, 10);
  
  // FIXED EFFECTS (PDS kernel)
  mu_guess ~ normal(0.0, 1.0);
  
  // RANDOM EFFECTS (analyst-added)
  z_epsilon_guess ~ std_normal();
  
  // LIKELIHOOD (PDS kernel with modifications)
  y[i] ~ normal(mu_guess[item[i]] + epsilon_guess[participant[i]], sigma_e);
}
```

Highlighted lines show the kernel model from PDS

---

## From kernel to full model

::: {.fragment}
- The unhighlighted portions add statistical machinery for real data:
  - Hierarchical priors
  - Random effects  
  - Indexed parameters
:::

---

# Modeling Vagueness

## From degrees to likelihood judgments

Our next model addresses how speakers reason about the likelihood that gradable adjectives apply: *how likely (is it) that Jo is tall?*

::: {.incremental}
- Metalinguistic judgment about adjective application
- Requires probabilistic reasoning
- **Key**: Threshold uncertainty creates vagueness
:::

---

## The PDS implementation

```haskell {style="font-size: 85%;"}
-- From sources/pds/src/
expr1 = ["jo", "is", "a", "soccer", "player"]
expr2 = ["how", "likely", "that", "jo", "is", "tall"]
s1 = getSemantics @Adjectives 0 expr1
q1 = getSemantics @Adjectives 0 expr2
discourse = ty tau $ assert s1 >>> ask q1
likelihoodExample = asTyped tau 
  (betaDeltaNormal deltaRules . 
   adjectivesRespond likelihoodPrior) discourse
```

---

## Lexical entries (Part 1)

```haskell {style="font-size: 75%;"}
-- From Grammar.Lexica.SynSem.Adjectives
"tall" -> [ SynSem {
    syn = AP,
    sem = ty tau (lam s (purePP (lam x (lam i 
      (sCon "(≥)" @@ (sCon "height" @@ i @@ x) @@ 
       (sCon "d_tall" @@ s)))) @@ s))
    } ]
```

::: {.fragment}
Key semantic components:

- `height`: $\iota \to e \to r$ (entity heights)
- `d_tall`: $\sigma \to r$ (contextual threshold)
- `(≥)`: $r \to r \to t$ (comparison)
:::

---

## Lexical entries (Part 2)

```haskell {style="font-size: 75%;"}
"likely" -> [ SynSem {
    syn = S :\: Deg :/: S,
    sem = ty tau (lam s (purePP (lam p (lam d (lam _' 
      (sCon "(≥)" @@ 
       (Pr (let' i (CG s) (Return (p @@ i)))) @@ 
       d)))) @@ s))
    } ]
```

::: {.fragment}
Compares probability of embedded proposition to a degree
:::

---

## delta-reduction for vagueness

Starting with *Jo is tall*:
$$\ct{(≥)}(\ct{height}(i)(j))(\ct{d\_tall}(s))$$

::: {.fragment}
Apply `states` rule to extract threshold:
$$\ct{(≥)}(\ct{height}(i)(j))(d)$$
:::

::: {.fragment}
Apply `indices` rule to extract height:
$$\ct{(≥)}(h)(d)$$
:::

---

## The states delta-rule

```haskell {style="font-size: 75%;"}
-- From Lambda.Delta (lines 167-183)
states :: DeltaRule
states = \case
  CG      (UpdCG cg _)     -> Just cg
  CG      (UpdDTall _ s)   -> Just (CG s)
  DTall   (UpdDTall d _)   -> Just d
  DTall   (UpdCG _ s)      -> Just (DTall s)
  QUD     (UpdQUD q _)     -> Just q
  QUD     (UpdCG _ s)      -> Just (QUD s)
  TauKnow (UpdTauKnow b _) -> Just b
  TauKnow (UpdCG _ s)      -> Just (TauKnow s)
  _                        -> Nothing
```

---

## The probabilities delta-rule

```haskell {style="font-size: 75%;"}
-- From Lambda.Delta (lines 156-164)
probabilities :: DeltaRule
probabilities = \case
  Pr (Return Tr)         -> Just 1
  Pr (Return Fa)         -> Just 0
  Pr (Bern x)            -> Just x
  Pr (Disj x t u)        -> Just (x * Pr t + (1 - x) * Pr u)
  Pr (Let v (Normal x y) (Return (GE t (Var v')))) 
    | v' == v -> Just (NormalCDF x y t)
  Pr (Let v (Normal x y) (Return (GE (Var v') t))) 
    | v' == v -> Just (NormalCDF (- x) y t)
  _                      -> Nothing
```

---

## Probabilistic computation

The system computes $P(h \geq d)$ when both are uncertain:

::: {.incremental}
- $h \sim \mathcal{N}(\mu_h, \sigma_h)$ (height)
- $d \sim \mathcal{N}(\mu_d, \sigma_d)$ (threshold)
- Result: `normal_cdf` computation in Stan
:::

::: {.fragment}
```stan
response = 1 - normal_cdf(d[item[i]] | 
                         mu_guess[i], 
                         sigma_guess);
```
:::

---

## PDS kernel for vagueness

```stan
model {
  v ~ normal(0.0, 1.0);
  target += normal_lpdf(y | 
              normal_cdf(v, -0.0, 1.0), 
              sigma);
}
```

::: {.fragment}
The kernel captures likelihood via `normal_cdf`
:::

---

## Full vagueness model

```stan {style="font-size: 75%;" code-line-numbers="9-11"}
transformed parameters {
  for (i in 1:N_data) {
    // Add participant adjustment
    real threshold_logit = mu_guess0[item[i]] + 
                          epsilon_mu_guess[participant[i]];
    mu_guess[i] = inv_logit(threshold_logit);
    
    // KEY SEMANTIC COMPUTATION: P(adjective applies)
    response_rel[i] = 1 - normal_cdf(d[item[i]] | 
                                     mu_guess[i], 
                                     sigma_guess);
  }
}
```

---




# Summary

## The complete pipeline

```{style="text-align: center;"}
Semantic theory (λ-terms)
     ↓ delta-rules
Simplified form
     ↓ Compilation
Kernel model (Stan)
     ↓ Augmentation
Full statistical model
```

---

## Key ideas

::: {.incremental}
- **delta-rules** bridge theory and computation
- **Kernel models** isolate semantic content from statistical machinery
- **This distinction is crucial**: 
  - PDS automates the translation from compositional semantics to the core statistical model...
  - while leaving room for analysts to add domain-specific statistical structure
- **Compilation** embodies our linking hypothesis between semantic competence and performance
:::

---

## What we've covered

::: {.incremental}
- **Degree inference** from norming questions using degree-argument adjectives
- **Vagueness** as threshold uncertainty in gradable adjectives
- **Likelihood judgments** via probabilistic computation with `normal_cdf`
- **Full delta-rule walkthroughs** showing how complex λ-terms become Stan parameters
:::

---

# Future Directions

## Automated augmentation

Currently analysts manually add:

- Random effects
- Hierarchical priors
- Measurement models

::: {.fragment}
**Goal**: Automate common statistical patterns
:::

---

## Formally verified compilation

::: {.incremental}
- delta-rules with Agda/Coq proofs
- Ensure semantic preservation
- Enable generic transformations
- Build trust in compilation
:::

---

## Richer kernel models

Extend PDS to directly output:
- Mixture models
- Censoring mechanisms
- Complex statistical structures

::: {.fragment}
**Key**: Encode more semantic theory in kernels
:::

---

## Community contributions

The modular PDS design invites:

- New lexical entries
- Domain-specific delta-rules
- Alternative compilation strategies
- Cross-linguistic extensions

---

## Conclusion

::: {.incremental}
- PDS bridges formal semantics and statistical modeling
- delta-rules enable computational tractability
- Kernel models preserve theoretical clarity
- This approach scales to complex phenomena
:::

::: {.fragment}
Next: How this handles factivity—where gradience poses even deeper theoretical puzzles
:::