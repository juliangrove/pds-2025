---
title: "Gradable adjectives"
bibliography: ../../pds.bib
format:
  revealjs:
    css: ../styles.css
    html-math-method: mathjax
    mathjax-config:
      loader: {load: ['[tex]/bussproofs','[tex]/bbox','[tex]/colorbox']}
      tex:
        packages: {'[+]': ['bussproofs','bbox','colorbox']}
---

::: {.hidden}
$$
\newcommand{\expr}[3]{\begin{array}{c}
#1 \\
\bbox[lightblue,5px]{#2}
\end{array} ‚ä¢ #3}
\newcommand{\ct}[1]{\bbox[font-size: 0.8em]{\mathsf{#1}}}
\newcommand{\updct}[1]{\ct{upd\_#1}}
\newcommand{\abbr}[1]{\bbox[transform: scale(0.95)]{\mathtt{#1}}}
\newcommand{\pure}[1]{\bbox[border: 1px solid orange]{\bbox[border: 4px solid transparent]{#1}}}
\newcommand{\return}[1]{\bbox[border: 1px solid black]{\bbox[border: 4px solid transparent]{#1}}}
\def\P{\mathtt{P}}
\def\Q{\mathtt{Q}}
\def\True{\ct{T}}
\def\False{\ct{F}}
\def\ite{\ct{if\_then\_else}}
\def\Do{\abbr{do}}
$$
:::

## Vague adjectives

- *tall*, *wide*, *expensive*, *happy*, ... 
  
(@ex-coffee) The coffee in Rome is [expensive]{.gb-orange}. [@kennedy_vagueness_2007]
  
- True if: the cost of coffee in Rome is as great as a salient threshold, [$d$]{.gb-orange}:
  $$c ‚â• d$$
- Maybe the cost of coffee is 3 euros.
- $d$ is [vague]{.gb-orange}... maybe it ranges somewhere from 2 euros to 4 euros.

---

## Vagueness: strange inference patterns

Vague adjectives such as *expensive*

- admit [borderline cases]{.gb-orange}: 
  - Mud Blend: $1.50/lb \ \ üôÅ
  - Organic Kona: $20/lb \ \ ü•∞
  - Swell Start Blend: $9.25/lb ??

---

## Vagueness: strange inference patterns

- produce [sorites paradoxes]{.gb-orange}:

---

## Sorites: $10
  
Is a $10 cup of coffee expensive? 

::: {.fragment}
![](images/hands1.jpg){width=800}
:::

---

## Sorites: $9.99
  
Take $0.01 away. Is a $9.99 cup of coffee expensive? 

::: {.fragment}
![](images/hands2.jpg){width=800}
:::

---

## Sorites: $9.98
  
Take $0.01 away. Is a $9.98 cup of coffee expensive? 

::: {.fragment}
![](images/hands3.jpg){width=800}
:::

---

## Vagueness: strange inference patterns

- produce [sorites paradoxes]{.gb-orange}: 
  - Premise 1: A $10 cup of coffee is expensive. 
  - Premise 2: If an expensive cup of coffee were 1 cent cheaper, it would still be expensive.
  - Conclusion: *Therefore*, a [free]{.gb-orange} cup of coffee is expensive!
  

---

## Why tricky?
Borderline cases and sorites paradoxes: 
inference patterns that do not fall into any traditional semantic classification. 

- Borderline cases:
  things to which the adjective neither applies nor doesn't apply. 
- The inference is not simply "on" or "off".
- The sorites paradox: troublesome because... 
  - inferences should be closed under implication: 
	if $10 ‚Üù $9.99, and $9.99 ‚Üù $9.98, then $10.00 ‚Üù $9.98. 
	- *a $10 cup is expensive* ‚Üù *a free cup is expensive*

---

## Inference judgments

Ongoing work with Helena Aparicio (Cornell Linguistics):

- Collect [scale norming]{.gb-orange} data. 
  - "X drank a coffee. Guess how [expensive]{.gb-orange} it was?" 
- Collect [adjectival inference]{.gb-orange} data. 
  - "X drank a coffee. How likely is it that it was [expensive]{.gb-orange}?" (vague) 

---

## Modeling pipeline

Using the norming data to help model the adjectival inference data: 

- Assume that people make likelihood judgments based on their probabilistic estimate of where an object lands on the adjective's scale.<br> 
- Then, we can model the distribution over these estimates *using* a model fit to the norming data.

---

## Norming task

<br>
<div style="text-align: center;">
<img src="images/expensive_norming.png" style="width: 60%;">
</div>

- 6 relative adjectives
- 3 possible contexts for each adjective ("high", "medium", "low")

---

## Norming data

![](images/norming_histograms.png)

---

## Adjectival inference task

<br>
<div style="text-align: center;">
<img src="images/expensive_test.png" style="width: 70%;">
</div>

- Same 6 adjectives (six relative, six absolute)
- Same 3 possible contexts for each adjective ("high", "medium", "low") 

---

## Adjectival inference data
  
::: {style="text-align: center;"}
![](images/expensive.png){height=200}
![](images/tall_wide.png){height=200}
:::

::: {style="text-align: center;"}
![](images/deep.png){height=200}
![](images/heavy_old.png){height=200}
$\scriptsize \text{Likelihood that adj. is true}$
:::

---

## The compilation challenge

::: {.incremental}
- **Semantic theory**: Œª-terms, compositional rules
- **Statistical models**: Parameters, likelihoods, inference  
- **The gap**: Need explicit linking hypotheses
:::

::: {.fragment}
PDS automates the core translation while leaving room for statistical expertise
:::

---

## What PDS produces

- PDS outputs what we term a *kernel model*‚Äîthe semantic core that corresponds directly to the lexical and compositional semantics
- This kernel can in principle be augmented with other components:
  - Random effects
  - Hierarchical priors  
  - Other statistical machinery
- The current implementation focuses on producing just the semantic kernel

---

## Why Stan?

- Stan is a probabilistic programming language designed for statistical inference
- Unlike general-purpose programming languages, Stan is specialized for stating distributional assumptions
- These assumptions are translated to a C++ backend that performs statistical inference
- Usually uses a form of Markov Chain Monte Carlo (MCMC)

---

## Why Stan?

::: {.incremental}
- **Declarative(-ish)**: We specify probability models, not procedures
- **Specialized**: Designed for Bayesian inference
- **Parallel to semantics**: Both declare "what" not "how"
  - Semantics: truth conditions
  - Stan: probabilistic relationships
:::

---

## Stan's block structure

```stan {style="font-size: 85%;"}
data {
  int<lower=1> N;           // observed
  vector[N] y;              
}
parameters {
  real mu;                  // inferred
  real<lower=0> sigma;      
}
model {
  mu ~ normal(0, 10);       // prior
  y ~ normal(mu, sigma);    // likelihood
}
```

::: {.fragment}
**Key idea**: This approach aligns well with our goals:

- Formal semantic analyses declare some representation of the semantic content
- Stan declares probabilistic relationships rather than sampling algorithms
:::

---

# Case Study: Norming Model

## Our first example

::: {.incremental}
- **Question**: "How tall is Jo?"
- **Task**: Participants report degrees on scales
- **Goal**: Infer the "true" degree for each item
:::

::: {.fragment}
This seemingly simple task will reveal the complexity of the PDS-to-Stan pipeline
:::

---

## The experimental data

:::{style="font-size: 85%;"}
| participant | item | adjective | condition | response |
|-------------|------|-----------|-----------|----------|
| 1 | tall_high | tall | high | 0.82 |
| 1 | wide_low | wide | low | 0.34 |
| 1 | expensive_mid | expensive | mid | 0.62 |
| 1 | full_high | full | high | 1.00 |
:::

::: {.fragment}
- **Items** = adjective √ó condition pairs
- **Responses** on 0-1 scale (slider)
- **Challenge**: Handle boundary responses (0s and 1s)
:::

---

## Building the data block - counts

```stan
data {
  int<lower=1> N_item;        // number of items
  int<lower=1> N_participant; // number of participants  
  int<lower=1> N_data;        // number of data points
```

::: {.fragment}
### Why constraints?

- `<lower=1>` ensures valid data
- Helps Stan optimize algorithms
- Catches errors early
:::

---

## Handling boundary responses

```stan
  // Censoring at boundaries
  int<lower=1> N_0;           // number of 0s
  int<lower=1> N_1;           // number of 1s
```

::: {.fragment}
### Why separate these?

- Slider endpoints are special
- 0 might mean "even less than 0"
- 1 might mean "even more than 1"
- Censored data needs special treatment
:::

---

## Response data and indexing

```stan
  vector<lower=0, upper=1>[N_data] y; // responses in (0,1)
  
  // Which item/participant for each response
  array[N_data] int<lower=1, upper=N_item> item;
  array[N_data] int<lower=1, upper=N_participant> participant;
}
```

::: {.fragment}
These arrays are lookup tables: `item[5] = 3` means the 5th response is about item #3
:::

---

## Parameters: What we want to learn

```stan
parameters {
  // Semantic parameters - what PDS cares about
  vector[N_item] mu_guess;    // degree for each item
```

::: {.fragment}
- These means represent our best guess, as researchers, about each item's true degree on its scale
- They capture the theoretical degrees that the semantic analysis posits
- Crucially, they can also be viewed as representing subjects' uncertainty about these degrees
- They reflect what unresolved uncertainty subjects maintain when they make these guesses
:::

---

## Random effects structure

```stan
  // How much people vary
  real<lower=0> sigma_epsilon_guess;
  
  // Each person's deviation (z-scores)
  vector[N_participant] z_epsilon_guess;
```

::: {.fragment}
### Non-centered parameterization

- Separate scale (`sigma`) from standardized deviations (`z`)
- Helps Stan's algorithms converge
- Mathematically equivalent but computationally superior
:::

---

## Measurement and censoring

```stan
  real<lower=0,upper=1> sigma_e;  // response noise
  
  // True values for censored data
  array[N_0] real<upper=0> y_0;    // true values for 0s
  array[N_1] real<lower=1> y_1;    // true values for 1s
}
```

::: {.fragment}
We infer what the "true" values might have been beyond scale boundaries
:::

---

## Transformed parameters

```stan
transformed parameters {
  // Convert z-scores to actual effects
  vector[N_participant] epsilon_guess = 
    sigma_epsilon_guess * z_epsilon_guess;
```

::: {.fragment}
If `sigma = 0.2` and `z[3] = 1.5`, then participant 3 gives responses 0.3 units higher than average
:::

---

## Computing predictions

```stan
  vector[N_data] guess;
  for (i in 1:N_data) {
    guess[i] = mu_guess[item[i]] + 
               epsilon_guess[participant[i]];
  }
}
```

::: {.fragment}
### Example trace
- Response i: item 5, participant 3
- `mu_guess[5] = 0.7` (item's degree)
- `epsilon_guess[3] = 0.1` (participant's adjustment)
- `guess[i] = 0.7 + 0.1 = 0.8`
:::

---

## The model block

```stan
model {
  // Priors
  sigma_epsilon_guess ~ exponential(1);
  z_epsilon_guess ~ std_normal();
  
  // Likelihood
  for (i in 1:N_data) {
    y[i] ~ normal(guess[i], sigma_e);
  }
}
```

---

# PDS to Stan Compilation

## The PDS implementation

```haskell
-- From sources/pds/src/
s1'        = termOf $ getSemantics @Adjectives 1 
             ["jo", "is", "a", "soccer player"]
q1'        = termOf $ getSemantics @Adjectives 0 
             ["how", "tall", "jo", "is"]
discourse' = ty tau $ assert s1' >>> ask q1'
scaleNormingExample = asTyped tau 
  (betaŒîNormal Œ¥Rules . 
   adjectivesRespond scaleNormingPrior) discourse'
```

::: {.fragment}
This establishes context and asks for Jo's height
:::

::: {.fragment style="font-size: 85%;"}
Note the use of certain convenience functions:

- `getSemantics` retrieves one of the meanings (in the Œª-calculus) for the expression
- It takes a string of strings as input
- Uses the parser implemented at [`Grammar.Parser`](https://juliangrove.github.io/pds/Grammar-Parser.html)
:::

---

## Lexical entries for degree questions

```haskell {style="font-size: 80%;"}
-- From Grammar.Lexica.SynSem.Adjectives
"tall" -> [ SynSem {
  syn = AP :\: Deg,
  sem = ty tau (purePP (lam d (lam x (lam i 
    (sCon "(‚â•)" @@ (sCon "height" @@ i @@ x) @@ d)))))
  }, ... ]

"how" -> [ SynSem {
  syn = Qdeg :/: (S :/: AP) :/: (AP :\: Deg),
  sem = ty tau (purePP (lam x (lam y (lam z 
    (y @@ (x @@ z))))))
  }, ... ]
```

::: {.fragment}
The degree-argument version of adjectives enables degree questions
:::

---

## Compositional semantics

For *how tall is Jo?*, composition yields:

$$Œªd, i.\ct{height}(i)(\ct{j}) ‚â• d$$

::: {.fragment}
When $\abbr{respond}$ comes into the picture, some index $i^{\prime}$ is sampled from the common ground, and the maximal answer to the question is determined to be:

$$\ct{max}(Œªd.\ct{height}(i^{\prime})(\ct{j}) ‚â• d)$$
:::

---

## Œ¥-rules: What are they?

```haskell
type DeltaRule = Term -> Maybe Term
```

- Œ¥-rules enable different semantic computations while preserving semantic equivalence
- Order of rule application doesn't affect the final result
- They should be [sound]{.gb-orange}
  - Meaning: the equalities they encode should be (mathematically) true equalities

---

## The `indices` and `maxes` Œ¥-rules

```haskell {style="font-size: 80%;"}
-- From Lambda.Œî module
indices :: DeltaRule
indices = \case
  Height (UpdHeight p _) -> Just p
  Height (UpdSocPla _ i) -> Just (Height i)
  SocPla (UpdSocPla p _) -> Just p
  SocPla (UpdHeight _ i) -> Just (SocPla i)
  -- ... other cases for Ling, Epi
  _                      -> Nothing
```

::: {.fragment}
Extracts values from the discourse state
:::

::: {.fragment}
```haskell
maxes :: DeltaRule
maxes = \case
   Max (Lam y (GE x (Var y'))) | y' == y -> Just x
   _                                     -> Nothing  
```
:::

::: {.fragment}
Extracts the unique degree satisfying the inequality
:::

---

## Œ¥-reduction walkthrough

For degree questions like *how tall is Jo?*, the compositional semantics produces:
$$\ct{max}(Œªd.\ct{height}(i^{\prime})(\ct{j}) ‚â• d)$$

::: {.fragment}
Apply `indices` rule to extract height:
$$\text{becomes}\,\,\, \ct{max}(Œªd.h ‚â• d)$$
where $h$ represents Jo's actual height at index $i$
:::

::: {.fragment}
Apply max extraction using the following Œ¥-rule: becomes $h$
:::

---

## Key transformations

- **Challenge:** translating abstract semantic computations into Stan's parameter space
- Translation embodies (some of) our linking hypothesis between semantic competence and performance

::: {.incremental}
1. **Degree extraction feeds parameter inference**
   - $\ct{max}(Œªd.\ct{height}(i)(\ct{j}) ‚â• d)$ ‚Üí Infer parameter `height_jo`
   - The unique degree satisfying the equation becomes a parameter to estimate

:::

---

## Key transformations

::: {.fragment}
2. **Functions become arrays**
   - $\ct{height} : \iota \to e \to r$: Array `height[person]`
   - Function application: Array indexing

3. **Propositions become probabilities**
   - Truth values: Real numbers in [0,1]
   - Logical operations: Probabilistic operations

4. **The probabilistic bind statements become Stan's target**
   - The $‚àº$ structures sequential computation
   - This determines Stan's log probability
:::

---

## PDS kernel output

The PDS system outputs the following kernel model, given the semantic fragment:

```stan
model {
  // FIXED EFFECTS
  w ~ normal(0.0, 1.0);
  
  // LIKELIHOOD
  target += normal_lpdf(y | w, sigma);
}
```

<!-- ::: {.fragment} -->
<!-- ::: {style="font-size: 85%;"} -->
<!-- Actual PDS output: `model { w ~ normal(0.0, 1.0); target += normal_lpdf(y | w, sigma); }` -->
<!-- ::: -->

Captures the essential degree-based semantics where `w` represents the degree on the height scale.

---

## From kernel to full model

The full model with analyst augmentations looks like:

```stan {code-line-numbers="7,13"}
model {
  // PRIORS (analyst-added)
  sigma_epsilon_guess ~ exponential(1);
  sigma_e ~ beta(2, 10);
  
  // FIXED EFFECTS (PDS kernel)
  mu_guess ~ normal(0.0, 1.0);
  
  // RANDOM EFFECTS (analyst-added)
  z_epsilon_guess ~ std_normal();
  
  // LIKELIHOOD (PDS kernel with modifications)
  y[i] ~ normal(mu_guess[item[i]] + epsilon_guess[participant[i]], sigma_e);
}
```

Highlighted lines show the kernel model from PDS

---

## From kernel to full model

::: {.fragment}
- The unhighlighted portions add statistical machinery for real data:
  - Hierarchical priors
  - Random effects  
  - Indexed parameters
:::

---

# Modeling Vagueness

## From degrees to likelihood judgments

Our next model addresses how speakers reason about the likelihood that gradable adjectives apply: *how likely (is it) that Jo is tall?*

::: {.incremental}
- Metalinguistic judgment about adjective application
- Requires probabilistic reasoning
- **Key**: Threshold uncertainty creates vagueness
:::

---

## The PDS implementation

```haskell {style="font-size: 85%;"}
-- From sources/pds/src/
expr1 = ["jo", "is", "a", "soccer", "player"]
expr2 = ["how", "likely", "that", "jo", "is", "tall"]
s1 = getSemantics @Adjectives 0 expr1
q1 = getSemantics @Adjectives 0 expr2
discourse = ty tau $ assert s1 >>> ask q1
likelihoodExample = asTyped tau 
  (betaŒîNormal Œ¥Rules . 
   adjectivesRespond likelihoodPrior) discourse
```

---

## Lexical entries (Part 1)

```haskell {style="font-size: 75%;"}
-- From Grammar.Lexica.SynSem.Adjectives
"tall" -> [ SynSem {
    syn = AP,
    sem = ty tau (lam s (purePP (lam x (lam i 
      (sCon "(‚â•)" @@ (sCon "height" @@ i @@ x) @@ 
       (sCon "d_tall" @@ s)))) @@ s))
    } ]
```

::: {.fragment}
Key semantic components:

- `height`: $\iota \to e \to r$ (entity heights)
- `d_tall`: $\sigma \to r$ (contextual threshold)
- `(‚â•)`: $r \to r \to t$ (comparison)
:::

---

## Lexical entries (Part 2)

```haskell {style="font-size: 75%;"}
"likely" -> [ SynSem {
    syn = S :\: Deg :/: S,
    sem = ty tau (lam s (purePP (lam p (lam d (lam _' 
      (sCon "(‚â•)" @@ 
       (Pr (let' i (CG s) (Return (p @@ i)))) @@ 
       d)))) @@ s))
    } ]
```

::: {.fragment}
Compares probability of embedded proposition to a degree
:::

---

## Œ¥-reduction for vagueness

Starting with *Jo is tall*:
$$\ct{(‚â•)}(\ct{height}(i)(\ct{j}))(\ct{d\_tall}(s))$$

::: {.fragment}
Apply `states` rule to extract threshold:
$$\text{becomes}\,\,\,\ct{(‚â•)}(\ct{height}(i)(\ct{j}))(d)$$
:::

::: {.fragment}
Apply `indices` rule to extract height:
$$\text{becomes}\,\,\,\ct{(‚â•)}(h)(d)$$
:::

---

## The states Œ¥-rule

```haskell {style="font-size: 75%;"}
-- From Lambda.Œî (lines 167-183)
states :: DeltaRule
states = \case
  CG      (UpdCG cg _)     -> Just cg
  CG      (UpdDTall _ s)   -> Just (CG s)
  DTall   (UpdDTall d _)   -> Just d
  DTall   (UpdCG _ s)      -> Just (DTall s)
  QUD     (UpdQUD q _)     -> Just q
  QUD     (UpdCG _ s)      -> Just (QUD s)
  TauKnow (UpdTauKnow b _) -> Just b
  TauKnow (UpdCG _ s)      -> Just (TauKnow s)
  _                        -> Nothing
```

---

## The probabilities Œ¥-rule

```haskell {style="font-size: 75%;"}
-- From Lambda.Œî (lines 156-164)
probabilities :: DeltaRule
probabilities = \case
  Pr (Return Tr)         -> Just 1
  Pr (Return Fa)         -> Just 0
  Pr (Bern x)            -> Just x
  Pr (Disj x t u)        -> Just (x * Pr t + (1 - x) * Pr u)
  Pr (Let v (Normal x y) (Return (GE t (Var v')))) 
    | v' == v -> Just (NormalCDF x y t)
  Pr (Let v (Normal x y) (Return (GE (Var v') t))) 
    | v' == v -> Just (NormalCDF (- x) y t)
  _                      -> Nothing
```

---

## Probabilistic computation

The system computes $P(h \geq d)$ when both are uncertain:

::: {.incremental}
- $h \sim \ct{Normal}(\mu_h, \sigma_h)$ (height)
- $d \sim \ct{Normal}(\mu_d, \sigma_d)$ (threshold)
- Result: `normal_cdf` computation in Stan
:::

::: {.fragment}
```stan
response = 1 - normal_cdf(d[item[i]] | 
                         mu_guess[i], 
                         sigma_guess);
```
:::

---

## PDS kernel for vagueness

```stan
model {
  v ~ normal(0.0, 1.0);
  target += normal_lpdf(y | 
              normal_cdf(v, -0.0, 1.0), 
              sigma);
}
```

::: {.fragment}
The kernel captures likelihood via `normal_cdf`
:::

---

## Full vagueness model

```stan {style="font-size: 75%;" code-line-numbers="9-11"}
transformed parameters {
  for (i in 1:N_data) {
    // Add participant adjustment
    real threshold_logit = mu_guess0[item[i]] + 
                          epsilon_mu_guess[participant[i]];
    mu_guess[i] = inv_logit(threshold_logit);
    
    // KEY SEMANTIC COMPUTATION: P(adjective applies)
    response_rel[i] = 1 - normal_cdf(d[item[i]] | 
                                     mu_guess[i], 
                                     sigma_guess);
  }
}
```

---

<!-- # Summary -->

<!-- ## The complete pipeline -->

<!-- ```{style="text-align: center;"} -->
<!-- Semantic theory (Œª-terms) -->
<!--      ‚Üì Œ¥-rules -->
<!-- Simplified form -->
<!--      ‚Üì Compilation -->
<!-- Kernel model (Stan) -->
<!--      ‚Üì Augmentation -->
<!-- Full statistical model -->
<!-- ``` -->

<!-- --- -->

## Key ideas

::: {.incremental}
- **Œ¥-rules** bridge theory and computation
- **Kernel models** isolate semantic content from statistical machinery
- **This distinction is crucial**: 
  - PDS automates the translation from compositional semantics to the core statistical model...
  - while leaving room for analysts to add domain-specific statistical structure
- **Compilation** embodies our linking hypothesis between semantic competence and performance
:::

---

## What we've covered

::: {.incremental}
- **Degree inference** from norming questions using degree-argument adjectives
- **Vagueness** as threshold uncertainty in gradable adjectives
- **Likelihood judgments** via probabilistic computation with `normal_cdf`
- **Full Œ¥-rule walkthroughs** showing how complex Œª-terms become Stan parameters
:::

<!-- --- -->

<!-- # Future Directions -->

<!-- ## Automated augmentation -->

<!-- Currently analysts manually add: -->

<!-- - Random effects -->
<!-- - Hierarchical priors -->
<!-- - Measurement models -->

<!-- ::: {.fragment} -->
<!-- **Goal**: Automate common statistical patterns -->
<!-- ::: -->

<!-- --- -->

<!-- ## Formally verified compilation -->

<!-- ::: {.incremental} -->
<!-- - Œ¥-rules with Agda/Coq proofs -->
<!-- - Ensure semantic preservation -->
<!-- - Enable generic transformations -->
<!-- - Build trust in compilation -->
<!-- ::: -->

<!-- --- -->

<!-- ## Richer kernel models -->

<!-- Extend PDS to directly output: -->
<!-- - Mixture models -->
<!-- - Censoring mechanisms -->
<!-- - Complex statistical structures -->

<!-- ::: {.fragment} -->
<!-- **Key**: Encode more semantic theory in kernels -->
<!-- ::: -->

<!-- --- -->

<!-- ## Community contributions -->

<!-- The modular PDS design invites: -->

<!-- - New lexical entries -->
<!-- - Domain-specific Œ¥-rules -->
<!-- - Alternative compilation strategies -->
<!-- - Cross-linguistic extensions -->

<!-- --- -->

## Conclusion

::: {.incremental}
- PDS bridges formal semantics and statistical modeling
- Œ¥-rules enable computational tractability
- Kernel models preserve theoretical clarity
- This approach scales to complex phenomena
:::

::: {.fragment}
Next: How this handles factivity‚Äîwhere gradience poses even deeper theoretical puzzles
:::

---

## References
