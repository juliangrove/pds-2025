---
title: "Factivity Results and Implications"
bibliography: ../../pds.bib
format:
  revealjs:
    css: ../styles.css
    html-math-method: mathjax
    mathjax-config:
      loader: {load: ['[tex]/bussproofs','[tex]/bbox','[tex]/colorbox']}
      tex:
        packages: {'[+]': ['bussproofs','bbox','colorbox']}
---

# Model Results

## Quantitative results

::: {style="text-align: center;"}
![](plots/fits_elpd.png){width=600}

Discrete-factivity dramatically outperforms alternatives
:::

::: {.fragment}
- vs. wholly-gradient: ΔELPD = 834.5 ± 55.4
- vs. discrete-world: ΔELPD = 766.1 ± 53.8
- vs. wholly-discrete: ΔELPD = 295.1 ± 34.8
:::

---

## Posterior predictive checks

::: {style="text-align: center;"}
![](plots/contentful_all_6_pp.png){width=700}

How well do models capture response distributions?
:::

::: {.fragment}
**Discrete-factivity** captures characteristic dips mid-scale—mixture of factive (≈1) and non-factive (varies) interpretations
:::

---

## Why discrete-factivity succeeds

The discrete-factivity model captures non-monotonic response patterns:

::: {.incremental}
- **Canonical factives** (*know*, *discover*): High probability of factive interpretation
- **Non-factives** (*think*, *say*): Low probability of factive interpretation  
- **Variable predicates** (*confirm*, *prove*): Intermediate probabilities
:::

::: {.fragment}
Wholly-gradient model produces smooth, unimodal distributions—missing the bimodality
:::

---

## Robustness checks

### Anti-veridicality

::: {style="text-align: center;"}
![](plots/fits_a-v_elpd.png){width=500}

Pattern holds even with anti-veridicality component
:::

---

## Cross-experimental validation

::: {style="text-align: center;"}
![](plots/contentful_elpd.png){width=500}

Discrete-factivity advantage persists in replication
:::

---

## Replication success

::: {style="text-align: center;"}
![](plots/projection_item_means.png){width=700}

Near-perfect correlation at verb level (r = 0.98)
:::

---

## Non-contentful experiments

Test with minimal contexts:

::: {.columns}
::: {.column width="50%"}
**Bleached**: "a particular thing happened"

**Templatic**: "X happened"
:::
::: {.column width="50%"}
![](plots/bleached_verb_means.png){width=350}
:::
:::

::: {.fragment}
Factivity patterns persist even without rich content!
:::

---

## Context effects

The norming study reveals how world knowledge varies:

::: {style="text-align: center;"}
![](plots/norming_3_thetas.png){width=600}

Clear separation between high/low prior contexts validates experimental manipulation
:::

---

## Predicate-specific patterns

::: {style="text-align: center;"}
![](plots/fit_6_thetas.png){width=700}

Variation in propensity to trigger factive interpretations
:::

::: {.fragment}
**Key insight**: Canonical factives (*know*, *discover*) show high probability; variable predicates (*confirm*, *prove*) show intermediate probabilities with high uncertainty
:::

---

## Complete posterior predictive distributions

:::{.callout-tip collapse="true" title="Click to see all 20 predicates"}
::: {style="text-align: center;"}
![](plots/fits_all_full_pp.png){width=700}

Posterior predictive distributions for all predicates
:::
:::

---

# Implementation Challenges

## Identifiability issues

In mixture models, components can sometimes "trade off"—different parameter combinations yield identical predictions.

::: {.fragment}
**Solutions**:
- Informative priors using norming data to constrain world knowledge parameters
- Hierarchical structure with partial pooling across predicates and contexts
- Multiple contexts per predicate
:::

---

## Computational efficiency

Mixture models can be slow to fit or fail to converge.

::: {.fragment}
**Improvements**:
- Non-centered parameterizations (as shown in transformed parameters blocks)
- Vectorization (operating on arrays rather than loops where possible)  
- Warm starts (initializing chains near reasonable values)
:::

---

## Model validation

Beyond posterior predictive checks:

::: {.incremental}
- **Prior predictive checks**: Ensuring priors generate reasonable data
- **Residual analysis**: Checking for systematic deviations
- **Cross-validation**: Using held-out data to assess generalization
:::

---

## Alternative response distributions

To ensure robustness, we tested ordered beta distributions:

:::{.callout-tip collapse="true" title="Robustness to response distribution choice"}
::: {style="text-align: center;"}
![](plots/ordered_beta_elpd.png){width=600}

Left: ELPDs for ordered beta models. Right: Truncated normal vs. ordered beta comparison
:::

Discrete-factivity maintains advantage regardless of response distribution
:::

---

# Theoretical Implications

## For semantic theory

### Factivity involves categorical semantic properties

- Gradience emerges from uncertainty about interpretation
- Not from gradient truth conditions
- Supports traditional presuppositional approaches

::: {.fragment}
### Evidence against wholly-gradient theories
- Gradient Projection Principle (@tonhauser_how_2018) not supported
- Content doesn't project "to the extent it is not at-issue"
:::

---

## For projection theories

### Underlying mechanisms operate discretely

Whether QUD-based, at-issueness, or commitment-based:

::: {.incremental}
- Categorical inclusion/exclusion of content
- Binary decisions about presupposition accommodation
- Discrete contextual factors determining interpretation
:::

::: {.fragment}
**Implication**: Projection mechanisms involve binary operations, not continuous scales
:::

---

## Methodological implications

::: {.incremental}
1. **Aggregate ≠ individual gradience**
   - Population patterns can emerge from discrete processes
   
2. **Theory-driven models matter**
   - PDS maintains theoretical commitments
   - Enables quantitative tests of linguistic hypotheses
   
3. **Multiple paradigms strengthen conclusions**
   - Convergent results across experiments
   - Robust evidence for theoretical claims
:::

---

## Connections across phenomena

### Adjectives vs. Factivity

::: {style="font-size: 90%;"}
**Adjectives**:
- Gradient uncertainty from vagueness
- Unresolved indeterminacy
- Continuous threshold parameters

**Factivity**:
- Discrete uncertainty from interpretation
- Resolved indeterminacy  
- Mixture model components
:::

::: {.fragment}
PDS compilation naturally reflects these theoretical distinctions
:::

---

## Summary

### Empirically
Discrete-factivity models dramatically outperform gradient alternatives

::: {.fragment}
### Theoretically
Factivity involves categorical interpretation selection
:::

::: {.fragment}
### Methodologically
PDS enables principled theory comparison through compositional semantics → statistical models
:::

::: {.fragment}
The framework provides a template for investigating similar questions across semantic phenomena
:::

---

## Looking forward

### Future directions

::: {.incremental}
- Extend to other projection phenomena (conventional implicatures, definite descriptions)
- Investigate individual differences in interpretation strategies
- Explore developmental and cross-linguistic variation
- Apply to real-world language understanding systems
:::

::: {.fragment}
**The broader vision**: Probabilistic semantics as a bridge between theoretical linguistics and computational modeling
:::

---

## Thank you

Questions and discussion

::: {style="text-align: center; margin-top: 40px; font-size: 80%;"}
**Previous sections:**  
[← Factivity Introduction](factivity-intro.html) | [← Models and Implementation](factivity-models.html)
:::