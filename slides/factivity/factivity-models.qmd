---
title: "Factivity Models and Implementation"
bibliography: ../../pds.bib
format:
  revealjs:
    css: ../styles.css
    html-math-method: mathjax
    mathjax-config:
      loader: {load: ['[tex]/bussproofs','[tex]/bbox','[tex]/colorbox']}
      tex:
        packages: {'[+]': ['bussproofs','bbox','colorbox']}
---

# From PDS to Stan

## Discrete-factivity in PDS

```haskell
-- From Grammar.Lexica.SynSem.Factivity
"knows" -> [ SynSem {
    syn = S :\: NP :/: S,
    sem = ty tau (lam s (purePP (lam p (lam x (lam i 
      (ITE (TauKnow s) 
           (And (epi i @@ x @@ p) (p @@ i))  -- factive
           (epi i @@ x @@ p))))))            -- non-factive
      @@ s)
} ]
```

::: {.fragment}
`ITE` creates discrete choice based on `TauKnow` parameter
:::

---

## PDS compilation process

```haskell
-- From Grammar.Parser and Grammar.Lexica.SynSem.Factivity
expr1 = ["jo", "knows", "that", "bo", "is", "a", "linguist"]
expr2 = ["how", "likely", "that", "bo", "is", "a", "linguist"]
s1 = getSemantics @Factivity 0 expr1
q1 = getSemantics @Factivity 0 expr2
discourse = ty tau $ assert s1 >>> ask q1

-- Compile to Stan using factivityPrior and factivityRespond
factivityExample = asTyped tau (betaDeltaNormal deltaRules . factivityRespond factivityPrior) discourse
```

::: {.fragment}
This process transforms compositional semantics into Stan kernel models
:::

---

## The discrete-factivity kernel

PDS outputs this kernel model:

```stan
model {
  // FIXED EFFECTS
  v ~ logit_normal(0.0, 1.0);  // probability of factive interpretation
  w ~ logit_normal(0.0, 1.0);  // world knowledge (from norming)
  
  // LIKELIHOOD
  target += log_mix(v, 
                    truncated_normal_lpdf(y | 1.0, sigma, 0.0, 1.0),     // factive branch
                    truncated_normal_lpdf(y | w, sigma, 0.0, 1.0));      // non-factive branch
}
```

::: {.fragment}
This captures discrete branching: with probability `v`, response is near 1.0; otherwise, it depends on world knowledge `w`
:::

---

## The full Stan implementation

Key factivity-specific components:

```stan {.line-numbers highlight="17-18,21-22,25-27"}
model {
  // ... priors and hierarchical structure ...
  
  for (n in 1:N) {
    // Probability of factive interpretation
    real verb_prob = inv_logit(verb_intercept[verb[n]] + 
                               subj_intercept_verb[subj[n], verb[n]]);
    
    // World knowledge probability  
    real context_prob = inv_logit(context_intercept[context[n]] + 
                                  subj_intercept_context[subj[n], context[n]]);
    
    // MIXTURE LIKELIHOOD
    target += log_mix(verb_prob,
                      truncated_normal_lpdf(y[n] | 1.0, sigma_e, 0.0, 1.0),
                      truncated_normal_lpdf(y[n] | context_prob, sigma_e, 0.0, 1.0));
  }
}
```

---

## Hierarchical structure

The kernel is augmented with statistical machinery for real data:

::: {.incremental}
- **Hierarchical priors**: Verb-specific and context-specific effects
- **Integration with norming**: `mu_omega` and `sigma_omega` from norming study
- **Subject-level variation**: Random effects for individual differences
- **Mixture likelihood**: Core discrete-factivity structure (highlighted lines)
:::

---

## Wholly-gradient factivity in PDS

Alternative lexical entry branches on common ground indices:

```haskell
-- From Grammar.Lexica.SynSem.Factivity
"knows" -> [ SynSem {
    syn = S :\: NP :/: S,
    sem = ty tau (lam s (purePP (lam p (lam x (lam i
      (ITE (TauKnow i)
           (And (epi i @@ x @@ p) (p @@ i)) -- factive
           (epi i @@ x @@ p))))) @@ s))     -- non-factive
    } ]
```

::: {.fragment}
Now `TauKnow` parameter varies by *index* rather than discourse state
:::

---

## Gradient PDS compilation

The alternative implementation requires different prior structure:

```haskell
-- Prior over indices encoded in common ground
factivityPrior = let' x (LogitNormal 0 1) (let' y (LogitNormal 0 1) 
  (let' z (LogitNormal 0 1) (Return (UpdCG (let' b (Bern x) 
    (let' c (Bern y) (let' d (Bern z) (Return (UpdTauKnow b 
      (UpdLing (lam x c) (UpdEpi (lam x (lam p d)) _0))))))) ϵ))))
```

::: {.fragment}
Bernoulli statement regulating factivity is part of the common ground definition
:::

---

## Gradient Stan kernel

```stan
model {
  // FIXED EFFECTS
  v ~ logit_normal(0.0, 1.0);  // degree of factivity
  w ~ logit_normal(0.0, 1.0);  // world knowledge
  
  // GRADIENT LIKELIHOOD
  target += truncated_normal_lpdf(y | v + (1.0 - v) * w, sigma, 0.0, 1.0);
}
```

::: {.fragment}
Continuous computation: `response = v + (1-v) * w`
:::

---

## Understanding the gradient computation

Let's trace through this computation:

::: {.incremental}
- If `v = 0` (no factivity): `response = 0 + 1 * w = w` (pure world knowledge)
- If `v = 1` (full factivity): `response = 1 + 0 * w = 1` (certain)
- If `v = 0.5` (partial factivity): `response = 0.5 + 0.5 * w` (boosted world knowledge)
:::

::: {.fragment}
This provides a "boost" to world knowledge but never forces certainty
:::

---

## Delta rules implementation

Both models require modifications to handle factivity parameters:

```haskell
-- From Lambda.Delta: Computes functions on states
states :: DeltaRule
states = \case
  TauKnow (UpdTauKnow b _) -> Just b
  TauKnow (UpdCG _ s)      -> Just (TauKnow s)
  TauKnow (UpdQUD _ s)     -> Just (TauKnow s)
  -- ... other cases
```

::: {.fragment}
**Discrete model**: `TauKnow` varies by discourse state
:::

::: {.fragment}
**Gradient model**: `TauKnow` varies by common ground indices
:::

---

## Alternative models for comparison

We also consider two models created by directly manipulating Stan code:

### Wholly-discrete model
Both factivity and world knowledge are discrete

### Discrete-world model  
World knowledge discrete, factivity gradient

::: {.fragment}
These serve as computational experiments but don't correspond to theoretical proposals
:::

---

## Response distributions

Both models use truncated normal distributions:

```stan
real truncated_normal_lpdf(real y | real mu, real sigma, real lower, real upper) {
  // Log probability of y under Normal(mu, sigma) truncated to [lower, upper]
  real lpdf = normal_lpdf(y | mu, sigma);
  real normalizer = log(normal_cdf(upper | mu, sigma) - normal_cdf(lower | mu, sigma));
  return lpdf - normalizer;
}
```

::: {.fragment}
Truncation handles bounded slider scales where responses cluster at boundaries
:::

---

## Generated quantities

Both models compute posterior predictions:

```stan {.line-numbers highlight="6-12"}
generated quantities {
  array[N] real y_pred;
  
  for (n in 1:N) {
    if (model_type == "discrete") {
      int branch = bernoulli_rng(verb_prob);
      if (branch == 1) {
        y_pred[n] = truncated_normal_rng(1.0, sigma_e, 0.0, 1.0);
      } else {
        y_pred[n] = truncated_normal_rng(context_prob, sigma_e, 0.0, 1.0);
      }
    } else {
      real response_prob = verb_prob + (1.0 - verb_prob) * context_prob;
      y_pred[n] = truncated_normal_rng(response_prob, sigma_e, 0.0, 1.0);
    }
  }
}
```

---

## Model summary

Four models tested:

1. **Discrete-factivity**: Factivity discrete, world knowledge gradient
2. **Wholly-gradient**: Both factivity and world gradient
3. **Discrete-world**: World knowledge discrete, factivity gradient
4. **Wholly-discrete**: Both discrete

::: {.fragment}
PDS derives the first two from theoretical commitments; the others are computational experiments
:::

---

## Next: Results and Implications

Now let's see how these models perform empirically and what this tells us about the nature of factivity.

::: {style="text-align: center; margin-top: 40px;"}
**Continue to:** [Factivity Results →](factivity-results.html)
:::