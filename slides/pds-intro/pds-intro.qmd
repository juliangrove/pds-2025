---
title: "PDS Introduction"
bibliography: ../../pds.bib
format:
  revealjs:
    css: ../styles.css
    html-math-method: mathjax
    mathjax-config:
      loader: {load: ['[tex]/bussproofs','[tex]/bbox','[tex]/colorbox']}
      tex:
        packages: {'[+]': ['bussproofs','bbox','colorbox']}
---

::: {.hidden}
$$
\newcommand{\expr}[3]{\begin{array}{c}
#1 \\
\bbox[lightblue,5px]{#2}
\end{array} ⊢ #3}
\newcommand{\ct}[1]{\bbox[font-size: 0.8em]{\mathsf{#1}}}
\newcommand{\updct}[1]{\ct{upd\_#1}}
\newcommand{\abbr}[1]{\bbox[transform: scale(0.95)]{\mathtt{#1}}}
\newcommand{\pure}[1]{\bbox[border: 1px solid orange]{\bbox[border: 4px solid transparent]{#1}}}
\newcommand{\return}[1]{\bbox[border: 1px solid black]{\bbox[border: 4px solid transparent]{#1}}}
\def\P{\mathtt{P}}
\def\Q{\mathtt{Q}}
\def\True{\ct{T}}
\def\False{\ct{F}}
\def\ite{\ct{if\_then\_else}}
\def\Do{\abbr{do}}
$$
:::

# A little review

---

## Motivation

Semantic frameworks provide powerful tools for characterizing what we can and cannot mean in using linguistic expressions.

::: {.fragment}
Important properties:
*compositional* and *modular*.

- Compositional:
  the meanings of complex expressions systematically computed from the meanings of smaller expressions, how they are assembled.
- Modular:
  we can analyze one linguistic phenomenon at a time (e.g., anaphora, vagueness)…
  then use a systematic recipe for putting the analyses together (e.g., anaphora *and* vagueness).
:::

---

## Prior approach #1

Challenge:
semantic frameworks don't generally provide an apparatus for characterizing *uncertainty* about what we can mean in using linguistic expressions.
 
::: {.fragment}
Result:
they have great difficulty characterizing:

- the actual inferences that a language-comprehender draws;
- the statistical patterns exhibited by these inferences (e.g., in a human inference dataset).

:::

---

## Inferential uncertainty

(@ex-magician)
The magician's assistant won't admit that they laughed during the trick.


(@ex-laugh-q)
Did the magician's assistant laugh?

---

## Inferential uncertainty

::: {.nonincremental}
(@ex-magician)
The magician's assistant won't admit that they laughed during the trick.


(@ex-laugh-q)
Did the magician's assistant laugh?
:::

![](images/1.png){width=50px}

---

## Inferential uncertainty

::: {.nonincremental}
(@ex-magician)
The magician's assistant won't admit that they laughed during the trick.


(@ex-laugh-q)
Did the magician's assistant laugh?
:::

![](images/2.png)

---

## Inferential uncertainty

::: {.nonincremental}
(@ex-magician)
The magician's assistant won't admit that they laughed during the trick.


(@ex-laugh-q)
Did the magician's assistant laugh?
:::

![](images/3.png)

---

## Inferential uncertainty

::: {.nonincremental}
(@ex-magician)
The magician's assistant won't admit that they laughed during the trick.


(@ex-laugh-q)
Did the magician's assistant laugh?
:::

![](images/4.png)

---

## Inferential uncertainty

::: {.nonincremental}
(@ex-magician)
The magician's assistant won't admit that they laughed during the trick.


(@ex-laugh-q)
Did the magician's assistant laugh?
:::

![](images/4.png)

Average response:

![](images/5.png)

---

## Inferential uncertainty

::: {.nonincremental}
(@ex-magician)
The magician's assistant won't admit that they laughed during the trick.


(@ex-laugh-q)
Did the magician's assistant laugh?
:::

![](images/4.png)

Average response:

![](images/6.png)

---


## Prior approach #2

Frameworks for probabilistic semantics and pragmatics provide powerful tools for characterizing [uncertainty]{.gb-orange} about what we can mean in using linguistics expressions: 

- sampling from and querying arbitrary probability distributions 
- relating the output to (e.g., human) data 
- formal model comparison
  
::: {.fragment}
Challenge:
no general structure-preserving method for mapping between semantic analyses and probabilistic analyses.
:::

---

## "Probabilistic Dynamic Semantics"

Goal:
framework where probabilistic reasoning can be [added]{.gb-orange} to a semantic analysis without changing its structure.

::: {.fragment}
Result:

- characterize and distinguish difference sources of uncertainty 
- use *existing semantic theories* by plugging them into the framework 
- formally compare different semantic theories with each other

:::

---

## A general goal

- Have a methodology that is widely available to linguists working on meaning.

::: {.fragment}
Large-scale inference datasets are becoming more and more central to linguistic methodology, and semantic theory should keep up! 

- Tools to render semantic theories as probabilistic models can help catalyze things.
:::

---

## The basic idea

We should think of an experimental trial as a little discourse.
  
<br>
<div style="text-align: center;">
<img src="images/schematic.png" style="width: 80%;">
</div>
  
---

## The basic idea
  
- [Sentences:]{.gb-orange}
  Start with a prior distribution over parameters some kind (e.g., encoding world knowledge, basic meanings). 
  Update this prior with $⟦\textit{s1}⟧$, then $⟦\textit{s2}⟧$, etc. 
- [Question:]{.gb-orange}
  Add $⟦\textit{q}⟧$ to the stack of questions under discussion [@ginzburg_dynamics_1996; @farkas_reacting_2010]. 
- [Answer:]{.gb-orange}
  Retrieve $⟦\textit{q}⟧$ from the question stack;
  respond (i.e., query an inference distribution).

---

## Upshot

  Once we have modeled the entire discourse in terms of some semantic analysis, we end up with [a distribution over answers to the question prompt]{.gb-orange}.
  
- A distribution we can learn about from data.

---

## Two kinds of uncertainty

- [resolved]{.gb-orange} (or *type-level*) uncertainty
  - lexical, structural, or semantic (e.g., scopal) ambiguity
- [unresolved]{.gb-orange} (or *token-level*) uncertainty
  - present on individual occasions of language use

---

## Example of resolved uncertainty

(@ex-run-sketch) Jo ran a race.

<br>

- locomotion sense of *ran*
- management/organizational sense of *ran*

::: {.fragment}
About nature of speech act.
:::

---

## Example of unresolved uncertainty

(@ex-tall-sketch) Jo is tall.

<br>

- uncertainty about Jo's height

::: {.fragment}
In some sense, independent of speech act.
:::

---

## Uncertainty in PDS

- Probability distributions may be "stacked"; this stacking is reflected in the *types* of semantic values.
  - E.g., $\P e$ vs. $\P (\P e)$.
- Resolved uncertainty: about the *state* of some discourse.
  - $\P σ$
- Unresolved uncertainty: encoded in the *common ground*.
  - $\P ι$ ($ι$ the type of possible worlds)
  - The common ground is an aspect of the state! $\P σ = \P (… \P ι …)$

---

## Discourse states

- Lists of parameters - can be arbitrarily complex.
  - The common ground (or context set; @stalnaker_assertion_1978)
  - The question under discussion [@roberts_information_2012;@ginzburg_dynamics_1996]
  - @farkas_reacting_2010 stuff (e.g., projected sets)
  - (Whatever you want)

---

## Common grounds

- Probability distributions over indices of some type
  - encode what is true "in the world" <br> 
	e.g., $\ct{height}(i) : e → r$ returns people's heights
  - maybe certain linguistic parameters
	- e.g., the height threshold for *tall*: $\ct{d}_{\textit{tall}}(i) : r$

::: {.fragment}
### States vs. indices of the common ground

What kind of thing goes where?

- Ultimately, an empirical question.
:::

---

## Expressions and discourses

- map discourse states onto probability distributions over new discourse states: $σ → \P (α × σ^{\prime})$
- complex linguistic acts may be *sequenced*
  - an operation, *bind*, native to the probability monad

::: {.fragment}

### For instance

- $\abbr{assert}(⟦\textit{Jo is tall}⟧) : σ → \P (⋄ × σ)$
- $\abbr{ask}(⟦\textit{how tall?}⟧) : σ → \P (⋄ × \Q ι r σ)$
- $\abbr{assert}(⟦\textit{Jo is tall}⟧) >> \abbr{ask}(⟦\textit{how tall?}⟧) : σ → \P (⋄ × \Q ι r σ)$
:::

---

## Response functions/linking models

- Take a discourse, together with a prior distribution over states…
- Give back a distribution over responses to the last question asked (given some testing instrument).

::: {.fragment}
$$
\begin{align*}
\ct{respond} &: \P σ → (σ → \P (⋄ × \Q ι α σ^{\prime})) → \P ρ
\end{align*}
$$
:::

---

## Examples of linking models

$$
\begin{align*}
\ct{respond} &: \P σ → (σ → \P (⋄ × \Q ι α σ^{\prime})) → \P ρ
\end{align*}
$$

- Likert scale (categorical distribution);<br>
  e.g., $ρ = \{\textit{yes}, \textit{maybe}, \textit{no}\}$
- Slider scale (truncated normal distribution); <br>
  $ρ = r$
- Binary forced choice (Bernoulli distribution); <br>
  $ρ = t$

---

# Syntax, meaning, compositionality

---

## CCG

### Atomic types
$$
\begin{align*}
\mathcal{A} &\Coloneqq np ∣ n ∣ s
\end{align*}
$$
Noun phrases ($np$), nouns ($n$), and sentences ($s$).

::: {.fragment}
### Complex types
$$
\begin{align*}
\mathcal{C}_{\mathcal{A}} &\Coloneqq \mathcal{A} ∣ \mathcal{C}_{\mathcal{A}}/\mathcal{C}_{\mathcal{A}} ∣ \mathcal{C}_{\mathcal{A}}\backslash\mathcal{C}_{\mathcal{A}}
\end{align*}
$$

E.g., $s/np, s\backslash np, np/n, (np\backslash n)/ np, (s\backslash s)/s$
:::

--- 

## Haskell

<br><br>

``` haskell
data Cat = NP | N | S  -- atomic categories
         | Cat :/: Cat -- the forward slash
         | Cat :\: Cat -- the backward slash
  deriving (Eq)
```

---

## CCG expressions

An expression:

$$\Large
\begin{align*}
  \expr{\textit{dog}}{λx, i.\ct{dog}(i)(x)}{n}
\end{align*}
$$

- $\textit{dog}$ is a string.
- $λx, i.\ct{dog}(i)(x)$ is a meaning (reprented in the λ-calculus).
  - Its type is $e → ι → t$.
- $n$ is a CCG category.

---

## Semantic types (in detail)

::: {.fragment}
### Atomic types

$$
\begin{align*}
A \Coloneqq e ∣ t
\end{align*}
$$
:::

::: {.fragment}
### Complex types

$$
\begin{align*}
   \mathcal{T}_{A} \Coloneqq A ∣ \mathcal{T}_{A} → \mathcal{T}_{A} ∣ \mathcal{T}_{A} × \mathcal{T}_{A} ∣ ⋄
\end{align*}
$$
:::

---

## Haskell

```haskell
-- | Atomic types for entities and truth values.
data Atom = E | T deriving (Eq, Show)

-- | Arrows, and products, as well as type variables for encoding polymorphism.
data Type = At Atom
          | Type :→ Type
          | Unit
          | Type :× Type
          | TyVar String
  deriving (Eq)
```

- Note the type variables!

---

## Hindley-Milner style polymorphism

::: {.fragment}
Good:

- $λx^{α}.x^{α} : α → α$ \ \  🥰
- $λx^{α → β}, y^{α}.x(y)^{β} : (α → β) → α → β$ \ \  🥰
:::

::: {.fragment}
Bad:

- $λf^{α → t}.f(λy^{β}.y^{β}) ∧ f(λx^{β}, y^{γ}.x^{γ})$ \ \  🙁
:::

---

## Typing rules

<br>
$$ \scriptsize
\begin{array}{c}
\begin{prooftree}
\AxiomC{}
\RightLabel{$\mathtt{Ax}$}\UnaryInfC{$Γ, x : α ⊢ x : α$}
\end{prooftree}
& \begin{prooftree}
\AxiomC{$Γ, x : α ⊢ t : β$}
\RightLabel{${→}\mathtt{I}$}\UnaryInfC{$Γ ⊢ λx.t : α → β$}
\end{prooftree}
& \begin{prooftree}
\AxiomC{$Γ ⊢ t : α → β$}
\AxiomC{$Γ ⊢ u : α$}
\RightLabel{${→}\mathtt{E}$}\BinaryInfC{$Γ ⊢ t(u) : β$}
\end{prooftree} \\[2mm]
\begin{prooftree}
\AxiomC{}
\RightLabel{$⋄\mathtt{I}$}\UnaryInfC{$Γ ⊢ ⋄ : ⋄$}
\end{prooftree}
& \begin{prooftree}
\AxiomC{$Γ ⊢ t : α$}
\AxiomC{$Γ ⊢ u : β$}
\RightLabel{$×\mathtt{I}$}\BinaryInfC{$Γ ⊢ ⟨t, u⟩ : α × β$}
\end{prooftree}
& \begin{prooftree}
\AxiomC{$Γ ⊢ t : α_1 × α_2$}
\RightLabel{$×\mathtt{E}_{j}$}\UnaryInfC{$Γ ⊢ π_{j}(t) : α_{j}$}
\end{prooftree}
\end{array}
$$

---

## Typing rules

### $→\mathtt{E}$

<br>
$$
\begin{prooftree}
\AxiomC{$Γ ⊢ t : α → β$}
\AxiomC{$Γ ⊢ u : α$}
\RightLabel{${→}\mathtt{E}$}\BinaryInfC{$Γ ⊢ t(u) : β$}
\end{prooftree}
$$

---

## Haskell: vanilla terms

```haskell
-- | Untyped λ-terms. Types are assigned separately (i.e., "extrinsically").
data Term = Var VarName           -- Variables.
          | Con Constant          -- Constants.
          | Lam VarName Term      -- Abstractions.
          | App Term Term         -- Applications.
          | TT                    -- The 0-tuple.
          | Pair Term Term        -- Pairing.
          | Pi1 Term              -- First projection.
          | Pi2 Term              -- Second projection.
```
- Terms are "Curry-typed".

::: {.fragment}
Constants:

```haskell
-- | Constants are indexed by either strings or real numbers.
type Constant = Either String Double
```
:::

---

## Deriving meanings

::: {.fragment}
### Type homomorphism

- $\small ⟦np⟧ = e \hspace{2cm} ⟦n⟧ = e → ι → t \hspace{2cm} ⟦s⟧ = ι → t$ <br>
- $\small ⟦b / a⟧ = ⟦b \backslash a⟧ = ⟦a⟧ → ⟦b⟧$
:::

::: {.fragment}
### Example: application

(@ex-every-linguist)
$$\scriptsize
\frac{\expr{\textit{every}}{λp^{e → ι → t}, q^{e → ι → t}, i^{ι}.(∀y.p(y)(i) → q(y)(i))^{t}}{s}/(s\backslash np)/ n \hspace{1cm} \expr{\textit{linguist}}{λx^{e}, i^{ι}.(\ct{ling}(i)(x))^{t}}{n}}{
\expr{\textit{every linguist}}{λq^{e → ι → t}, i^{ι}.(∀y.\ct{ling}(i)(y) → q(y)(i))^{t}}{s}/(s\backslash np)
}>
$$
:::

---

### Example: composition

(@ex-every-linguist-saw)
$$ \scriptsize
\frac{\expr{\textit{every linguist}}{λq^{e → ι → t}, i^{ι}.(∀y.\ct{ling}(i)(y) → q(y)(i))^{t}}{s}/ (s\backslash np)
\hspace{1cm}
\expr{\textit{saw}}{λx^{e}, y^{e}, i^{ι}.(\ct{see}(i)(x)(y))^{t}}{s}\backslash np/ np}{
\expr{\textit{every linguist saw}}{λx^{e}, i^{ι}.(∀y.\ct{ling}(i)(y) → \ct{see}(i)(x)(y))^{t}}{s}/ np
}{>}\textbf{B}
$$

---

## Adding probabilistic types

::: {.fragment}
### New atomic types

$$
A \Coloneqq e ∣ t ∣ r
$$
:::

::: {.fragment}
### New complex types

$$
\mathcal{T}_{A} \Coloneqq A ∣ \mathcal{T}_{A} → \mathcal{T}_{A} ∣ \mathcal{T}_{A} × \mathcal{T}_{A} ∣ ⋄ ∣ \P \mathcal{T}_{A}
$$

- $\P t$: a Bernoulli distribution---probabilistically True or False.
- $\P r$: e.g., a normal distribution.

:::

---

## Haskell

```haskell
-- | Atomic types for entities, truth values, and real numbers.
data Atom = E | T | R deriving (Eq, Show)

-- | Arrows, products, and probabilistic types, as well as type variables for
-- encoding polymorphism.
data Type = At Atom
          | Type :→ Type
          | Unit
          | Type :× Type
          | P Type
          | TyVar String
  deriving (Eq)
```

---

## Probabilistic typing rules

<br><br>
$$
\begin{array}{c}
\begin{prooftree}
\AxiomC{$Γ ⊢ t : α$}
\RightLabel{$\mathtt{Return}$}\UnaryInfC{$Γ ⊢ \pure{t} : \P α$}
\end{prooftree}
& \begin{prooftree}
\AxiomC{$Γ ⊢ t : \P α$}
\AxiomC{$Γ, x : α ⊢ u : \P β$}
\RightLabel{$\mathtt{Bind}$}\BinaryInfC{$Γ ⊢ \left(\begin{array}{l} x ∼ t \\ u\end{array}\right) : \P β$}
\end{prooftree}
\end{array}
$$

---

## Haskell: probabilistic programs

```haskell
-- | Untyped λ-terms. Types are assigned separately (i.e., "extrinsically").
data Term = Var VarName           -- Variables.
          | Con Constant          -- Constants.
          | Lam VarName Term      -- Abstractions.
          | App Term Term         -- Applications.
          | TT                    -- The 0-tuple.
          | Pair Term Term        -- Pairing.
          | Pi1 Term              -- First projection.
          | Pi2 Term              -- Second projection.
          | Return Term           -- Construct a degenerate distribution.
          | Let VarName Term Term -- Sample from a distribution and continue.
```

- `Let x t u` \ \ \ =\ \ \ 
  $\begin{array}[t]{l}
  x ∼ t \\
  u
  \end{array}$
- `Return t` \ \ \ = \ \ \ $\pure{t}$


---

<!-- ## The probability monad -->

<!-- $$\small -->
<!-- \begin{array}{c} -->
<!-- \textit{Left identity} & \textit{Right identity} & \textit{Associativity} \\[1mm] -->
<!-- \begin{array}{l} -->
<!-- x ∼ \pure{v} \\ -->
<!-- k -->
<!-- \end{array}\ \ =\ \ k[v/x]  -->
<!-- & \begin{array}{l} -->
<!-- x ∼ m \\ -->
<!-- \pure{x} -->
<!-- \end{array}\ \ =\ \ m  -->
<!-- & \begin{array}{l} -->
<!-- y ∼ \left(\begin{array}{l} -->
<!-- x ∼ m \\ -->
<!-- n -->
<!-- \end{array}\right) \\ -->
<!-- o -->
<!-- \end{array}\ \ =\ \ \begin{array}{l} -->
<!-- x ∼ m \\ -->
<!-- y ∼ n \\ -->
<!-- o -->
<!-- \end{array} -->
<!-- \end{array} -->
<!-- $$ -->

<!-- ::: {.fragment} -->

<!-- ### Example (associativity) -->

<!-- $$ -->
<!-- \begin{array}{l} -->
<!-- y ∼ \left(\begin{array}{l} -->
<!-- x ∼ \abbr{Normal}(0, 1) \\ -->
<!-- \abbr{Normal}(x, 1) -->
<!-- \end{array}\right) \\ -->
<!-- o -->
<!-- \end{array} \ \ =\ \  -->
<!-- \begin{array}{l} -->
<!-- x ∼ \abbr{Normal}(0, 1) \\ -->
<!-- y ∼ \abbr{Normal}(x, 1) \\ -->
<!-- o -->
<!-- \end{array} -->
<!-- $$ -->

<!-- ::: -->

<!-- --- -->

### Mother

$$
\begin{array}[t]{l}
x ∼ \ct{mammal}\\
\pure{\ct{mother}(x)}
\end{array}
$$

- Some distribution $\ct{mammal} : \P e$
- Sample a mammal from it; return that mammal's mother.
- You get a distribution over mothers of mammals.

---

## Reweighting distributions

$$\ct{factor} : r → \P ⋄$$

::: {.fragment}

$$\begin{array}[t]{l}
x ∼ \ct{mammal} \\
\ct{factor}(\ct{hungry}(x)) \\
\pure{\ct{mother}(x)}
\end{array}$$

:::

---

## Making observations

$$
\begin{align*}
\ct{observe}\ \ &:\ \ t → \P ⋄ \\
\ct{observe}(p)\ \ &=\ \ \ct{factor}(𝟙(p))
\end{align*}
$$

::: {.fragment}

$$
\begin{array}[t]{l}
x ∼ \ct{mammal} \\
\ct{observe}(\ct{dog}(x)) \\
\pure{\ct{mother}(x)}
\end{array}
$$

- Distribution over mothers of mammals which are also dogs.

:::

---

### Something wacky

$$
\begin{array}[t]{l}
x ∼ \ct{mammal} \\
\ct{factor}(\ct{hungry}(x)) \\
\ct{observe}(\ct{dog}(x)) \\
\pure{\ct{mother}(x)}
\end{array}
$$

- What distribution is this?

---

## Haskell: typing constants

```haskell
-- | Assign types to constants.
type Sig = Constant -> Maybe Type
```
<br>

::: {.fragment}

### An example signature

```haskell
t, r :: Type
t = At T
r = At R

tau :: Sig
tau = \case
  Left  "factor"  -> Just (r :→ P Unit)
  Left  "observe" -> Just (t :→ P Unit)
  Right _         -> Just r
```
:::

---

## Intensionality

Some intensional constants:

$$
\begin{align*}
\ct{see} &: ι → e → e → t \\
\ct{ling} &: ι → e → t
\end{align*}
$$

::: {.fragment}

We require other constants:

$$
\begin{align*}
\updct{see} &: (e → e → t) → ι → ι \\
\updct{ling} &: (e → t) → ι → ι 
\end{align*}
$$

:::

---

## Intensionality

$$
\begin{align*}
\ct{see}(\updct{see}(p)(i)) &= p \\
\ct{see}(\updct{ling}(p)(i)) &= \ct{see}(i)
\end{align*}
$$

::: {.fragment}
$$
\begin{align*}
\ct{ling}(\updct{ling}(p)(i)) &= p \\
\ct{ling}(\updct{see}(p)(i)) &= \ct{ling}(i)
\end{align*}
$$

- Can be seen as a theory of states and locations.
  - Indices are states.
  - (Pairs of) constants are "locations".
  
:::

---

## The common ground

### Definition

A *common ground* is a probabilistic program of type $\P ι$.

- $ι$, a variable over types.

::: {.fragment}
### A "starting" index

$$\ct{@} : ι$$

- Constants that *update* indices can add information, to be later retrieved by intensional constants.

:::

---

### An example common ground

$$\small
\begin{array}[t]{l}
h ∼ \abbr{Normal}(0, 1) \\
\pure{\updct{height}(λx.h)(\ct{@})}
\end{array}
$$

- Encodes uncertainty about Jo's height.

::: {.fragment}

### Another one

$$\scriptsize
\begin{array}[t]{l}
h_{j} ∼ \abbr{Normal}(0, 1) \\
h_{b} ∼ \abbr{Normal}(0, 1) \\
\pure{\updct{height}(λx.\ite(x = \ct{j}, h_{j}, h_{b}))(\ct{@})}
\end{array}
$$

- Encodes uncertainty about Jo's height and Bo's height.
:::

---

## States

Some state-sensitive constants:

$$
\begin{align*}
\ct{CG} &: σ → \P ι \\
\end{align*}
$$

::: {.fragment}
$$
\begin{align*}
\ct{QUD} &: \Q ι α σ → α → ι → t
\end{align*}
$$
:::

::: {.fragment}
$$
\begin{align*}
\updct{CG} &: \P ι → σ → σ \\
\updct{QUD} &: (α → ι → t) → σ → \Q ι α σ 
\end{align*}
$$
:::

---

## States

$$
\begin{align*}
\ct{CG}(\updct{CG}(cg)(s)) &= cg \\
\ct{CG}(\updct{QUD}(q)(s)) &= \ct{CG}(s)
\end{align*}
$$

::: {.fragment}
$$
\begin{align*}
\ct{QUD}(\updct{QUD}(q)(s)) &= q \\
\ct{QUD}(\updct{CG}(cg)(s)) &= \ct{QUD}(s)
\end{align*}
$$
:::

::: {.fragment}

### A "blank" starting state

$$
\ct{ϵ} : σ
$$

:::

---

## Expression meanings

$$
ℙ^{σ}_{σ^{\prime}} α ≝ σ → \P (α × σ^{\prime})
$$

- New type for, e.g., noun phrases:
  $$
  ℙ^{σ}_{σ^{\prime}}(⟦np⟧) \,\, = \,\, ℙ^{σ}_{σ^{\prime}} e \,\, = \,\, σ → \P (e × σ^{\prime})
  $$

---

## Program composition via parameterized monads

$$
\begin{align*}
ℙ^{σ}_{σ^{\prime}} α\ \ &=\ \ σ → \P (α × σ^{\prime})
\end{align*}
$$

::: {.fragment}
$$
\begin{align*}
\return{v}_{σ}\ \ &=\ \ λs.\pure{⟨v, s⟩} : ℙ^{σ}_{σ}
\end{align*}
$$
:::

::: {.fragment}
$$
\begin{align*}
\begin{array}{rl}
\Do_{σ, σ^{\prime}, σ^{\prime\prime}} & x ← m : ℙ^{σ}_{σ^{\prime}} \\
& k(x) : ℙ^{σ^{\prime}}_{σ^{\prime\prime}}
\end{array}\ \ 
&=\ \ λs.\left(\begin{array}{l}
⟨x, s^{\prime}⟩ ∼ m(s) \\
k(x)(s^{\prime})
\end{array}\right) : ℙ^{σ}_{σ^{\prime\prime}}
\end{align*}
$$
:::

---

## Parameterized monad laws

$$\small
\begin{array}{c}
\textit{Left identity} & \textit{Right identity} \\[1mm]
\begin{array}{rl}
\Do_{p, p, q} & x ← \return{v}_{p} \\
& k(x)
\end{array}\ \ =\ \ \ k(v)
& \begin{array}{rl}
\Do_{p, q, q} & x ← m \\
& \return{x}_{q}
\end{array} \ \ =\ \ \ m
\end{array}
$$

::: {.fragment}
$$\small
\begin{array}{c}
\textit{Associativity} \\[1mm]
\begin{array}{rl}
\Do_{p, r, s} & y ← \left(\begin{array}{rl}
	\Do_{p, q, r} & x ← m \\
	& n(x)
	\end{array}\right) \\
	& o(y)
\end{array}\ \ =\ \ \begin{array}{rl}
	\Do_{p, q, s} & x ← m \\
	& \begin{array}{rl} 
		\Do_{q, r, s} & y ← n(x) \\
		& o(y)
	\end{array}
\end{array}
\end{array}
$$
:::

---

## Manipulating stateful programs

### $\ct{get}$ and $\ct{put}$

$$
\begin{align*}
\abbr{get} &: ℙ^{σ}_{σ} σ
\end{align*}
$$

- Gets the current state.

::: {.fragment}
$$
\begin{align*}
\abbr{put} &: σ^{\prime} → ℙ^{σ}_{σ^{\prime}} ⋄ \\
\end{align*}
$$

- Overwrites the current state with a new one.

:::

---

## Haskell

<br><br>
```haskell
getPP :: Term
getPP = lam' s (Return (s & s))

putPP :: Term -> Term
putPP s = Lam fr (Return (TT & s))
  where fr:esh = fresh [s]
```

---


## PDS Rules

### Example: rightward application/composition

$$ \scriptsize
\begin{prooftree}
\AxiomC{$\expr{s_{1}}{M_{1}}{c/ b}$}
\AxiomC{$\expr{s_{2}}{M_{2}}{b∣_{n}a_{n}\,\,⋯∣_{1}a_{1}}$}
\RightLabel{${>}\textbf{B}_{n}$}\BinaryInfC{\(\expr{s_{1}\,s_{2}}{
\begin{array}{rl}
\Do & \{\,m_{1} ← M_{1};\,m_{2} ← M_{2}; \\
& \return{λx_{1}, …, x_{n}.m_{1}(m_{2}(x_{1})…(x_{n}))}\,\}
\end{array}
}{c∣_{n}a_{n}\,\,⋯∣_{1}a_{1}}\)}
\end{prooftree}
$$

---

### Making an assertion

$$
\begin{align*}
\abbr{assert} &: ℙ^{σ}_{σ^{\prime}} (ι → t) → ℙ^{σ}_{σ^{\prime}} ⋄
\end{align*}
$$

::: {.fragment}
$$
\begin{align*}
\abbr{assert}(⟦\textit{Jo is tall}⟧) &= \begin{array}[t]{rl}
\Do & p^{ι → t} ← ⟦\textit{Jo is tall}⟧^{ℙ^{σ}_{σ} (ι → t)} \\
& s ← \abbr{get} \\
& c ← \return{\ct{CG}(s)} \\
& c^{\prime} ← \return{\left(\begin{array}{l}
i ∼ c \\
\ct{observe}(p(i)) \\
\pure{i}
\end{array}\right)} \\
& \abbr{put}(\updct{CG}(c^{\prime})(s))
\end{array}
\end{align*}
$$
:::

---

## Asking a question

$$
\begin{align*}
\abbr{ask} &: ℙ^{σ}_{σ^{\prime}} (α → ι → t) → ℙ^{σ}_{\Q ι α σ^{\prime}} ⋄
\end{align*}
$$

::: {.fragment}
$$
\begin{align*}
\abbr{ask}(⟦\textit{how tall?}⟧) &= \begin{array}[t]{rl}
\Do & q^{r → ι → t} ← ⟦\textit{how tall?}⟧^{ℙ^{σ}_{σ}(r → ι → t)} \\
& s ← \abbr{get} \\
& \abbr{put}(\updct{QUD}(q)(s))
\end{array}
\end{align*}
$$
:::

::: {.fragment}
### Corollary
$$
\begin{align*}
\updct{QUD} &: (α → ι → t) → σ → \Q ι α σ \\
\ct{QUD} &: \Q ι α σ → α → ι → t
\end{align*}
$$
:::

---

## Haskell: the $\Q$ constructor

<br><br>
```haskell
-- | Arrows, products, and probabilistic types, as well as (a) abstract types
-- representing the addition of a new Q, and (b) type variables for encoding
-- polymorphism.
data Type = At Atom
          | Type :→ Type
          | Unit
          | Type :× Type
          | P Type
          | Q Type Type Type
          | TyVar String
  deriving (Eq)
```

---

## Responding to a question

$$
\begin{align*}
\abbr{respond}^{f_Φ : r → \P ρ} &: \P σ → ℙ^{σ}_{\Q ι r σ^{\prime}} ⋄ → \P ρ
\end{align*}
$$

::: {.fragment}
$$
\begin{align*}
\abbr{respond}^{f_Φ : r → \P ρ}(bg)(m) &= \begin{array}[t]{l}
s ∼ bg \\
⟨⋄, s^{\prime}⟩ ∼ m(s) \\
i ∼ \ct{CG}(s^{\prime}) \\
f(\ct{max}(λd.\ct{QUD}(s)(d)(i)), Φ)
\end{array}
\end{align*}
$$
:::

- Example $f_{Φ}$
  - $f(x, Φ) = \abbr{Normal}(x, 1)$

---

## Delta-rules

How do we actually compute stuff?

::: {.fragment}
```haskell
-- | The type of Delta-rules.
type DeltaRule = Term -> Maybe Term
```
:::

--- 

### Indices

```haskell
-- | Computes functions on indices.
indices :: DeltaRule
indices = \case
  Ling   (UpdLing p _)   -> Just p
  Ling   (UpdSocPla _ i) -> Just (Ling i)
  SocPla (UpdSocPla p _) -> Just p
  SocPla (UpdLing _ i)   -> Just (SocPla i)
  _                      -> Nothing
```

::: {.fragment}
### States

```haskell
-- | Computes functions on states.
states :: DeltaRule
states = \case
  CG      (UpdCG cg _)     -> Just cg
  CG      (UpdQUD _ s)     -> Just (CG s)
  CG      (UpdTauKnow _ s) -> Just (CG s)
  QUD     (UpdQUD q _)     -> Just q
  QUD     (UpdCG _ s)      -> Just (QUD s)
  _                        -> Nothing
```
:::

---

### If then else

```haskell
-- | Computes /if then else/.
ite :: DeltaRule
ite = \case
  ITE Tr x y -> Just x
  ITE Fa x y -> Just y
  _          -> Nothing
```

---

### Making observations

```haskell
-- | Observing @Tr@ is trivial, while observing @Fa@ yields an undefined
-- probability distribution.
observations :: DeltaRule
observations = \case
  Let _ (Observe Tr) k -> Just k
  Let _ (Observe Fa) k -> Just Undefined
  _                    -> Nothing
```

---

## Summing up

- We have a computational framework for:
  - encoding grammar fragments
  - representing speech acts (assertions, questions)
  - representing theories linking inferences to behavior (as response functions)
  - computing with the resulting probabilistic programs (via delta-rules)

---

### Note

- Delta-rules are in principle open ended!
  - Might incorporate more sophisticated algorithms for simplifying representations of probability distributions.
  - Might incorporate theorem provers.

---

### References
