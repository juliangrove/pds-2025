---
bibliography: ../../pds.bib
format:
  revealjs:
    css: ../styles.css
    html-math-method: mathjax
    mathjax-config:
      loader: {load: ['[tex]/bussproofs','[tex]/bbox','[tex]/colorbox']}
      tex:
        packages: {'[+]': ['bussproofs','bbox','colorbox']}
---

::: {.hidden}
$$
\newcommand{\expr}[3]{\begin{array}{c}
#1 \\
\bbox[lightblue,5px]{#2}
\end{array} ‚ä¢ #3}
\newcommand{\ct}[1]{\bbox[font-size: 0.8em]{\mathsf{#1}}}
\newcommand{\updct}[1]{\ct{upd\_#1}}
\newcommand{\abbr}[1]{\bbox[transform: scale(0.95)]{\mathtt{#1}}}
\newcommand{\pure}[1]{\bbox[border: 1px solid orange]{\bbox[border: 4px solid transparent]{#1}}}
\newcommand{\return}[1]{\bbox[border: 1px solid black]{\bbox[border: 4px solid transparent]{#1}}}
\def\P{\mathtt{P}}
\def\Q{\mathtt{Q}}
\def\True{\ct{T}}
\def\False{\ct{F}}
\def\ite{\ct{if\_then\_else}}
\def\Do{\abbr{do}}
$$
:::

---

### Motivation

Semantic frameworks provide powerful tools for characterizing what we can and cannot mean in using linguistic expressions. \\ \pause


\textbf{Important property:} \\
In their modern forms these characterizations are \emph{compositional} and \emph{modular}. \pause
\begin{itemize}
\item Compositional:
the meanings of complex expressions are systematically computed from the meanings of smaller expressions and how they are assembled. \pause
\item Modular:
we can develop analyses of one linguistic phenomenon at a time (e.g., anaphora, vagueness)\ldots \\ \pause
\ldots and then we can use some systematic recipe for putting the analyses together (e.g., anaphora \emph{and} vagueness).
\end{itemize}

---

<!-- \begin{frame}{Prior approach \#1} -->
<!--   \textbf{Challenge:} \\ -->
<!--   Semantic frameworks don't generally provide an apparatus for characterizing \emph{uncertainty} about what we can mean in using linguistic expressions. \pause -->
<!--   ~\\~\\ -->
<!--   \textbf{Result:} \\ -->
<!--   They therefore have great difficulty characterizing: -->
<!--   \begin{itemize} -->
<!--   \item the actual inferences that a language-comprehender draws; \pause -->
<!--   \item the statistical patterns exhibited by these inferences (e.g., in a human inference dataset, exhibited by a language model, etc.). -->
<!--   \end{itemize} -->
<!-- \end{frame} -->

<!-- \begin{frame}{Inferential uncertainty} -->
<!--   \ex[exno=1] -->
<!--    The magician's assistant won't admit that they laughed during the trick. \vspace{-3mm} -->
<!--   \xe -->
<!--   \pause -->
<!--   \ex[exno=2] -->
<!--   Did the magician's assistant laugh? -->
<!--   \xe \vspace{-1cm} -->
<!--   \pause -->
<!--   \begin{itemize}[label={}] -->
<!--   \item \includegraphics[height=5mm]{./images/laugher.png}\,\,\,{\LARGE \color{gb-red} X} -->
<!--     \hfill -->
<!--     \only<3>{\includegraphics[width=7cm]{./images/yes-no-slider1}} -->
<!--     \only<4->{\includegraphics[width=7cm]{./images/no.png}} \pause\pause -->
<!--   \item \includegraphics[height=5mm]{./images/laugher.png} \checkmark -->
<!--     \hfill -->
<!--     \only<5>{\includegraphics[width=7cm]{./images/yes-no-slider1.png}} -->
<!--     \only<6->{\includegraphics[width=7cm]{./images/yes.png}} -->
<!--   \end{itemize} -->
<!--   \pause\pause -->
<!--   Average response: -->
<!--   \begin{center} -->
<!--     \only<8>{\includegraphics[width=7cm]{./images/yes-no-slider1.png}} -->
<!--     \only<9>{\includegraphics[width=7cm]{./images/yes-no-slider2.png}} -->
<!--   \end{center} -->
<!-- \end{frame} -->

<!-- \begin{frame}{Prior approach \#2} -->
<!--   Frameworks for probabilistic semantics and pragmatics provide powerful tools for characterizing \alert{uncertainty} about what we can mean in using linguistics expressions: \pause -->
<!--   \begin{itemize} -->
<!--   \item sampling from and querying arbitrary probability distributions \pause -->
<!--   \item relating the output to (e.g., human) data \pause -->
<!--   \item formal model comparison -->
<!--   \end{itemize} \pause -->

<!--   \textbf{Challenge:} no general structure-preserving method for mapping between semantic analyses and probabilistic analyses. -->
<!-- \end{frame} -->

<!-- \begin{frame}{Probabilistic Dynamic Semantics} -->
<!--   \textbf{Goal:} arrive at a framework that views probabilistic reasoning as arising from a module that can be \alert{added} to a semantic analysis without changing its structure. \\ \pause -->
<!--   ~\\ -->
<!--   \textbf{Results:} \pause -->
<!--   \begin{itemize} -->
<!--   \item We can characterize and distinguish difference sources of inferential uncertainty. \pause -->
<!--   \item We can use \emph{existing semantic theories} to characterize this uncertainty by plugging them into the framework. \pause -->
<!--   \item We can formally compare different semantic theories with each other, by seeing how well they characterize the uncertainty exhibited by a dataset. -->
<!--   \end{itemize} -->
<!-- \end{frame} -->

<!-- \begin{frame}{The general program} -->
<!--   Ideally, such a methodology would be widely available to linguists working on meaning. \\ \pause -->
<!--   ~\\ -->
<!--   Large-scale inference datasets are becoming more and more central to linguistic methodology, and semantic theory should keep up! \\ \pause -->
<!--   ~\\ -->
<!--   Tools to render semantic theories as probabilistic models can help catalyze this process, while making the latter more broadly accessible. -->
<!--   (More on this later.) -->
<!-- \end{frame} -->

<!-- \begin{frame}{The basic idea} -->
<!--   We should think of an experimental trial as a little discourse. -->
<!--   \pause -->
<!--   \begin{center} -->
<!--     \includegraphics[width=200px]{./images/schematic.png} -->
<!--   \end{center} -->
<!--   \pause \vspace{-5mm} -->
<!--   \begin{itemize} -->
<!--   \item \alert{Sentences:} -->
<!--     Start with a prior distribution over parameters some kind (e.g., encoding world knowledge, basic meanings). \pause -->
<!--     Update this prior with \(‚ü¶\textit{s1}‚üß\), then \(‚ü¶\textit{s2}‚üß\), etc. \pause -->
<!--   \item \alert{Question:} -->
<!--     Add \(‚ü¶\textit{q}‚üß\) to the stack of questions under discussion \autocite{ginzburg_dynamics_1996,farkas_reacting_2010}. \pause -->
<!--   \item \alert{Answer:} -->
<!--     Retrieve \(‚ü¶\textit{q}‚üß\) from the question stack; -->
<!--     respond (i.e., query an inference distribution). -->
<!--   \end{itemize} -->
<!-- \end{frame} -->

<!-- \begin{frame}{Upshot} -->
<!--   Once we have modeled the entire discourse in terms of some semantic analysis, we end up with \alert{a distribution over answers to the question prompt}. \\ \pause -->
<!--   ~\\ -->
<!--   Crucially, this is a distribution we can learn about, i.e., from data collected using exactly such an experimental trial!  -->
<!-- \end{frame} -->

# Overview

Compositional dynamic semantic theories often model utterance meanings as maps from discourse states into sets of discourse states.^[
	In its distributive implementations, that is.
	For a discussion of distributive vs. non-distributive variants of dynamic semantics, see, e.g., @charlow_where_2019.
]
PDS inherits this functional view of utterances; 
but following much work in the probabilistic semantics and pragmatics literature \citep[][i.a.]{van_benthem_dynamic_2009,lassiter_vagueness_2011,frank_predicting_2012,zeevat_implicit_2013,lassiter_adjectival_2017,bergen_pragmatic_2016}, it translates this idea into a probabilistic setting:
in PDS, utterances denote maps from discourse states to *probability distributions* over discourse states.
Thus in comparison to traditional dynamic semantics, PDS introduces a weighting on discourse states, allowing one to model preferences for certain resolutions of ambiguity over others.

## Probability distributions as monadic values

In and of itself, this extension is not novel.
More novel is that we view probability distributions as *monadic values* that inhabit types arising from a *probability monad* (see, e.g., @giorgolo_one_2014, @bernardy_predicates_2019, @grove_probabilistic_2023).
We formalize this view soon;
but the gist is that viewing probability distributions this way allows PDS (i) to map linguistic expressions of a particular type to probability distributions over objects of that type so that the usual compositional structure of semantic analyses is retained;
and thereby (ii) to compose probabilistic analyses with other analyses of, say, anaphora;
as well as (iii) to define explicit *linking models* that map probability distributions over discourse states to probability distributions over judgments recorded using some response instrument.^[
  This type of capability is often discussed in the experimental linguistics literature under the heading of *linking hypotheses* or *linking assumptions* (see @phillips_theories_2021).
  For our purposes, we define linking models to be statistical models that relate a PDS analysis (which determines a probability distribution over the inferences supported by a linguistic expression) to comprehenders' judgments, as recorded using a particular instrument.
]

Crucial for PDS is that because probability distributions are characterized by a monad, they may themselves be *stacked* while retaining properties important for semantic composition.^[
  More to the point, monads give rise to *functors*, which are composable, giving rise to the "stacking".
]
That is, the types derived from a probability monad may be inhabited by distributions over familiar types of objects---entities, truth values, functions from entities to truth values, and the like---or they may be inhabited by distributions *over* such distributions.
And this stacking can be as deep as is necessary to model the sorts of uncertainty of interest to the analyst.

## Two kinds of uncertainty

We argue here that at least two levels of stacking are necessary in order to appropriately model two kinds of interpretive uncertainty, respectively, which we refer to as *resolved* (or *type-level*) *uncertainty* and *unresolved* (or *token-level*) *uncertainty*.
Resolved uncertainty is any kind of uncertainty which relates to lexical, structural, or semantic (e.g., scopal) ambiguity.
For example, a polysemous word gives rise to resolved uncertainty.
Based on the content of its direct object, *ran* in (@ex-run-sketch) seems likely to take on its locomotion sense, though it remains plausible that it has a management sense if Jo is understood to be the race's organizer.

(@ex-run-sketch) Jo ran a race.

In contrast, unresolved uncertainty is that which is associated with an expression in view of some *fixed* meaning it has.
Vague adjectives may give rise to unresolved uncertainty, for example, as witnessed by the vague inferences they support:
the minimum degree of height *tall* requires to hold of entities of which it is true remains uncertain on any use of (@ex-tall-sketch), even while the adjective's meaning plausibly does not always vary across such uses.

(@ex-tall-sketch) Jo is tall.

In general, we conceptualize unresolved uncertainty as reflecting the uncertainty that one has about a given inference at a particular point in some discourse, having fixed the meanings of the linguistic expressions.

Put slightly differently, resolved uncertainty is a property of one's knowledge about the meanings of expressions *qua* expressions.
Sometimes *run* means this;
sometimes it means that.
Thus, any analysis of the uncertainty about the meaning of *run* should capture that it is uncertainty about *types* of utterance act.
In contrast, unresolved uncertainty encompasses any semantic uncertainty which remains, having fixed the type of utterance act---it is uncertainty pertaining to the semantically licensed inferences themselves.^[
  See @beaver_presupposition_1999 and @beaver_presupposition_2001, which describe an analogous bifurcation of orders of pragmatic reasoning in the representation of the common ground.
]

To capture this idea, our approach regards these types of uncertainty as interacting with each other in a restricted fashion by taking advantage of the fact that distributions may be stacked.
Because resolved uncertainty must be resolved in order for one to draw semantically licensed inferences from uses of particular expressions, we take resolved parameters to be *fixed* in the computation of unresolved uncertainty.
This rigid connection among sources of uncertainty is a natural consequence of structuring probabilistic reasoning in terms of stacked probability distributions.

## Discourse states

We follow a common in dynamic semantics practice by regarding discourse states as lists of parameters.
We depart slightly from the usual assumption that these lists are homogenous by treating them as potentially arbitrarily complex, i.e., *heterogeneous* [though see @bumford_dynamic_2022].
As such, they could be structured according to a variety of models sometimes employed in formal pragmatics [e.g., @farkas_reacting_2010].
For example, we will define one parameter of this list to be a representation of the Stalnakerian common ground [or more aptly, the "context set": @stalnaker_assertion_1978 et seq.] and another parameter to be a stack of Questions Under Discussion [QUDs: @ginzburg_dynamics_1996; @roberts_information_2012].

We represent common grounds as probability distributions over indices encoding information about possible worlds, as well as what we call *contexts*.
The possible world part of an index represents facts about how the (non-linguistic) world is---e.g., a particular individual's height---while the context part encodes certain facts about lexical meaning---e.g., the identity of the height threshold conveyed by a vague adjective, such as *tall* [see, i.a.: @kennedy_scale_2005; @kennedy_vagueness_2007; @lassiter_vagueness_2011].

Utterances---and more broadly, discourses---map tuples of parameters onto probability distributions over new tuples of parameters.
Moreover, complex linguistic acts may be *sequenced*;
in general, the effect on an ongoing discourse of multiple linguistic acts may be computed by using the sequencing operation (*bind*) native to the probability monad.
In this sense, compositionality of interpretation obtains in PDS from the level of individual morphemes all the way up to the level of complex exchanges.
For example, a discourse may consist in (i) making an assertion, which (perhaps, under a simplified model) modifies the common ground; 
(ii) asking a question, which adds a QUD to the top of the QUD stack;
or (iii) a sequence of these.
Regardless, we require the functions encoding discourses to return probabilistic values, in order to capture their inherent uncertainty.

## Linking models

A linking model takes a discourse as conceived above, together with an initial probability distribution over discourse states, and links them to a distribution over responses to the current QUD.
The possible responses to the QUD are determined by a data collection instrument, which could be a Likert scale, a slider scale, or something else.
Furthermore, the *distribution* over responses is fixed by a likelihood function whose choice is constrained by the nature of the values encoded by the instrument. 
Thus a Bernoulli distribution for instruments that produce binary values; 
a categorical distribution for instruments that produce unordered, multivalued discrete responses;
a linked logit distribution for instruments that produce ordered, multivalued discrete responses;
and so on.

## Haskell

Throughout these sets of notes, we include code snippets in the [Haskell](https://www.haskell.org/) programming language to illustrate concepts that we introduce.
There is a working Haskell implementation of PDS, which is currently undergoing further development, and which can translate PDS models into minimal pieces of code in the [Stan](https://mc-stan.org/) programming language for several of the example modeling cases that we will discuss.
Since the components of PDS are presented with their computational implementation in mind, we think it is particularly revealing to see the code itself.
Thus we will interleave relevant code with the prose and semantic formulae.

# Syntax, meaning, compositionality

The syntactic and semantic substrate we employ is Combinatory Categorial Grammar (CGG).
CCG is a highly lexicalized grammar formalism, in which expressions are equipped with syntactic types---i.e., *categories*.
Syntactic types in CCG encode an expression's selectional and distributional properties.
A noun phrase such as *a race*, for example, may be given the type $np$, while a determiner---something which, in English, occurs to the left of a noun in order to form a noun phrase---may be given the type $np / n$.
Thus the forward direction of the slash indicates that a noun should occur to the *right* of the determiner.

We use CCG in our presentation of PDS because one of our goals is to write *semantic grammar fragments* which produce analyses of a given collection of probabilistic semantic phenomena.
Having a grammar fragment (e.g., one which generates the stimuli about which inference judgments are experimentally collected to create some linguistic dataset) allows one to implement an unbroken chain that connects the semantic analysis of some phenomenon to a probabilistic model of judgments about expressions featuring the phenomenon.
CCG is likely to be sufficiently expressive to capture most (if not all) of the kinds of syntactic dependencies found in natural languages (@joshi_tree_1985, @vijay-shanker_equivalence_1994 et seq.; cf. @kobele_generating_2006).
Meanwhile, because it is semantically transparent, it makes writing such grammar fragments relatively straightforward.

## CCG

For current purposes, we can assume the following small set of atomic syntactic types.
$$
\begin{align*}
\mathcal{A} &\Coloneqq np ‚à£ n ‚à£ s
\end{align*}
$$
Here we have the usual categories for noun phrases ($np$), nouns ($n$), and sentences ($s$).
$$
\begin{align*}
\mathcal{C}_{\mathcal{A}} &\Coloneqq \mathcal{A} ‚à£ \mathcal{C}_{\mathcal{A}}/\mathcal{C}_{\mathcal{A}} ‚à£ \mathcal{C}_{\mathcal{A}}\backslash\mathcal{C}_{\mathcal{A}}
\end{align*}
$$
Thus following \Cref{def:atomic_cats}, $\mathcal{C}_{\mathcal{A}}$ includes the five elements of $\mathcal{A}$, as well as
\begin{align*}
  s/np, s\backslash np, np/n, (np\backslash n)/ np, (s\backslash s)/s,
\end{align*}
and so on.
Any complex syntactic type in $\mathcal{C}_{\mathcal{A}}$ features slashes, which indicate on which side an expression of that type takes its argument.
Thus an expression of type $b/ a$ (for some two types $a\) and \(b$) occurs with an expression of type $a$ on its right in order to form an expression of type $b$, while an expression of type $b\backslash a$ occurs with an expression of type $a$ on its *left* in order to form an expression of type $b$.
We adopt the convention of notating syntactic types without parentheses when possible, under the assumption that they are left-associative;
i.e., $a‚à£_{1}b‚à£_{2}c ‚âù (a‚à£_{1}b)‚à£_{2}a$ (where $‚à£_{1}$ and $‚à£_{2}$ are either forward or backward slashes).
Thus for example, the type $s\backslash(np/ np)$ continues to be written as such, while the type $(s\backslash np)/ np$ may be shortened to '$s\backslash np/ np$'.

In Haskell, we can introduce a single data type to encode both atomic categories and categories featuring slashes.

``` haskell
data Cat = NP | N | S  -- atomic categories
         | Cat :/: Cat -- the forward slash
         | Cat :\: Cat -- the backward slash
  deriving (Eq)
```

To write CCG expressions, we use the notation
\begin{align*}
  \expr{s}{m}{c}
\end{align*}
which is to be read as stating that string $s$ has category $c$ and semantic value $m$.
We assume $s$ to be a string over some alphabet $Œ£$ (i.e., $s ‚àà Œ£^{*}$), which we regard as a finite set;
e.g., the set of ``morphemes of English''.
Meanwhile, we assume $m$ to be a typed Œª-term.
We leave somewhat open the question of what types of Œª-terms may be used to define semantic values, but we adopt at least the typing rules in \Cref{fig:typing_lc}.
Assuming that all semantic values are closed terms, we therefore have abstractions ($Œªx.t$), applications ($t(u)$), and $n$-ary tuples ($‚ü®t_{1}, ‚ãØ, t_{n}‚ü©$), along with the empty tuple $‚ãÑ$.
We additionally assume that Œª-terms can feature constants, drawn from some countable set.

As for the semantic types themselves, we can assume that there are the following atomic types, where $e$ is the type of entities, and $t$ is the type of the truth values $\True$ and $\False$.

\begin{align*}
A \Coloneqq e ‚à£ t
\end{align*}

The full set of types over $A$ ($\mathcal{T}_{A}$) is then defined as follows:

\begin{align*}
   \mathcal{T}_{A} \Coloneqq A ‚à£ \mathcal{T}_{A} ‚Üí \mathcal{T}_{A} ‚à£ \mathcal{T}_{A} √ó \mathcal{T}_{A} ‚à£ ‚ãÑ
\end{align*}

Types can be encoded in Haskell via two data types for atomic and complex types, respectively:

```haskell
-- | Atomic types for entities and truth values.
data Atom = E | T deriving (Eq, Show)

-- | Arrows, and products, as well as type variables for encoding polymorphism.
data Type = At Atom
          | Type :‚Üí Type
          | Unit
          | Type :√ó Type
          | TyVar String
  deriving (Eq)
```

Note that the Haskell encoding allows types to be polymorphic, by allowing type variables (`TyVar String`).
Our use of polymorphism is fairly restricted, however, in that any type variable may only be quantified at the top level.
Thus functions and values may themselves be polymorphic---their types can be underspecified---but functions may not take polymorphic values as arguments;
any ambiguity about the type of an argument must be global.
This means that while an expression such as $Œªx.x$ has the polymoprhic type $Œ± ‚Üí Œ±$, the expression $Œªf.f(Œªy.y)(f(Œªx, y.x))$, where $f$ must have two distinct types in each of its bound occurrences, will not receive a type.
Including such limited polymorphism is useful because it allows for the inclusion of certain kinds of polymorphic constants, e.g., the universal quantifier $‚àÄ : (Œ± ‚Üí t) ‚Üí t$, which we may wish to be able to quantify not only over entities, but other types of objects (e.g., real numbers, or even functions).

Just as with complex syntactic types, we adopt the convention of notating complex semantic types without parentheses when possible.
Unlike syntactic types, we assume semantic types are right-associative.^[
  These conventions mirror each other in the sense that the input type of a function type is assumed to be atomic unless otherwise specified by the use of parentheses.
Typing rules for typed Œª-terms may then be given as follows:

$$ \small
\begin{array}{c}
\begin{prooftree}
\AxiomC{}
\RightLabel{$\mathtt{Ax}$}\UnaryInfC{$Œì, x : Œ± ‚ä¢ x : Œ±$}
\end{prooftree}
& \begin{prooftree}
\AxiomC{$Œì, x : Œ± ‚ä¢ t : Œ≤$}
\RightLabel{${‚Üí}\mathtt{I}$}\UnaryInfC{$Œì ‚ä¢ Œªx.t : Œ± ‚Üí Œ≤$}
\end{prooftree}
& \begin{prooftree}
\AxiomC{$Œì ‚ä¢ t : Œ± ‚Üí Œ≤$}
\AxiomC{$Œì ‚ä¢ u : Œ±$}
\RightLabel{${‚Üí}\mathtt{E}$}\BinaryInfC{$Œì ‚ä¢ t(u) : Œ≤$}
\end{prooftree} \\[2mm]
\begin{prooftree}
\AxiomC{}
\RightLabel{$‚ãÑ\mathtt{I}$}\UnaryInfC{$Œì ‚ä¢ ‚ãÑ : ‚ãÑ$}
\end{prooftree}
& \begin{prooftree}
\AxiomC{$Œì ‚ä¢ t : Œ±$}
\AxiomC{$Œì ‚ä¢ u : Œ≤$}
\RightLabel{$√ó\mathtt{I}$}\BinaryInfC{$Œì ‚ä¢ ‚ü®t, u‚ü© : Œ± √ó Œ≤$}
\end{prooftree}
& \begin{prooftree}
\AxiomC{$Œì ‚ä¢ t : Œ±_1 √ó Œ±_2$}
\RightLabel{$√ó\mathtt{E}_{j}$}\UnaryInfC{$Œì ‚ä¢ œÄ_{j}(t) : Œ±_{j}$}
\end{prooftree}
\end{array}
$$

These cover Œª-abstractions, applications, the unit type $‚ãÑ$ (which is inhabited by the empty tuple $‚ãÑ$), pairing, and projections.

We provide an untyped implementation of Œª-terms in Haskell;
meanwhile, we provide a separate mechanism for doing type inference for these terms.^[
	Our Œª-terms are thus Curry-typed, so that operations on terms need only attend to their syntax.
	The alternative, Church-typing, makes types an inherent part of the terms themselves.
]

```haskell
-- | Untyped Œª-terms. Types are assigned separately (i.e., "extrinsically").
data Term = Var VarName           -- Variables.
          | Con Constant          -- Constants.
          | Lam VarName Term      -- Abstractions.
          | App Term Term         -- Applications.
          | TT                    -- The 0-tuple.
          | Pair Term Term        -- Pairing.
          | Pi1 Term              -- First projection.
          | Pi2 Term              -- Second projection.
```

This implementation of the Œª-calculus allows it to feature constants.
We allow for both the Haskell `Double` type to be encoded as constants, as well as the `String` type.
Allowing doubles to be constants will make, e.g., arithmetic computations more straightforward as the system is further developed.
```haskell
-- | Constants are indexed by either strings or real numbers.
type Constant = Either String Double
```

Although we employ atomic types only for entities and truth values, we will make use of a form of intensionality in our semantic fragments, so that meanings will generally depend on an index of evaluation (which we typically denote '$i$').
However, we make no commitments about its type, thus allowing expressions' meanings to be polymorphic---this choice will be justified later on in these notes, when we introduce the full system.
Meanwhile, we'll provide the polymorphic types of such meanings using Greek letters to represent type variables (e.g., $Œπ$ for $i$), while retaining Latin letters for atomic types.

In CCG, expressions are combined to form new expressions using application rules, as well as composition ($\textbf{B}$) rules and (often) type-raising ($\textbf{T}$) and substitution ($\textbf{S}$) rules (see, e.g., @steedman_syntactic_2000).
The string *every linguist* can be derived by *right* application from expressions for the strings *every* and *linguist*, for example.

(@ex-every-linguist)
$$ \small
\frac{\expr{\textit{every}}{Œªp, q, i.‚àÄy.p(y)(i) ‚Üí q(y)(i)}{s}/(s\backslash np)/ n \hspace{1cm} \expr{\textit{linguist}}{Œªx, i.\ct{ling}(i)(x)}{n}}{
\expr{\textit{every linguist}}{Œªq, i.‚àÄy.\ct{ling}(i)(y) ‚Üí q(y)(i)}{s}/(s\backslash np)
}>
$$

The resulting expression has the syntactic type of a quantifier;
in this case, it takes on its right an expression which takes a noun phrase on its left to form a sentence, and it forms a sentence with that expression.
This type---$s/(s\backslash np)$---is mirrored by the type of the Œª-term which is the expression's semantic value:
$(e ‚Üí Œπ ‚Üí t) ‚Üí Œπ ‚Üí t$.
Indeed, the two are related by a *type homomorphism*;
i.e., a map from syntactic types to semantic types that preserves certain structure---here, the structure of syntactic types formed via slashes ($/$ and $\backslash$), which get turned into semantic types formed via arrows ($‚Üí$).
We may codify the behavior of this homomorphism on atomic syntactic types.

(@ex-type-interp)
$‚ü¶np‚üß = e$ <br>
$‚ü¶n‚üß = e ‚Üí Œπ ‚Üí t$ <br>
$‚ü¶s‚üß = Œπ ‚Üí t$

The CCG derivation given in (@ex-every-linguist) tacitly assumes that noun phrases denote entities, that nouns denote functions from entities to *propositions* (i.e., functions of type $Œπ ‚Üí t$), and that sentences denote propositions.

Crucially, *every* CCG rule is analogous to the application rules in that it preserves the structure of syntactic types in the types of semantic values via the type homomorphism.
For another example, the rightward composition rule can be used to combine *every linguist* with *saw*.

(@ex-every-linguist-saw)
$$ \small
\frac{\expr{\textit{every linguist}}{Œªq, i.‚àÄy.\ct{ling}(i)(y) ‚Üí q(y)(i)}{s}/ (s\backslash np)
\hspace{1cm}
\expr{\textit{saw}}{Œªx, y, i.\ct{see}(i)(x)(y)}{s}\backslash np/ np}{
\expr{\textit{every linguist saw}}{Œªx, i.‚àÄy.\ct{ling}(i)(y) ‚Üí \ct{see}(i)(x)(y)}{s}/ np
}{>}\textbf{B}
$$

Here, the resulting type---$s/ np$---is mapped to $‚ü¶np‚üß ‚Üí ‚ü¶s‚üß = e ‚Üí Œπ ‚Üí t$, which is precisely the type of the resulting semantic value.

## Adding probabilistic types

The type system presented in above included types for entities, truth values, and types formed from these.
PDS is inspired by the presentation in @grove_probabilistic_2023, who illustrate how a semantics incorporating Bayesian reasoning can be encoded using a Œª-calculus with such a frugal type system;
however, whereas @grove_probabilistic_2023 represent probabilistic reasoning using continuations, we employ a somewhat more abstract presentation by incorporating a new type constructor ($\P$).
In addition, we add a type $r$ to represent real numbers, for the following new set of atomic types.

$$
A \Coloneqq e ‚à£ t ‚à£ r
$$

Then, the full (and final) set of types can be given as follows.

$$
\mathcal{T}_{A} \Coloneqq A ‚à£ \mathcal{T}_{A} ‚Üí \mathcal{T}_{A} ‚à£ \mathcal{T}_{A} √ó \mathcal{T}_{A} ‚à£ ‚ãÑ ‚à£ \P \mathcal{T}_{A}
$$

In Haskell

```haskell
-- | Atomic types for entities, truth values, and real numbers.
data Atom = E | T | R deriving (Eq, Show)

-- | Arrows, products, and probabilistic types, as well as type variables for
-- encoding polymorphism.
data Type = At Atom
          | Type :‚Üí Type
          | Unit
          | Type :√ó Type
          | P Type
          | TyVar String
  deriving (Eq)
```

Types of the form $\P Œ±$ are inhabited by *probabilistic programs* that represent probability distributions over values of type $Œ±$.
For example, a program of type $\P t$ represents a probability distribution over truth values (i.e., a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution)); 
a program of type $\P e$ represents a probability distribution over entities (e.g., a [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution});
a program of type $\P r$ represents a probability distribution over real numbers (e.g., a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution));
and a program of type $\P (e ‚Üí t)$ represents a probability distribution over functions from entities to truth values.
Given the new inventory of probabilistic types, probabilistic programs are typed as follows:

$$
\begin{array}{c}
\begin{prooftree}
\AxiomC{$Œì ‚ä¢ t : Œ±$}
\RightLabel{$\mathtt{Return}$}\UnaryInfC{$Œì ‚ä¢ \pure{t} : \P Œ±$}
\end{prooftree}
& \begin{prooftree}
\AxiomC{$Œì ‚ä¢ t : \P Œ±$}
\AxiomC{$Œì, x : Œ± ‚ä¢ u : \P Œ≤$}
\RightLabel{$\mathtt{Bind}$}\BinaryInfC{$Œì ‚ä¢ \left(\begin{array}{l} x ‚àº t \\ u\end{array}\right) : \P Œ≤$}
\end{prooftree}
\end{array}
$$

Thus there are to constructors that can be used to produce typed probabilistic programs;
we call these '*return*', and '*bind*'.
The $\mathtt{Return}$ rule effectively turns any value $t$ into a [degenerate distribution](https://en.wikipedia.org/wiki/Degenerate_distribution);
i.e., a probability distribution all of whose probability mass is assigned to the value $t$.
We denote this distribution by wrapping the relevant value in an orange box, as shown.
Meanwhile, the $\mathtt{Bind}$ rule allows one to compose probabilistic programs together.
Given some program $t$, one can sample a value ($x$) from $t$ and then keep going with the program $u$.
We describe some of the interactions between return and bind in a little more detail (and with some illustrative examples) next.

We also upgrade our Œª-terms in Haskell to reflect these constructors:

```haskell
-- | Untyped Œª-terms. Types are assigned separately (i.e., "extrinsically").
data Term = Var VarName           -- Variables.
          | Con Constant          -- Constants.
          | Lam VarName Term      -- Abstractions.
          | App Term Term         -- Applications.
          | TT                    -- The 0-tuple.
          | Pair Term Term        -- Pairing.
          | Pi1 Term              -- First projection.
          | Pi2 Term              -- Second projection.
          | Return Term           -- Construct a degenerate distribution.
          | Let VarName Term Term -- Sample from a distribution and continue.
```
Here, the bind operator is notated using `Let`:
`Let x t u` is to be read as

$$
\begin{array}{l}
x ‚àº t \\
u
\end{array}
$$

### The probability monad

Importantly, the map $\P$ from types to types is defined to be a *monad*.
The typing rules given above feature one rule corresponding to each of two different *monadic operators*:
$\mathtt{Return}$ (an introduction rule) and $\mathtt{Bind}$ (an elimination rule).^[
  See \cite{bernardy_bayesian_2022}, where similar rules are presented in a somewhat more refined, dependently typed setting.
]
As a monad, $\P$ (together with return and bind) should satisfy the following *monad laws*;
i.e., the following equalities---*Left identity*, *Right identity*, and *Associativity*---should be supported:

$$
\begin{array}{c}
\textit{Left identity} & \textit{Right identity} & \textit{Associativity} \\[1mm]
\begin{array}{l}
x ‚àº \pure{v} \\
k
\end{array}\ \ =\ \ k[v/x] 
& \begin{array}{l}
x ‚àº m \\
\pure{x}
\end{array}\ \ =\ \ m 
& \begin{array}{l}
y ‚àº \left(\begin{array}{l}
x ‚àº m \\
n
\end{array}\right) \\
o
\end{array}\ \ =\ \ \begin{array}{l}
x ‚àº m \\
y ‚àº n \\
o
\end{array}
\end{array}
$$

These provide tight constraints on the behavior probabilistic programs.
Left identity says that sampling a value from a degenerate distribution (via return and bind) is trivial:
it can only result in the single value that the degenerate distribution assigns all of its mass to.
The law encodes this fact by allowing one to simply continue with rest of the relevant probabilistic program ($k$, whatever that may be), but with the returned value $v$ substituted for the sampled value $x$. 

What Right identity says is sort of symmetrical:
sampling a value from a program $m$ and immediately returning that value as new degenerate distribution is *also* trivial;
you can always get rid of this extra step.

Finally, Associativity says that sampling a value ($y$) from a complex probabilistic program is the same as sampling it from the distribution defined in the final step of this program.
For example, if one has one normal distribution parameterized by a value sampled from another,

$$
\begin{array}{l}
y ‚àº \left(\begin{array}{l}
x ‚àº \abbr{Normal}(0, 1) \\
\abbr{Normal}(x, 1)
\end{array}\right) \\
o
\end{array}
$$

one can always pull out the parts of this complex distribution to yield a series of bind statements:

$$
\begin{array}{l}
x ‚àº \abbr{Normal}(0, 1) \\
y ‚àº \abbr{Normal}(x, 1) \\
o
\end{array}
$$

### Some examples

We can recruit return and bind to characterize complex probability distributions.
To illustrate, suppose we have some categorical distribution, $\ct{mammal}: \P e$, on mammals.
We can represent a distribution on mammals' mothers as in (@ex-mother).

(@ex-mother)
$$
\begin{array}[t]{l}
x ‚àº \ct{mammal}\\
\pure{\ct{mother}(x)}
\end{array}
$$

Here, a random entity $x: e$ is *sampled* from $\ct{mammal}: \P e$ using bind, and then $\ct{mother}(x): e$ is returned, as indicated by the orange box.
Since return turns things of type $Œ±$ into probabilistic programs of type $\P Œ±$, the resulting probabilistic program is of type $\P e$.
Furthermore, assuming that the probability distribution $\ct{mammal}$ only has support on (i.e., assigns non-zero probability to) the entities which are mammals, the distribution which results will only have support on the entities which are the mothers of entities which are mammals.

#### Reweighting distributions

Our probabilistic language also comes with an operator $\ct{factor}$ for scaling probability distributions according to some weight.^[
  We define $\ct{factor}$ here as a primitive of the language of probabilistic programs (i.e., a constant).
  In their continuation-based treatment, \cite{grove_probabilistic_2023} implement $\ct{factor}$ so that it has the scaling behavior described only informally here.
  One could (if they wanted to) interpret the current system into one that uses continuations, so that $\ct{factor}$ has the behavior needed.
]

(@ex-factor)
$$\ct{factor} : r ‚Üí \P ‚ãÑ$$

For instance, we may constrain our "mother" distribution so that it assigns more weight to the mothers of mammals which are hungrier.

(@ex-hungry-mother)
$$\begin{array}[t]{l}
x ‚àº \ct{mammal} \\
\ct{factor}(\ct{hungry}(x)) \\
\pure{\ct{mother}(x)}
\end{array}$$

Here, $\ct{hungry} : e ‚Üí r$ maps entities onto degrees representing how hungry they are.
Thus the program above represents a probability distribution over entities which assigns non-zero probabilities only to entities which are the mother of some mammal, and which assigns greater probabilities to entities the hungrier their children are.

#### Making observations

In terms of $\ct{factor}$, we may define another function, $\ct{observe}$.

(@ex-observe)
$$
\begin{align*}
\ct{observe}\ \ &:\ \ t ‚Üí \P ‚ãÑ \\
\ct{observe}(p)\ \ &=\ \ \ct{factor}(ùüô(p))
\end{align*}
$$

$\ct{observe}$ takes a truth value and either keeps or throws out the distribution represented by the expression which follows it, depending on whether this truth value is $\True$ or $\False$.
This is accomplished by factoring a distribution by the value of an indicator function ($ùüô$) applied to the truth value.^[
  See @grove_probabilistic_2023 for further details.
]

(@ex-indicator)
$$
\begin{align*}
ùüô\ \ &:\ \ t ‚Üí r \\
ùüô(\True)\ \ &=\ \ 1 \\
ùüô(\False)\ \ &=\ \ 0
\end{align*}
$$

For instance, we may instead constrain our "mother" program to describe a distribution over only dogs' mothers.

(@ex-dog-mother)
$$
\begin{array}[t]{l}
x ‚àº \ct{mammal} \\
\ct{observe}(\ct{dog}(x)) \\
\pure{\ct{mother}(x)}
\end{array}
$$

This distribution assigns a probability of $0$ to any entity which is not the mother of some dog.
Indeed, we could use both $\ct{factor}$ and $\ct{observe}$ to define another distribution which assigns a probability of $0$ to any entity which is not the mother of some dog, and which assigns greater probabilities to mothers of hungrier dogs.

(@ex-hungry-dog-mother)
$$
\begin{array}[t]{l}
x ‚àº \ct{mammal} \\
\ct{factor}(\ct{hungry}(x)) \\
\ct{observe}(\ct{dog}(x)) \\
\pure{\ct{mother}(x)}
\end{array}
$$

### Typing constants in Haskell

The implemented system relies on a [Hindley-Milner](https://en.wikipedia.org/wiki/Hindley%E2%80%93Milner_type_system)-style type inference algorithm that finds any Œª-term's principal type---i.e., the most general polymorphic type it can have, given its structure and the types of any constants it contains.
We omit the full algorithm here to save space, but it's still useful to illustrate how constants are assigned types.
In general, we rely on *signatures* to type constants, i.e., partial functions from constants to types:

```haskell
-- | Assign types to constants.
type Sig = Constant -> Maybe Type
```
For example, a signature that assigns the appropriate types to $\ct{observe}$ and $\ct{factor}$, as well as constants formed out of `Double`s, is the following one (note that the following syntax requires the `LambdaCase` Haskell language extension):

```haskell
t, r :: Type
t = At T
r = At R

tau :: Sig
tau = \case
  Left  "factor"  -> Just (r :‚Üí P Unit)
  Left  "observe" -> Just (t :‚Üí P Unit)
  Right _         -> Just r
```
It may be useful to think about how `tau` could be extended to accommodate the other expressions mentioned above.

## The common ground

Here, we make good on the assumption mentioned earlier that common grounds amount to probability distributions over indices of some kind.
In general, we will allow the meanings of expressions to be determined by indices in the following way.
For any constants, e.g.,

$$
\begin{align*}
\ct{see} &: Œπ ‚Üí e ‚Üí e ‚Üí t \\
\ct{ling} &: Œπ ‚Üí e ‚Üí t
\end{align*}
$$

etc., there are other constants

$$
\begin{align*}
\updct{see} &: (e ‚Üí e ‚Üí t) ‚Üí Œπ ‚Üí Œπ \\
\updct{ling} &: (e ‚Üí t) ‚Üí Œπ ‚Üí Œπ 
\end{align*}
$$

which may update some index $i$ with a particular value.
Thus our theory of indices is effectively a theory of *states and locations*:
any given index represents a kind of state;
meanwhile, constants such as $\ct{see}$ and $\ct{ling}$ represent different locations associated with that state.
For example, given some index $i$, $\updct{see}(p)(i)$ is a new index just like $i$, but where the value stored at the location $\ct{see}$ has been overwritten by $p$.
As a result, our constants should satisfy equations like the following:

(@ex-indices-eqs)
$$
\begin{align*}
\ct{see}(\updct{see}(p)(i)) &= p \\
\ct{see}(\updct{ling}(p)(i)) &= \ct{see}(i) \\[2mm]
\ct{ling}(\updct{ling}(p)(i)) &= p \\
\ct{ling}(\updct{see}(p)(i)) &= \ct{ling}(i)
\end{align*}
$$

That is, when $\ct{see}$ encounters an index which has been updated at its associated location, it grabs the value that the index has been updated with.
If it encounters an index which has been updated at a different location, it keeps looking.
(Similarly, for $\ct{ling}$.)

Finally, we define a common ground to be a probability distribution over indices.

(@ex-common-ground)
Definition:
a *common ground* is a probabilistic program of type $\P Œπ$.

Here, again, $Œπ$ is understood to be a variable over types:
its type doesn't really matter, as long as it can be understood as supporting the theory of states and locations described just above.
We further define a constant representing a *starting* index, which we call '$\ct{@}$'. 

(@ex-starting-index)
$$\ct{@} : Œπ$$

Let's briefly consider a concrete example.
One way of defining a common ground is by encoding a distribution over heights for some entity;
say, Jo.
The following common ground updates the value stored for the constant $\ct{height} : Œπ ‚Üí e ‚Üí r$:

(@ex-jo-cg)
$$
\begin{array}[t]{l}
h ‚àº \abbr{Normal}(0, 1) \\
\pure{\updct{height}(Œªx.h)(\ct{@})}
\end{array}
$$

This common ground encodes uncertainty about Jo's height by associating it with a normal distribution centered at 0 and with a standard deviation of 1.
Note that because we are considering only one individual---Jo---we can update the height value globally.
If we wish to describe a common ground that encodes uncertainty about the heights of more than individual---say, Jo and Bo---we can make the function with which indices are updated a bit more sophisticated:

(@ex-jo-bo-cg)
$$
\begin{array}[t]{l}
h_{j} ‚àº \abbr{Normal}(0, 1) \\
h_{b} ‚àº \abbr{Normal}(0, 1) \\
\pure{\updct{height}(Œªx.\ite(x = \ct{j}, h_{j}, h_{b}))(\ct{@})}
\end{array}
$$

Here, $\ite$ should be understood as satisfying the following two equations:

(@ex-ite)
$$
\begin{align*}
\ite(\True, x, y) &= x \\
\ite(\False, x, y) &= y
\end{align*}
$$

Thus the common ground in (@ex-jo-bo-cg) updates the starting index with a value for $\ct{height}$ consisting of a function that returns $h_{j}$ on the argument $\ct{j}$ (i.e., Jo) and $h_{b}$ otherwise (i.e., when the argument is $\ct{b}$, i.e., Bo)

## Expressions and discourses

### States

We now turn to discourse states.
These, like indices, are understood in terms of a theory of states and locations (and similarly, as having some polymorphic type $œÉ$).
We generally refer to the values stored in discourse states as *metalinguistic parameters*.
These include, e.g., the common ground and the QUD, along with other conversationally relevant features of discourse (e.g., representations of the entities to which pronouns can refer, the available antecedents for ellipsis, etc.).
One can view a discourse state as akin to the context state of @farkas_reacting_2010, though the type of state we employ is in principle less constrained, insofar as the type of individual parameters is open ended.
Since discourse states provide access to the common ground and the QUD, they are associated with constants and equations like the following:

(@ex-states-eqs)
$$
\begin{align*}
\ct{CG}(\updct{CG}(cg)(s)) &= cg \\
\ct{CG}(\updct{QUD}(q)(s)) &= \ct{CG}(s) \\[2mm]
\ct{QUD}(\updct{QUD}(q)(s)) &= q \\
\ct{QUD}(\updct{CG}(cg)(s)) &= \ct{QUD}(s)
\end{align*}
$$

Though we haven't yet discussed the types we take to be associated with QUDs, note that $\ct{CG}$ and $\updct{CG}$ ought to have the following types:

(@ex-cg-types)
$$
\begin{align*}
\ct{CG} &: œÉ ‚Üí \P Œπ \\
\updct{CG} &: \P Œπ ‚Üí œÉ ‚Üí œÉ
\end{align*}
$$

Finally, as for indices, we provide a constant $\ct{œµ}$ representing a "starting" state:

(@ex-epsilon)
$$
\ct{œµ} : œÉ
$$

### Expression meanings

We regard expressions' probabilistic semantic values as functions of type $œÉ ‚Üí \P (Œ± √ó œÉ^{\prime})$, where $œÉ$ and $œÉ^{\prime}$ should be understood to represent the types of discourse states.
We abbreviate this type as $‚Ñô^{œÉ}_{œÉ^{\prime}} Œ±$:

(@ex-p-abbrev)
$$
‚Ñô^{œÉ}_{œÉ^{\prime}} Œ± ‚âù œÉ ‚Üí \P (Œ± √ó œÉ^{\prime})
$$

Thus given an input state $s : œÉ$, the semantic value of an expression produces a probability distribution over pairs of ordinary semantic values of type $Œ±$ and possible output states $s^{\prime} : œÉ^{\prime}$.
An expression of category $np$, for instance, now has the *probabilistic* type $‚Ñô^{œÉ}_{œÉ^{\prime}}(‚ü¶np‚üß) \,\, = \,\, ‚Ñô^{œÉ}_{œÉ^{\prime}} e \,\, = \,\, œÉ ‚Üí \P (e √ó œÉ^{\prime})$.

Building on this view of expressions, we regard an ongoing discourse as a function of type $‚Ñô^{œÉ}_{œÉ^{\prime}} ‚ãÑ$.
The effects that both expressions and discourses have are therefore *stateful-probabilistic*:
they map input states to probability distributions over output states.
Discourses differ from expressions in that the value discourses compute is trivial:
it is invariably the empty tuple $‚ãÑ$, as determined by its type.
Thus while expressions produce both stateful-probabilistic effects *and* values, discourses have *only* effects, i.e., they merely update the state.

#### Program composition via parameterized monads

The setup we have introduced allows for the possibility that the state parameter $œÉ$ *changes* in the course of evaluating an expression's probabilistic semantic value.
Such a value may map an input state $s : œÉ$ onto a probability distribution over outputs states of type $œÉ^{\prime}$ ($œÉ^{\prime} ‚â† œÉ$).
This flexibility is useful to capture the changing nature of certain components of the discourse state.
For example, the QUDs stored in a state may consist of questions of different types---e.g., degree questions, individual questions, etc.
Thus whenever an utterance functions to add a QUD to the state, the input state's type may not match the output state's type.

To countenance such type-level flexibility, we view the types $‚Ñô^{œÉ}_{œÉ^{\prime}} Œ±$ as arising from a *parameterized* State.Probability monad, given the set $\mathcal{T}_{A}$ of types as the relevant collection of parameters.^[ 
  See @atkey_parameterised_2009 on the parameterized State monad and parameterized monads more generally.
  The current parameterized monad can be viewed as applying a parameterized State monad *transformer* to the underlying probability monad $\P$;
  see @liang_monad_1995 on monad transformers.
]
Parameterized monads are associated with their own definitions of (parameterized) return and bind.
To increase clarity, while distinguishing the notations for parameterized and vanilla monads, we present the bind statements of a parameterized monad $‚Ñô$ using Haskell's $\Do$-notation.

(@ex-parameterized-monads)
Given a collection $\mathcal{S}$ of parameters, a parameterized monad is a map $‚Ñô$ from triples consisting of two parameters and a type onto types (i.e., given parameters $p, q ‚àà \mathcal{S}$ and a type $Œ±$, $‚Ñô^{p}_{q} Œ±$ is some new type), equipped with two operators satisfying the parameterized monad laws in (@ex-p-monad-laws).
$$
\begin{align*}
\return{(¬∑)}_{p}\ \ &:\ \ Œ± ‚Üí ‚Ñô^{p}_{p} Œ± \tag{`return'} \\
\begin{array}{rl}
\Do_{p, q, r} & x ‚Üê \_\_ \\
 & \_\_(x)
\end{array}\ \
&:\ \ ‚Ñô^{p}_{q} Œ± ‚Üí (Œ± ‚Üí ‚Ñô^{q}_{r} Œ≤) ‚Üí ‚Ñô^{p}_{r} Œ≤ \tag{`bind'}
\end{align*}
$$

The parameterized monad laws themselves appear formally identical to the ordinary monad laws (see (@ex-p-monad-laws));
the crucial difference is their implicit manipulation of parameters moving from the left-hand side of each equality to the right-hand side.

(@ex-p-monad-laws)
$$
\begin{array}{c}
\textit{Left identity} & \textit{Right identity} \\[1mm]
\begin{array}{rl}
\Do_{p, p, q} & x ‚Üê \return{v}_{p} \\
& k(x)
\end{array}\ \ =\ \ \ k(v)
& \begin{array}{rl}
\Do_{p, q, q} & x ‚Üê m \\
& \return{x}_{q}
\end{array} \ \ =\ \ \ m
\end{array}
$$
$$
\begin{array}{c}
\textit{Associativity} \\[1mm]
\begin{array}{rl}
\Do_{p, r, s} & y ‚Üê \left(\begin{array}{rl}
	\Do_{p, q, r} & x ‚Üê m \\
	& n(x)
	\end{array}\right) \\
	& o(y)
\end{array}\ \ =\ \ \begin{array}{rl}
	\Do_{p, q, s} & x ‚Üê m \\
	& \begin{array}{rl} 
		\Do_{q, r, s} & y ‚Üê n(x) \\
		& o(y)
	\end{array}
\end{array}
\end{array}
$$

The $\Do$-notation in the above should be read as saying, "first bind the variable $x$ to the program $m$, and then do $k(x)$''.
Indeed, this statement gives an intuitive summary of what the definition of State.Probability accomplishes:
to bind $m$ to the continuation $k$, one must abstract over an input state $s$ and feed it to $m$, sample a value $x$ paired with an output state $s^{\prime}$ from the result, and finally, feed $x$, along with $s^{\prime}$, to $k$.

In practice, we will leave the parameters implicit when we use this notation.
We also suppress superfluous uses of $\Do$-notation, writing

$$
\begin{array}{rl}
\Do & x ‚Üê m \\
	& y ‚Üí m \\
	& n
\end{array}
$$

for

$$
\begin{array}{rl}
\Do & x ‚Üê m \\
	& \begin{array}{rl}
	\Do & y ‚Üí m \\
	& n
	\end{array}
\end{array}
$$

We will also sometimes use a "bracket" notation, writing one-liners such as

$$
\Do \{x ‚Üê m; n\}
$$

instead of

$$
\begin{array}{rl}
\Do & x ‚Üê m \\
	& n
\end{array}
$$

to save space.

#### State.Probability

The particular parameterized monad we employ is State.Probability, where the relevant collection of parameters is $\mathcal{T}_{A}$.

(@ex-state-prob)
$$
\begin{align*}
‚Ñô^{œÉ}_{œÉ^{\prime}} Œ±\ \ &=\ \ œÉ ‚Üí \P (Œ± √ó œÉ^{\prime}) \\[2mm]
\return{v}_{œÉ}\ \ &=\ \ Œªs.\pure{‚ü®v, s‚ü©} \\[2mm]
\begin{array}{rl}
\Do_{œÉ, œÉ^{\prime}, œÉ^{\prime\prime}} & x ‚Üê m \\
& k(x)
\end{array}\ \ 
&=\ \ Œªs.\left(\begin{array}{l}
‚ü®x, s^{\prime}‚ü© ‚àº m(s) \\
k(x)(s^{\prime})
\end{array}\right)
\end{align*}
$$

These definitions can be encoded in Haskell as functions that manipulate terms (ensuring that fresh variables are used when necessary):
```haskell
-- | ** Some convience functions

-- | Variable names are represented by strings.
type VarName = String

-- | Generate an infinite list of variable names fresh for some list of terms.
fresh :: [Term] -> [VarName]

-- | Smart(-ish) constructor for abstractions.
lam :: Term -> Term -> Term
lam (Var v) = Lam v

-- | Smart(-ish) constructor for bind.
let' :: Term -> Term -> Term -> Term
let' (Var v) = Let v

-- | Paramterized return.
purePP :: Term -> Term
purePP t = lam fr (Return (t & fr))
  where fr:esh = map Var $ fresh [t]

-- | Parameterized bind.
(>>>=) :: Term -> Term -> Term
t >>>= u = lam fr (let' e (t @@ fr) (u @@ Pi1 e @@ Pi2 e))
  where fr:e:sh = map Var $ fresh [t, u]
```

In general, it will be useful to have access to a couple of basic operations for retrieving ($\abbr{get}$) and updating ($\abbr{put}$) the state of an ongoing discourse:

(@ex-get-put)
$$
\begin{align*}
\abbr{get} &: ‚Ñô^{œÉ}_{œÉ} œÉ \\
\abbr{get} &= Œªs.\pure{‚ü®s, s‚ü©} \\[2mm]
\abbr{put} &: œÉ^{\prime} ‚Üí ‚Ñô^{œÉ}_{œÉ^{\prime}} \\
\abbr{put}(s^{\prime}) &= Œªs.\pure{‚ü®‚ãÑ, s^{\prime}‚ü©}
\end{align*}
$$

Now, the current state of a given discourse can be retrieved (as $s$) by writing the statement $s ‚Üê \abbr{get}$ inside of a $\Do$-block;
meanwhile, writing the statement $\abbr{put}(s)$ updates this state so that it *becomes* $s$.

These two operators can also be given the following Haskell encodings `getPP` and `putPP`:
```haskell
getPP :: Term
getPP = lam' s (Return (s & s))

putPP :: Term -> Term
putPP s = Lam fr (Return (TT & s))
  where fr:esh = fresh [s]
```

### PDS Rules

We provide our probabilistic CCG rule schemata in (@ex-pds-rules).
These schemata mimic the definitions of ordinary CCG rules, but now semantic values are considered to be of type $‚Ñô^{œÉ}_{œÉ^{\prime}} Œ±$ now, rather than simply of type $Œ±$.
Thus rather than apply CCG operations to semantic values directly, we must bind these semantic values to variables of type $Œ±$ and apply the operations to *those*.

(@ex-pds-rules)
$$ \small
\begin{array}{c}
\begin{prooftree}
\AxiomC{$\expr{s_{1}}{M_{1}}{c/ b}$}
\AxiomC{$\expr{s_{2}}{M_{2}}{b‚à£_{n}a_{n}\,\,‚ãØ‚à£_{1}a_{1}}$}
\RightLabel{${>}\textbf{B}_{n}$}\BinaryInfC{\(\expr{s_{1}\,s_{2}}{
\begin{array}{rl}
\Do & \{\,m_{1} ‚Üê M_{1};\,m_{2} ‚Üê M_{2}; \\
& \return{Œªx_{1}, ‚Ä¶, x_{n}.m_{1}(m_{2}(x_{1})‚Ä¶(x_{n}))}\,\}
\end{array}
}{c‚à£_{n}a_{n}\,\,‚ãØ‚à£_{1}a_{1}}\)}
\end{prooftree} \\[50pt]
\begin{prooftree}
\AxiomC{$\expr{s_{1}}{m_{1}}{b‚à£_{n}a_{n}\,\,‚ãØ‚à£_{1}a_{1}}$}
\AxiomC{$\expr{s_{1}}{m_{2}}{c\backslash b}$}
\RightLabel{${<}\textbf{B}_{n}$}\BinaryInfC{\(\expr{s_{1}\,s_{2}}{
\begin{array}{rl}
\Do & \{\,m_{1} ‚Üê M_{1};\,m_{2} ‚Üê M_{2}; \\
	& \return{Œªx_{1}, ‚Ä¶, x_{n}.m_{2}(m_{1}(x_{1})‚Ä¶(x_{n}))}\,\}
\end{array}
}{c‚à£_{n}a_{n}\,\,‚ãØ‚à£_{1}a_{1}}\)}
\end{prooftree} \\[50pt]
\begin{prooftree}
\AxiomC{$\expr{s_{1}}{M_{1}}{b‚à£_{n}a_{n}\,\,‚ãØ‚à£_{1}a_{1}}$}
\AxiomC{$\expr{s_{2}}{M_{2}}{c\backslash b‚à£_{n}a_{n}\,\,‚ãØ‚à£_{1}a_{1}‚ãØ‚à£_{1}a_{1}}$}
\RightLabel{${<}\textbf{S}_{n}$}\BinaryInfC{\(\expr{s_{1}\,s_{2}}{
\begin{array}{rl}
\Do & \{\,m_{1} ‚Üê M_{1};\,m_{2} ‚Üê M_{2}; \\
& \return{Œªx_{1}, ‚Ä¶, x_{n}.m_{1}(x_{1})‚Ä¶(x_{n})(m_{2}(x_{1})‚Ä¶(x_{n}))}\,\}
\end{array}
}{c‚à£_{n}a_{n}\,\,‚ãØ‚à£_{1}a_{1}}\)}
\end{prooftree} \\[50pt]
\begin{prooftree}
\AxiomC{$\expr{s_{1}}{M_{1}}{c/ b‚à£_{n}a_{n}\,\,‚ãØ‚à£_{1}a_{1}}$}
\AxiomC{$\expr{s_{2}}{M_{2}}{b‚à£_{n}a_{n}\,\,‚ãØ‚à£_{1}a_{1}}$}
\RightLabel{${>}\textbf{S}_{n}$}\BinaryInfC{\(\expr{s_{1}\,s_{2}}{
\begin{array}{rl}
\Do & \{\,m_{1} ‚Üê M_{1};\,m_{2} ‚Üê M_{2}; \\
	& \return{Œªx_{1}, ‚Ä¶, x_{n}.m_{2}(x_{1})‚Ä¶(x_{n})(m_{1}(x_{1})‚Ä¶(x_{n}))}\,\}
\end{array}
}{c‚à£_{n}a_{n}\,\,‚ãØ‚à£_{1}a_{1}}\)}
\end{prooftree}
\end{array}
$$
	
The upshot is that, while an expression's syntactic type continues to determine its compositional properties, its probabilistic, dynamic effects can be stated fairly independently.
	
### Making an assertion

Recall that we represent the meanings of expressions as functions of type $‚Ñô^{œÉ}_{œÉ^{\prime}}$:
given an input state of type $œÉ$, the meaning of an expression produces a probability distribution over *pairs* of ordinary meanings of type $Œ±$ and possible output states of type $œÉ^{\prime}$.
Furthermore, given a sentence whose probabilistic dynamic meaning $œÜ$ is of type $‚Ñô^{œÉ}_{œÉ^{\prime}} (Œπ ‚Üí t)$, we can represent an *assertion* of that sentence as a discourse which updates the common ground.
Specifically, we have a function $\abbr{assert}$:

(@ex-assert)
$$
\begin{align*}
  \abbr{assert} &: ‚Ñô^{œÉ}_{œÉ^{\prime}} (Œπ ‚Üí t) ‚Üí ‚Ñô^{œÉ}_{œÉ^{\prime}} ‚ãÑ \\
  \abbr{assert}(œÜ) &= \begin{array}[t]{rl}
	\Do & p ‚Üê œÜ \\
		& s ‚Üê \abbr{get} \\
		& c ‚Üê \return{\ct{CG}(s)} \\
		& c^{\prime} ‚Üê \return{\left(\begin{array}{l}
		i ‚àº c \\
		\ct{observe}(p(i)) \\
		\pure{i}
		\end{array}\right)} \\
		& \abbr{put}(\updct{CG}(c^{\prime})(s))
	\end{array}
\end{align*}
$$

Given such a $œÜ$, $\abbr{assert}(œÜ)$ is a discourse of type $\P^{œÉ}_{œÉ^{\prime}} ‚ãÑ$ representing an assertion of $œÜ$.
In plain English, $\abbr{assert}(œÜ$) samples a proposition $p$, given $œÜ$, and then updates the common ground of the current state with $p$.
Ultimately, assertions modify an ongoing discourse so that its probability distribution over output states involves common grounds in which the proposition returned by $œÜ$ has been observed to hold true.

Using a few new convenience functions, along with some new short-hands for named variables, $\abbr{assert}$ can be encoded in Haskell:
```haskell
assert :: Term
assert = lam œÜ (œÜ >>>= lam p (getPP >>>= lam s (purePP (cg s) >>>= lam c (purePP (let' i c (let' _' (observe (p @@ i)) (Return i))) >>>= lam d (putPP (upd_CG d s))))))
```



### Asking a question

We follow a categorial tradition by analyzing questions as denoting---given an index---sets of true short answer meanings (see @hausser_questions_1978, @hausser_syntax_1983, and @xiang_hybrid_2021; cf. @karttunen_syntax_1977 and @groenendijk_studies_1984).
Given some type $Œ±$ as the type of the short answer, a question therefore has a probabilistic dynamic meaning of type $‚Ñô^{œÉ}_{œÉ^{\prime}} (Œ± ‚Üí Œπ ‚Üí t)$.

Asking a question is a matter of pushing a question meaning onto the QUD stack [@ginzburg_dynamics_1996;@farkas_reacting_2010].
Reflecting this, we recruit another operation, $\abbr{ask}$:
\begin{align*}
  \abbr{ask} &: ‚Ñô^{œÉ}_{œÉ^{\prime}} (Œ± ‚Üí Œπ ‚Üí t) ‚Üí ‚Ñô^{œÉ}_{\Q Œπ Œ± œÉ^{\prime}} ‚ãÑ \\
  \abbr{ask}(Œ∫) &= \begin{array}[t]{rl}
	\Do & q ‚Üê Œ∫ \\
		& s ‚Üê \abbr{get} \\
		& \abbr{put}(\updct{QUD}(q)(s))
	\end{array}
\end{align*}
Given a probabilistic dynamic question meaning $Œ∫$, $\abbr{ask}(Œ∫)$ samples a question meaning $q : Œ± ‚Üí Œπ ‚Üí t$, given $Œ∫$, and then adds $q$ as a new QUD to the outgoing state.
Note the type of the output state that $\abbr{ask}$ returns:
$\Q Œπ Œ± œÉ^{\prime}$.
$\Q$ is a new map from types to types which, like $\P$, we leave abstract;
given a state type $œÉ^{\prime}$, the meaning of $\Q Œπ Œ± œÉ^{\prime}$ is the type of a new state with a question of type $Œ± ‚Üí Œπ ‚Üí t$ added to the QUD stack.
Thus the type of $\updct{QUD}$ should be as in (@ex-upd-qud-type).

(@ex-upd-qud-type)
$$
\begin{align*}
\updct{QUD} : (Œ± ‚Üí Œπ ‚Üí t) ‚Üí œÉ ‚Üí \Q Œπ Œ± œÉ
\end{align*}
$$

Indeed, we should update the set of types in our Haskell encoding to accommodate the new operator:
```haskell
-- | Arrows, products, and probabilistic types, as well as (a) abstract types
-- representing the addition of a new Q, and (b) type variables for encoding
-- polymorphism.
data Type = At Atom
          | Type :‚Üí Type
          | Unit
          | Type :√ó Type
          | P Type
          | Q Type Type Type
          | TyVar String
  deriving (Eq)
```

Finally, we can also provide an implementation of $\abbr{ask}$:
```haskell
ask :: Term
ask = lam Œ∫' (Œ∫' >>>= lam q (getPP >>>= lam s (putPP (upd_QUD q s))))
```

### Responding to a question

PDS also models responses to questions;
at any point in an ongoing discourse, one can respond to the QUD at the top of the current QUD stack based on one's prior knowledge.
Concretely, a given responder has some background knowledge $bg : \P œÉ$ constituting a prior distribution over *starting* states.
The responder uses this prior, in conjuction with the interim updates to the discourse, to derive a probability distribution over answers to the QUD.
If the set of possible answers are real numbers (e.g., representing degrees of likelihood), this answer distribution is gotten by retrieving the QUD of any given state $s^{\prime}$---resulting in a distribution over QUDs---and then taking the maximum value of which it is true at an index sampled from the common ground---resulting, finally, in a distribution over real numbers.

#### Linking assumptions

In practice (e.g., in the setting of a formal experiment), an answer needs to be given using a particular testing instrument.
We assume that a given testing instrument may be modeled by a family $f$ of distributions representing the likelihood, which is then fixed by a collection $Œ¶$ of nuisance parameters.
For the purposes of the implementation, we assume answers to the current QUD are real numbers, so that for fixed likelihood $f_{Œ¶}$, $f_{Œ¶} : r ‚Üí \P œÅ$, for the type $œÅ$ of responses;
this is not necessary, however, and the types of such functions could be generalized.

Thus we may define a family of *response functions*, parametric in the testing instrument (i.e., likelihood function), each of which takes a distribution $bg$ representing one's background knowledge, along with an ongoing discourse $m$:
\begin{align*}
  \abbr{respond}^{f_Œ¶ : r ‚Üí \P œÅ} &: \P œÉ ‚Üí ‚Ñô^{œÉ}_{\Q Œπ r œÉ^{\prime}} ‚ãÑ ‚Üí \P œÅ \\
  \abbr{respond}^{f_Œ¶ : r ‚Üí \P œÅ}(bg)(m) &= \begin{array}[t]{l}
	s ‚àº bg \\
	‚ü®‚ãÑ, s^{\prime}‚ü© ‚àº m(s) \\
	i ‚àº \ct{CG}(s^{\prime}) \\
	f(\ct{max}(Œªd.\ct{QUD}(s)(d)(i)), Œ¶)
	\end{array}
\end{align*}
For a fixed likelihood function $f_{Œ¶}$ mapping any given real number answer onto a distribution over possible responses of type $œÅ$ (for some $œÅ$), the response function takes a distribution representing background knowledge and a discourse to produce a response distribution.
It does this by composing the discourse with background knowledge, as above, and then obtaining the maximum degree (i.e., real number) answer to the current QUD, before applying the likelihood function $f_{Œ¶}$ to this degree.

The testing instrument employed in the studies we describe in the next few days, for example, is always a slider scale that records responses on the unit interval $[0, 1]$.
A suitable likelihood might therefore be a truncated normal distribution:
$f(x, Œ¶) = \mathcal{N}(x, œÉ)\,\ct{T}[0, 1]$ (so that $f = \mathcal{N}$ and $Œ¶ = œÉ$).
This likelihood---which @grove_factivity_2024 employ in their models of factivity---can be viewed as allowing some distribution of response errors, given the intended target response (i.e., the answer to the question).

Before moving onto some further implementation details, we can also show the Haskell encoding of $\abbr{respond}$, which follows the description above:

```haskell
respond :: Term
respond = lam f (lam bg (lam m (let' s bg m')))
  where m'          = let' _s' (m @@ s) (let' i (cg (Pi2 _s')) (f @@ max' (lam x (qud (Pi2 _s') @@ x @@ i))))
        s:_s':i:x:_ = map Var $ fresh [bg, m]
```

# Further implementation details

## Some more on types

Here we provide a bit more information about the types of constants, as they are encoded in Haskell.
The constants provided here are not exhaustive, but it should be more or less clear from these examples how to generalize the typing scheme to others.

### Logical constants

First, it is useful to have logical constants (e.g., to encode basic meanings for things):

```haskell
-- | Logical constants.
tauLogical :: Sig
tauLogical = \case
  Left "‚àÄ"   -> Just ((Œ± :‚Üí t) :‚Üí t)
  Left "‚àÉ"   -> Just ((Œ± :‚Üí t) :‚Üí t)
  Left "(‚àß)" -> Just (t :‚Üí t :‚Üí t)
  Left "(‚à®)" -> Just (t :‚Üí t :‚Üí t)
  Left "(‚áí)" -> Just (t :‚Üí t :‚Üí t)
  Left "¬¨"   -> Just (t :‚Üí t)
  Left "T"   -> Just t
  Left "F"   -> Just t
  _          -> Nothing
```

Note that the universal and existential quantifiers are typed to be polymorphic in the variable they quantify over.

### Non-logical constants

Second, non-logical constants, e.g., $\ct{ling}$, $\ct{j}$, etc.:

```haskell
-- | Some non-logical constants.
tauNonLogical :: Sig
tauNonLogical = \case
  Left "upd_ling" -> Just ((e :‚Üí t) :‚Üí Œπ :‚Üí Œπ)
  Left "ling"     -> Just (Œπ :‚Üí e :‚Üí t)
  Left "j"        -> Just e
  Left "b"        -> Just e
  Left "@"        -> Just Œπ -- the starting index
  _               -> Nothing
```

Following the discussion [here](#the-common-ground), constants which are intensional have corresponding constants for updating their values.

### Metalinguistc parameters

Following the disccusion [here](#states), we should encode constants that access metalinguistic parameters, i.e., components of the state:

```haskell
-- | Some metalinguistic parameters.
tauMetalinguistic :: Sig
tauMetalinguistic = \case
  Left "upd_CG"  -> Just (P Œπ :‚Üí œÉ :‚Üí œÉ)
  Left "CG"      -> Just (œÉ :‚Üí P Œπ)
  Left "upd_QUD" -> Just ((Œ∫ :‚Üí Œπ :‚Üí t) :‚Üí œÉ :‚Üí Q Œπ Œ∫ œÉ)
  Left "QUD"     -> Just (Q Œπ Œ∫ œÉ :‚Üí Œ∫ :‚Üí Œπ :‚Üí t)
  Left "œµ"       -> Just œÉ -- the starting state
  _              -> Nothing
```

### "Built-in" distributions

Some constants can be used to represent probability (e.g., Bernoulli and normal) distributions, and standard ways of manipulating them:

```haskell
-- | Some probability distributions (and certain ways of manipulating them).
tauDistributions ::Sig
tauDistributions = \case
  Left "Bernoulli" -> Just (r :‚Üí P t)
  Left "Normal"    -> Just (r :√ó r :‚Üí P r)
  Left "Truncate"  -> Just (r :√ó r :‚Üí P r :‚Üí P r)
  Left "#"         -> Just (P Œ±) -- undefined distributions
  _                -> Nothing
```

The third constant, for example, can be used to represent the truncation of some distribution to values within a specified range.
Note also the fourth constant, which encodes "undefined" probability distributions.

### Computing with numbers and truth values

Other constants can be used to do computations with, e.g., truth values and real numbers:

```haskell
-- | Some basic data types (e.g., truth values and reals) and ways of computing with them.
tauBasicStuff :: Sig
tauBasicStuff = \case
  Left  "if_then_else" -> Just (t :√ó Œ± :√ó Œ± :‚Üí Œ±) -- compute /if then else/
  Left  "ùüô"            -> Just (t :‚Üí r)           -- the indicator function
  Left  "mult"         -> Just (r :√ó r :‚Üí r)      -- multiply two numbers
  Left  "add"          -> Just (r :√ó r :‚Üí r)      -- add two numbers
  Left  "neg"          -> Just (r :‚Üí r)           -- add a minus sign
  Left  "(‚â•)"          -> Just (r :‚Üí r :‚Üí t)      -- compare two numbers
  Left  "max"          -> Just ((r :‚Üí t) :‚Üí r)    -- take the maximum number from a set
  Right _              -> Just r                  -- real numbers are constants
  _                    -> Nothing
```

### More probabilistic stuff

Other constants for, e.g., factoring, making observations, and computing probabilities.

```haskell
-- | The probability operator, /factor/, and /observe/.
tauProbabilities :: Sig
tauProbabilities = \case
  Left "Pr"      -> Just (P t :‚Üí r)
  Left "factor"  -> Just (r :‚Üí P Unit)
  Left "observe" -> Just (t :‚Üí P Unit)
  _              -> Nothing
```

### Combining signatures

It would be convenient to have a way, given any two signatures, to combine them.
In Haskell, we can accomplish this with the following function that combines values inhabiting types that instantiate the [`Alternative`](https://hackage.haskell.org/package/monadplus-1.4.3/docs/Control-Applicative-Alternative.html) class:

```haskell
(<||>) :: Alternative m => (a -> m b) -> (a -> m b) -> a -> m b
f <||> g = \x -> f x <|> g x
```

Because `Maybe` is an instance of `Alternative`, we can combine signatures using such a function.
For example, if we want to combine the signature `tauLogical` with the signature `tauNonLogical`, we can do:

```haskell
tauLogicalNonLogical :: Sig
tauLogicalNonLogical = tauLogical <||> tauNonLogical
```

Indeed, we can combine as many signatures as we want in this way, using `(<||>)`.
The resulting signature will type all of the constants that the component signatures type, with signatures listed further to the left taking precedence in case there is any overlap.

## Delta-rules

Now that we have constants, we would like to able to *do things* (i.e., compute) with them.
For example, following the discussion [here](#states), we would like for the encoding of expressions such as

$$
\ct{CG}(\updct{CG}(cg)(s))
$$

to be able to be *evaluated*---in this case---to $cg$.
We implement computations involving constants in terms of what we call *delta-rules*.^[
	Named after Œ¥-reduction.
]
In Haskell, we encode these as the following type of function:

```haskell
-- | The type of Delta rules.
type DeltaRule = Term -> Maybe Term
```

Thus a delta rule is a partial function taking terms onto terms.
It is partial because any given rule may only apply to some constants.
For example, a delta rule that performs arithmetic computations might be defined on constants representing real numbers---but not, for example, on constants representing truth values.

Here we list some example rules.
For clarity of presentation, the rules are defined using Haskell's [`PatternSynonyms`](https://ghc.gitlab.haskell.org/ghc/doc/users_guide/exts/pattern_synonyms.html) language extension.
It should be fairly clear what the relevant synonyms abbreviate.

### Arithmetic

```haskell
-- | Performs some arithmetic simplifications.
arithmetic :: DeltaRule
arithmetic = \case
  Add t u      -> case t of
                    Zero -> Just u
                    x@(DCon _) -> case u of
                                    Zero       -> Just x
                                    y@(DCon _) -> Just (x + y)
                                    _          -> Nothing
                    t'         -> case u of
                                    Zero -> Just t'
                                    _    -> Nothing
  Mult t u     -> case t of
                     Zero       -> Just Zero
                     One        -> Just u
                     x@(DCon _) -> case u of
                                     Zero       -> Just Zero
                                     One        -> Just x
                                     y@(DCon _) -> Just (x * y)
                     t'         -> case u of
                                     Zero -> Just Zero
                                     One  -> Just t'
                                     _    -> Nothing
  Neg (DCon x) -> Just (dCon (-x))
  _            -> Nothing
```

### Tidying up probabilistic programs

``` haskell
-- | Get rid of vacuous let-bindings.
cleanUp :: DeltaRule
cleanUp = \case
  Let v m k | sampleOnly m && v `notElem` freeVars k -> Just k
  _                                                  -> Nothing
```

### The indicator function

```haskell
-- | Computes the indicator function.
indicator :: DeltaRule
indicator = \case
  Indi Tr -> Just 1
  Indi Fa -> Just 0
  _       -> Nothing

```

### Indices

```haskell
-- | Computes functions on indices.
indices :: DeltaRule
indices = \case
  Ling   (UpdLing p _)   -> Just p
  Ling   (UpdSocPla _ i) -> Just (Ling i)
  SocPla (UpdSocPla p _) -> Just p
  SocPla (UpdLing _ i)   -> Just (SocPla i)
  _                      -> Nothing
```

### If then else

```haskell
-- | Computes /if then else/.
ite :: DeltaRule
ite = \case
  ITE Tr x y -> Just x
  ITE Fa x y -> Just y
  _          -> Nothing
```

### Logical operations

```haskell
logical :: DeltaRule
logical = \case
  And p  Tr -> Just p
  And Tr p  -> Just p
  And Fa _  -> Just Fa
  And _  Fa -> Just Fa
  Or  p  Fa -> Just p
  Or  Fa p  -> Just p
  Or  Tr _  -> Just Tr
  Or  _  Tr -> Just Tr
  _         -> Nothing
```

### Computing the max function

```haskell
-- | Computes the /max/ function.
maxes :: DeltaRule
maxes = \case
   Max (Lam y (GE x (Var y'))) | y' == y -> Just x
   _                                     -> Nothing          
```

### Making observations

```haskell
-- | Observing @Tr@ is trivial, while observing @Fa@ yields an undefined
-- probability distribution.
observations :: DeltaRule
observations = \case
  Let _ (Observe Tr) k -> Just k
  Let _ (Observe Fa) k -> Just Undefined
  _                    -> Nothing
```

### Some ways of computing probabilities

```haskell
-- | Computes probabilities for certain probabilitic programs.
probabilities :: DeltaRule
probabilities = \case
  Pr (Return Tr)                                             -> Just 1
  Pr (Return Fa)                                             -> Just 0
  Pr (Bern x)                                                -> Just x
  _                                                          -> Nothing
```

### States

```haskell
-- | Computes functions on states.
states :: DeltaRule
states = \case
  CG      (UpdCG cg _)     -> Just cg
  CG      (UpdQUD _ s)     -> Just (CG s)
  CG      (UpdTauKnow _ s) -> Just (CG s)
  QUD     (UpdQUD q _)     -> Just q
  QUD     (UpdCG _ s)      -> Just (QUD s)
  _                        -> Nothing
```

### Combining delta-rules

Note that delta-rules can be combined just like signatures.
