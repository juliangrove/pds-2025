---
title: "From theory to data"
bibliography: ../../pds.bib
format:
  revealjs:
    css: ../styles.css
    html-math-method: mathjax
    mathjax-config:
      loader: {load: ['[tex]/bussproofs','[tex]/bbox','[tex]/colorbox']}
      tex:
        packages: {'[+]': ['bussproofs','bbox','colorbox']}
---

## The bridge from theory to data

Semantic theory has achieved remarkable success in characterizing compositional structure of meaning

::: {.fragment}
**The opportunity**: Connect elegant formal theories to messy, gradient patterns from large-scale experiments
:::

::: {.fragment}
**The goal**: Maintain theoretical insights while extending to account for empirical richness

[Probabilistic Dynamic Semantics as systematic bridge]{.gb-orange}
:::

::: {.notes}
Welcome everyone. Today we're starting our journey into Probabilistic Dynamic Semantics by first understanding the motivation - why do we need a new framework?

Semantic theory has given us elegant tools for understanding how meaning works compositionally. Think of how we can understand "Every linguist saw a student" by understanding the parts and how they combine. This has been incredibly successful.

But now we face an exciting opportunity. With large-scale experimental methods, we're collecting thousands of judgments from hundreds of speakers. And what we find is messy - gradience everywhere, even where theory predicts categorical distinctions.

Our goal with PDS is not to replace traditional semantics but to extend it. We want to maintain those hard-won theoretical insights while being able to account for and predict the patterns we see in experimental data.

PDS will be our systematic bridge - taking compositional analyses from traditional Montagovian semantics and mapping them to probabilistic models we can test quantitatively.
:::

---

## Semantic theory's success

Elegant formal systems capturing how complex meanings arise from systematic combination

::: {.fragment}
- Decades of careful theoretical work
- Explains acceptability and inference judgments
- Compositional analyses via Montagovian methods
:::

::: {.fragment}
### The challenge
How to test theoretical predictions at unprecedented scale while maintaining formal rigor?
:::

::: {.notes}
Let's appreciate what semantic theory has accomplished. Over decades, semanticists have developed formal systems that elegantly capture compositionality - how complex meanings arise from simpler parts.

These theories explain two fundamental types of judgments speakers make: whether sentences are acceptable in context, and what inferences follow from what we say. The Montagovian tradition gave us tools to be precise about these intuitions.

But here's our challenge: How do we test these theories when we move from a handful of carefully constructed examples to thousands of judgments across hundreds of predicates? How do we maintain formal rigor while engaging with messy empirical reality?

This is what motivates PDS - we need a framework that respects the insights of formal semantics while enabling quantitative evaluation against large-scale data.
:::

---

## Traditional semantic methodology

**Acceptability judgments**: assess whether strings are well-formed in context 
::: {.smaller}
[@chomsky_syntactic_1957; @schütze_gramaticality_2016]
:::

::: {.smaller}
[]{#exm-comitative-good}
[]{#exm-coordination-bad}
- (@exm-comitative-good) What would you like with your coffee? ✓
- (@exm-coordination-bad) #What would you like and your coffee? ✗
::: {.smaller}
[@ross_constraints_1967; @sprouse_island_2021]
:::
:::

::: {.notes}
Traditional semantic methodology centers on two types of judgments. First, acceptability judgments - is a string well-formed in a given context?

Consider these examples. In a context where a host asks what a guest wants with coffee, the first is clearly acceptable - using "with" to indicate accompaniment. But the second, trying to coordinate "what" and "your coffee", is unacceptable. This follows from Ross's coordinate structure constraint.

These judgments have been the bread and butter of linguistic theory. They're usually quite clear - speakers have strong intuitions about what's acceptable and what isn't. This clarity has allowed theorists to build elegant accounts of linguistic constraints.

But as we'll see, when we scale up to hundreds of predicates and thousands of judgments, things get more complex.
:::

---

## Traditional semantic methodology (cont.)

**Inference judgments**: assess relationships between strings 
::: {.smaller}
[@davis_semantics_2004]
:::

::: {.smaller}
[]{#exm-love-antecedent}
[]{#exm-veridicality-inference}
- (@exm-love-antecedent) Jo loved that Mo left. ⟹
- (@exm-veridicality-inference) Mo left.
::: {.smaller}
[@white_lexically_2019]
:::
:::

::: {.notes}
The second pillar of semantic methodology is inference judgments - what follows from what speakers say?

Here's a classic example. When speakers hear "Jo loved that Mo left," they typically infer that Mo actually left. This inference is particularly interesting because it survives under negation and questioning - "Jo didn't love that Mo left" and "Did Jo love that Mo left?" both still suggest Mo left.

This pattern led theorists to identify a class of predicates - factives - that trigger presuppositions about their complements being true. We'll return to factivity as one of our main case studies, because it presents a fascinating puzzle when we look at experimental data.

These inference judgments are central to semantic theory, but they also reveal complexities we need to address.
:::

---

## Additional examples

[]{#exm-anaphora}
**Ambiguity resolution**:
::: {.smaller}
- (@exm-anaphora) Whenever anyone laughed, the magician scowled and their assistant smirked. They were secretly pleased.
- Inference: The assistant is pleased (not necessarily the magician)
:::

::: {.fragment}
[]{#exm-uncle-tall}
**Vagueness**:
::: {.smaller}
- (@exm-uncle-tall) My uncle is tall.
- Inherent uncertainty about height threshold
:::
:::

::: {.fragment}
[]{#exm-uncle-running}
**Resolved uncertainty**:
::: {.smaller}
- (@exm-uncle-running) My uncle is running the race.
- *run* = locomotion vs. management
:::
:::

::: {.notes}
Let me give you some more examples that illustrate different types of uncertainty we encounter.

First, anaphora resolution. In this example, "they" could refer to the magician, the assistant, or both. Most people infer it's the assistant who's pleased. This is an example of ambiguity that gets resolved - once we fix the referent, the inference is clear.

Second, vagueness. When we say "My uncle is tall," there's inherent uncertainty about what counts as tall. Unlike the anaphora case, this uncertainty persists even after we've fixed all ambiguities. There's no precise height threshold for tallness.

Third, lexical ambiguity. "Running the race" could mean participating or organizing. This is resolved uncertainty - once we pick an interpretation, the inferences follow determinately.

These examples preview a crucial distinction we'll develop: between resolved uncertainty (discrete choices) and unresolved uncertainty (inherent gradience). This distinction will be key to understanding our experimental results.
:::

---

## Observational adequacy

**Core desideratum:** predict acceptability and inference patterns for any string 
::: {.smaller}
[@chomsky_current_1964]
:::

- Requires mapping vocabulary to abstractions predicting judgments parsimoniously
- Abstractions: discrete or continuous, simple or richly structured

::: {.fragment}
### Powerful generalizations
- *love*, *hate*, *be surprised*, *know* share properties
- Inferences survive under negation/questioning
- Led to positing shared theoretical properties
::: {.smaller}
[@kiparsky_fact_1970; cf. @karttunen_observations_1971]
:::
:::

::: {.notes}
A core goal of semantic theory is observational adequacy - for any string, we should predict whether speakers find it acceptable and what inferences they draw.

Achieving this requires mapping vocabulary to abstractions. These abstractions might be discrete categories or continuous dimensions, simple features or rich structures. The key is parsimony - explaining the most data with the fewest theoretical commitments.

Through careful analysis, semanticists have identified powerful generalizations. Take predicates like "love," "hate," "be surprised," and "know." They all give rise to inferences about their complements that survive under negation and questioning. This pattern led Kiparsky and Kiparsky to posit a shared property - factivity.

This is theoretical success at its best: identifying deep regularities across superficially different expressions. But as we'll see when we look at experimental data, these generalizations become more complex at scale.
:::

---

## Descriptive adequacy

**Core desideratum:** capturing data "in terms of significant generalizations that express underlying regularities" 
::: {.smaller}
[@chomsky_current_1964, p. 63]
:::

::: {.fragment}
Two approaches:

1. **Analysis-driven**: Start with observationally adequate analyses, extract constraints 
::: {.smaller}
[@chomsky_conditions_1973]
:::
2. **Hypothesis-driven**: Begin with constrained formalisms, test empirical coverage 
::: {.smaller}
[@stabler_derivational_1997]
:::
:::

::: {.fragment}
PDS adopts hypothesis-driven approach for semantics
::: {.smaller}
[@baroni_proper_2022; @pavlick_symbols_2023]
:::
:::

::: {.notes}
Beyond observational adequacy lies a deeper goal: descriptive adequacy. We don't just want to predict the data - we want to capture it in terms of significant generalizations that reveal underlying regularities in language.

The history of generative syntax shows two approaches to achieving this. The analysis-driven approach starts with observationally adequate analyses in expressive formalisms, then extracts constraints. Think of how Chomsky started with transformational grammars and extracted island constraints.

The hypothesis-driven approach works differently - begin with constrained formalisms like CCG or minimalist grammars, then test their empirical coverage. This approach makes phenomena boundaries clearer through representational constraints.

PDS takes the hypothesis-driven approach for semantics. We start with constrained semantic representations and test what phenomena they can capture. This becomes especially important when we want models that both accord with theory AND can be evaluated quantitatively against behavioral data.
:::

---

## Our approach

Probabilistic Dynamic Semantics aims to:

- Provide a relatively unconstrained formalism
- State testable hypotheses about distribution of judgments  
- Test hypotheses using behavioral data

::: {.fragment}
### Key innovation
Delineate phenomena through representational constraints while enabling quantitative evaluation
:::

::: {.notes}
So what's our approach with PDS? We aim to provide a formalism that's relatively unconstrained - flexible enough to encode different theoretical positions - while still being principled and compositional.

The key is that we can state testable hypotheses about the distribution of judgments, not just individual data points. When hundreds of people judge whether "Jo knows that Mo left" implies Mo left, we get a distribution of responses. PDS lets us predict these distributions from our semantic theory.

Our key innovation is maintaining the delineation of phenomena through representational constraints - the hallmark of the hypothesis-driven approach - while enabling quantitative evaluation against behavioral data. 

As we'll see in the implementation, this means building probability distributions into the semantics compositionally, so that when meanings combine, the uncertainty combines too. This lets us maintain theoretical commitments while engaging with empirical complexity.
:::

---

## Traditional methods: Power and boundaries

### Profound insights achieved:
- Semantic composition
- Scope phenomena  
- Discourse dynamics
- Semantics-pragmatics interface

::: {.fragment}
### Natural boundaries:
- How well do generalizations from 5-10 predicates extend to thousands?
- What factors beyond semantic knowledge influence judgments?
- How does abstract knowledge produce concrete behavioral responses?
:::

::: {.notes}
Let's acknowledge what traditional methods have achieved - profound insights into how meaning works. We understand semantic composition, how scope ambiguities arise, how discourse context affects interpretation, and the delicate dance between semantics and pragmatics.

But every methodology has natural boundaries. Traditional methods excel at deep analysis of carefully chosen examples, but face constraints when we scale up. 

First, how well do generalizations from examining 5-10 predicates extend to the thousands in the lexicon? When we test 300 attitude predicates, do we find the neat categories theory predicts?

Second, what factors beyond pure semantic knowledge influence judgments? When people rate inferences, they're using world knowledge, contextual reasoning, and response strategies alongside semantic competence.

Third, how exactly does abstract semantic knowledge produce concrete behavioral responses? What cognitive processes intervene between computing a meaning and moving a slider on a scale?

These aren't failures of traditional methods - they're opportunities for extension. And that's where experimental semantics comes in.
:::

---

## Experimental semantics and pragmatics

Methodologies like inference judgment tasks study larger lexical areas

### Existing challenges:
- Unclear how modeling constructs relate to semantic theory
- What lexical properties are implicated?
- How do they interact with language-external factors?

::: {.fragment}
**The gradience question**: What theoretical constructs underlie distributions of judgments?
:::

::: {.notes}
Experimental semantics and pragmatics have made great strides in scaling up semantic investigation. Using inference judgment tasks and other methodologies, we can study entire lexical domains rather than handfuls of examples.

But challenges remain. Often it's unclear how the constructs in our statistical models relate to semantic theory. When we find that some predicates cluster together in their inference patterns, what lexical semantic properties are we actually discovering? And how do these properties interact with language-external factors like world knowledge?

The central challenge is the gradience question: when we get distributions of judgments rather than categorical patterns, what theoretical constructs underlie these distributions? Traditional semantic theory often assumes categorical distinctions, but the data shows continuity.

This is where PDS comes in - providing a framework where gradience can emerge from principled semantic sources while maintaining theoretical clarity about what those sources are.
:::

---

## The experimental turn

Experimental semantics brings behavioral experimentation to meaning questions

### Scaling semantic investigation
- Traditional: handful of predicates
- Experimental: entire lexical domains

::: {.notes}
The experimental turn in semantics represents a methodological revolution. We're bringing the tools of behavioral experimentation - controlled stimuli, statistical analysis, large participant samples - to questions about meaning.

The scale difference is dramatic. Where traditional methods might examine 5-10 predicates in detail, experimental approaches can investigate entire lexical domains. This isn't just more of the same - at scale, new patterns emerge that are invisible in small samples.

Let me give you a concrete example with the MegaAttitude project, which we'll look at next. This project studies hundreds of clause-embedding predicates - virtually the entire lexical class. At this scale, we can see gradience and clustering patterns that would be invisible looking at just "know," "believe," and "think."

This scaling reveals both the power of traditional generalizations and their limits. Some patterns hold beautifully; others reveal unexpected complexity.
:::

---

## MegaAttitude project example

Large-scale investigation of clause-embedding predicates:
::: {.smaller}
[@white_computational_2016; @white_role_2018; @white_lexicosyntactic_2018; @white_frequency_2020; @an_lexical_2020; @moon_source_2020; @kane_intensional_2022]
:::

- Hundreds of predicates
- Multiple contexts and inference types
- Reveals subtle patterns difficult to see traditionally

::: {.notes}
The MegaAttitude project exemplifies the power of experimental semantics at scale. This massive undertaking studies hundreds of clause-embedding predicates across multiple inference types and contexts.

The scale is unprecedented - instead of the usual suspects like "know" and "believe," we have data on predicates like "sense," "intimate," "let on," and hundreds more. Each predicate is tested in multiple syntactic frames, with different inference types, across many experimental participants.

What emerges is fascinating. We see continuous gradients where theory predicted discrete categories. We find predicates clustering in unexpected ways. "Admit" patterns differently than "confess" despite their apparent similarity. Some predicates show bimodal response distributions suggesting genuine ambiguity, while others show unimodal gradience.

This rich empirical landscape is exactly what PDS is designed to model. We can ask: what theoretical properties produce these patterns? How do discrete semantic features combine with continuous uncertainty to yield the distributions we observe?
:::

---

## Teasing apart contributing factors

Multiple factors influence inference judgments:

- **Semantic knowledge**: core meanings of expressions
- **World knowledge**: prior beliefs about plausibility
::: {.smaller}
[@degen_prior_2021 - systematic manipulation showing how beliefs modulate factive inferences]
:::
- **Contextual factors**: discourse context and QUD

::: {.notes}
One major advantage of experimental methods is the ability to tease apart different factors that influence inference judgments. When someone judges whether "Jo knows that Mo left" implies Mo left, multiple cognitive systems are at play.

First, semantic knowledge - the core meaning of "know" and how it composes with its complement. This is what traditional semantics focuses on.

Second, world knowledge. If Mo leaving is highly plausible given the context, people are more likely to infer it's true. Degen and Tonhauser showed this beautifully by manipulating prior probabilities while holding predicates constant. Even canonical factives like "know" show gradient behavior when world knowledge varies.

Third, contextual factors like the discourse context and question under discussion. What's at-issue can affect which inferences project.

[Continue to next slide for remaining factors]
:::

---

## Contributing factors (cont.)

- **Individual differences**: variation in interpretation
- **Response strategies**: how participants use rating scales

::: {.fragment}
These are windows into cognitive processes, not confounds!
:::

::: {.notes}
[Continuing from previous slide]

Fourth, individual differences. Different speakers may have slightly different lexical entries or interpretation strategies. What looks like gradience at the population level might reflect discrete differences between individuals.

Fifth, response strategies - how participants map their internal judgments onto the response scale. A 7 on a slider might mean different things to different people.

Crucially, these aren't confounds to be eliminated - they're windows into the cognitive processes underlying semantic interpretation. PDS provides a framework where each factor can be modeled explicitly and their interactions studied systematically.

This multiplicity of factors is why we need sophisticated frameworks. Simple models that ignore this complexity will miss crucial patterns in the data. But we also need principled ways to separate semantic from non-semantic factors, which is what PDS provides through its modular architecture.
:::

---

## Making linking hypotheses explicit

Experimental approaches force explicit theorizing about:
::: {.smaller}
[@jasbi_linking_2019; @waldon_modeling_2020; @phillips_theories_2021]
:::

- Link between semantic representations and behavioral responses
- What cognitive processes produce judgments?
- How do abstract representations map onto scale responses?

::: {.fragment}
**Key insight**: Different linking hypotheses → different predictions about response patterns

We can't ignore how representations map onto task responses
:::

::: {.notes}
One of the most important contributions of experimental semantics is forcing us to be explicit about linking hypotheses - how abstract semantic representations connect to concrete behavioral responses.

Traditional semantics can remain agnostic about this link. We say "know" triggers a presupposition, but how does that abstract property produce a slider response? What cognitive processes intervene?

Different linking hypotheses make different predictions. Maybe people directly report their certainty about inferences. Or maybe they perform pragmatic reasoning first. Maybe they're influenced by task demands or response biases.

The key insight is that we can't ignore this linking problem if we want to test theories against data. Even if our real interest is in semantic representations, we need theories of how those representations produce behavior.

PDS makes linking hypotheses explicit through response functions that map semantic computations to response distributions. As we'll see in the implementation, different response functions embody different assumptions about this crucial link.
:::

---

## Understanding gradience

**Striking finding**: Pervasive gradient judgments

- Even where theory assumes categorical distinctions
- Examples: gradable adjectives (expected) vs. factivity (puzzling)

::: {.fragment}
### Additional patterns observed:
::: {.smaller}
[@xue_correlation_2011; @smith_projection_2011; @djarv_prosodic_2017 - similar patterns]
:::

Understanding gradience crucial for connecting semantics to behavioral data
:::

::: {.notes}
The most striking finding from experimental semantics is the pervasiveness of gradient judgments. This isn't just in domains where we expect it, like gradable adjectives where "tall" has no sharp boundary. We find gradience even where traditional theory assumes categorical distinctions.

Factivity is the perfect example. Theory says predicates either trigger presuppositions or they don't - "know" is factive, "think" isn't. But experiments reveal continuous variation. Mean projection ratings vary smoothly from "pretend" at the bottom to "be annoyed" at the top, with no clear categorical break.

Multiple studies have found these gradient patterns. Whether using projection tasks, prosodic manipulations, or other paradigms, the gradience persists. This isn't measurement noise - it's a systematic phenomenon demanding theoretical explanation.

Understanding this gradience is crucial for connecting semantic theory to behavioral data. We need frameworks that can generate gradient predictions while maintaining theoretical clarity about what produces the gradience. That's a core goal of PDS.
:::

---

## A taxonomy of uncertainty

Two fundamental types producing gradience:

::: {style="font-size: 90%;"}
```
Sources of Gradience
├── Resolved (Type-Level) Uncertainty
│   ├── Ambiguity
│   │   ├── Lexical (e.g., "run" = locomote vs. manage)
│   │   ├── Syntactic (e.g., attachment ambiguities)
│   │   └── Semantic (e.g., scope ambiguities)
│   └── Discourse Status
│       └── QUD (Question Under Discussion)
└── Unresolved (Token-Level) Uncertainty
    ├── Vagueness (e.g., height threshold for "tall")
    ├── World knowledge (e.g., likelihood of facts)
    └── Task effects
        ├── Response strategies
        └── Response error
```
:::

::: {.notes}
To understand gradience, we need a taxonomy of uncertainty. I'll argue there are two fundamental types, and this distinction is crucial for building adequate models.

Resolved or type-level uncertainty involves discrete choices. There are multiple possible interpretations, and uncertainty about which one applies. This includes lexical ambiguity (run as locomotion vs. management), syntactic ambiguity (attachment sites), semantic ambiguity (scope), and discourse ambiguity (what's the QUD).

Unresolved or token-level uncertainty persists even after all ambiguities are resolved. This includes vagueness (where exactly is the threshold for "tall"?), world knowledge (how likely is it that Mo actually left?), and task effects (how do I use this slider scale?).

This distinction matters theoretically. Resolved uncertainty suggests discrete representations with probabilistic selection - mixture models. Unresolved uncertainty suggests gradient representations or continuous parameters.

Different phenomena may involve different types, and PDS can model both. The framework's power is in letting data adjudicate which type of uncertainty explains which patterns.
:::

---

## Resolved uncertainty: Details

Multiple discrete possibilities—speakers choose among interpretations

**Example**: "My uncle is running the race"
- *run* = locomotion or management?
- "How likely is it that my uncle has good managerial skills?"
  - Locomotion interpretation → ~0.2
  - Management interpretation → ~0.8
  - Population average → ~0.5 (mixture of discrete interpretations)

::: {.fragment}
Uncertainty "resolved" because once interpretation fixed, inference follows determinately
:::

::: {.notes}
Let's look at resolved uncertainty in detail. The key characteristic is that there are multiple discrete possibilities, and speakers must choose among them.

Take "My uncle is running the race." The verb "run" is ambiguous between locomotion and management senses. If asked about the uncle's managerial skills, people who interpret "run" as locomotion might respond around 0.2 - runners don't necessarily have management skills. Those interpreting it as management might respond around 0.8 - race organizers likely have such skills.

The population average might be 0.5, but this is a mixture of discrete interpretations, not genuine gradience. No individual thinks the uncle is simultaneously sort-of-running and sort-of-managing.

This uncertainty is "resolved" because once we fix the interpretation, the inference follows determinately. If running means locomotion, the managerial inference is weak. If it means managing, the inference is strong. The gradience emerges from averaging across resolutions, not from uncertainty within any single interpretation.

This pattern - bimodal or multimodal response distributions - is a signature of resolved uncertainty.
:::

---

## Unresolved uncertainty: Details

Persists even after fixing all ambiguities

**Example**: "My uncle is tall"
- No ambiguity about *tall*'s meaning
- Speakers uncertain about height threshold
- Classic vagueness—inherently gradient application
::: {.smaller}
[@fine_vagueness_1975; @graff_shifting_2000; @kennedy_vagueness_2007; @van_rooij_vagueness_2011; @sorensen_vagueness_2023]
:::

::: {.fragment}
World knowledge adds layers: even knowing someone runs races, uncertainty about speed, endurance, finishing likelihood remains
:::

::: {.notes}
Unresolved uncertainty contrasts sharply with resolved uncertainty - it persists even after we've fixed all ambiguities.

Consider "My uncle is tall." There's no ambiguity about what "tall" means - it's about height exceeding some threshold. But speakers remain uncertain about exactly where that threshold is. Is 5'11" tall? 6'0"? 6'1"? This is classic vagueness - the predicate's application conditions are inherently gradient.

The philosophical literature on vagueness is vast because it's genuinely puzzling. Unlike ambiguity, we can't just pick an interpretation and be done. The uncertainty is baked into the meaning itself.

World knowledge creates additional layers of unresolved uncertainty. Even knowing someone runs races (locomotion sense, ambiguity resolved), we remain uncertain about their speed, endurance, likelihood of finishing. These uncertainties appear within individual trials, not just across participants.

This type of uncertainty typically produces unimodal, continuous response distributions - very different from the multimodal patterns of resolved uncertainty.
:::

---

## Why this distinction matters

Type of uncertainty has profound implications:

- **Resolved uncertainty** → discrete representations with probabilistic selection
- **Unresolved uncertainty** → gradient representations or probabilistic reasoning within fixed meanings

::: {.fragment}
Different phenomena may involve different types:
- Vagueness: unresolved (inherently uncertain application)
- Factivity: puzzling—resolved ambiguity or unresolved projection?
:::

::: {.notes}
This distinction between resolved and unresolved uncertainty isn't just taxonomic - it has profound implications for semantic theory and how we model behavioral data.

If gradience comes from resolved uncertainty, we need discrete semantic representations with probabilistic selection mechanisms. Think mixture models where people probabilistically choose among interpretations. The semantics stays discrete; the probability is in the selection.

If gradience comes from unresolved uncertainty, we need gradient representations or probabilistic reasoning within fixed meanings. The uncertainty is in the semantics itself, not just in choosing between options.

Different phenomena may involve different types. Vagueness seems clearly to involve unresolved uncertainty - there's no discrete set of "tall" meanings to choose from. But factivity is puzzling. Is the gradience from resolved uncertainty (ambiguous predicates, variable discourse conditions) or unresolved uncertainty (gradient projection mechanisms)?

PDS lets us implement both possibilities and test them against data. As we'll see in our case studies, the empirical patterns can distinguish these theoretical alternatives.
:::

---

## Case studies: Testing theory at scale

Two case studies exemplifying different framework aspects:

### Case Study 1: Vagueness and gradable adjectives
- Ideal starting point—everyone agrees on gradient uncertainty
- *tall*, *expensive*, *old* lack sharp boundaries

### Case Study 2: Factivity and projection
- Traditional theory treats as discrete
- Experimental data reveals pervasive gradience
- A theoretical puzzle

::: {.notes}
We'll explore two case studies that exemplify different aspects of the PDS framework and show how it bridges theory and data.

Our first case study examines vagueness in gradable adjectives. This is an ideal starting point because everyone agrees these involve gradient uncertainty. Words like "tall," "expensive," and "old" lack sharp boundaries - there's no precise height where someone becomes tall. This makes them perfect for demonstrating how PDS incorporates gradience into compositional semantics.

Our second case study tackles factivity and projection - a much more controversial domain. Traditional theory treats factivity as discrete: predicates either trigger presuppositions or they don't. But experimental data reveals pervasive gradience that's difficult to explain. This presents a theoretical puzzle that PDS can help resolve.

These case studies aren't just examples - they represent two poles of semantic phenomena. Adjectives show uncontroversial gradience from vagueness. Factivity shows surprising gradience where theory expected discreteness. Together, they demonstrate PDS's range and power.
:::

---

## Case Study 1: Vagueness - Background

Vague predicates lack sharp boundaries:
::: {.smaller}
[@lakoff_hedges_1973; @sadock_truth_1977; @lasersohn_pragmatic_1999; @krifka_approximate_2007; @solt_vagueness_2015]
:::

Everyone agrees they involve gradient uncertainty

Makes vagueness ideal for demonstrating PDS framework

::: {.notes}
Vague predicates have been recognized as problematic for classical semantics since antiquity. If "tall" means exceeding some height, what height? There seems to be no principled answer.

The modern literature on vagueness is vast and sophisticated. Linguists and philosophers have developed numerous approaches - supervaluation, fuzzy logic, epistemic theories, degree-based theories. What they share is recognizing that vagueness involves gradient uncertainty.

This consensus makes vagueness ideal for demonstrating the PDS framework. We're not trying to convince anyone that adjectives involve gradience - that's accepted. Instead, we can focus on how PDS incorporates this gradience into compositional semantics while maintaining formal rigor.

We'll see how degree-based theories of adjectives - already probabilistic in spirit - map naturally into PDS. The framework adds explicit probability distributions where traditional theories left things implicit.
:::

---

## Case Study 1: Vagueness - Formal approaches

Degree-based theories long recognized gradience:
::: {.smaller}
[@klein_semantics_1980; @bierwisch_semantics_1989; @kamp_two_1975; @kennedy_projecting_1999; @kennedy_scale_2005; @kennedy_vagueness_2007; @barker_dynamics_2002]
:::

**Analysis**: *tall* is true of $x$ if height($x$) $≥ d_{\text{tall}}$(context)
- Threshold varies with context
- Gradient judgments even within fixed context

---

## Case Study 1: Vagueness - PDS approach

PDS can:
- Maintain compositional degree-based analysis
- Add probability distributions over thresholds
- Model context shifts
- Link distributions to slider responses

Makes vagueness ideal for demonstrating framework

---

## Case Study 1: Vagueness - Experimental findings

Recent work reveals additional complexity:

- **Relative adjectives** (*tall*, *wide*): maximum gradience
- **Absolute adjectives** (*clean*, *dry*): different threshold distributions
- **Minimum vs. maximum standard**: asymmetric patterns

::: {.notes}
Recent experimental work on adjectives reveals complexity beyond what traditional theories predicted. Different types of adjectives show distinct patterns of gradience.

Relative adjectives like "tall" and "wide" show maximum gradience in their positive form. There's no inherent standard - tallness is always relative to a comparison class. This produces smooth, continuous judgment curves.

Absolute adjectives like "clean" and "dry" work differently. They have endpoint-oriented standards - "clean" means no dirt, "dry" means no moisture. But experimentally, they still show gradience, just with different threshold distributions clustered near the endpoints.

There are also asymmetries between minimum and maximum standard adjectives. "Dirty" (minimum standard - any dirt suffices) patterns differently from "clean" (maximum standard - no dirt allowed). These patterns both support and refine formal theories.

[Continue to next slide for computational integration]
:::

---

## Case Study 1: Integration with models

### Integration with computational models:
::: {.smaller}
[@lassiter_context_2013; @qing_gradable_2014; @kao_nonliteral_2014; @lassiter_adjectival_2017; @bumford_rationalizing_2021]
:::

PDS synthesizes and compares these approaches

Both support and refine formal theories

::: {.notes}
Recent years have seen partial integration of adjective theories into computational models. These models make quantitative predictions about judgment patterns, but often in framework-specific ways.

Lassiter and colleagues model threshold uncertainty using Bayesian pragmatics. Qing and Franke use similar ideas for gradable adjectives. Kao et al. extend this to non-literal uses. Each approach captures important insights but uses different formal frameworks.

PDS provides a unifying framework where these different approaches can be compared. By implementing different theories within PDS, we can test their predictions against the same data using the same evaluation metrics.

This synthesis is powerful. We can ask: Does threshold uncertainty alone explain the patterns, or do we need pragmatic reasoning too? How do different linking hypotheses affect model fit? PDS enables principled theory comparison while maintaining compositionality.

The experimental patterns both support formal theories (yes, there are thresholds) and refine them (the threshold distributions are more complex than expected).
:::

---

## Case Study 2: Factivity - Traditional view

Traditional theory: predicates either trigger presuppositions or don't
::: {.smaller}
[@kiparsky_fact_1970; @karttunen_observations_1971]
:::

[]{#exm-love-positive}
[]{#exm-love-negative}
[]{#exm-love-question}
**Example**: *love* appears factive
- (@exm-love-positive) Jo loves that Mo left. ⟹ Mo left.
- (@exm-love-negative) Jo doesn't love that Mo left. ⟹ Mo left.
- (@exm-love-question) Does Jo love that Mo left? ⟹ Mo left.

::: {.notes}
Now let's turn to factivity - a phenomenon that presents a real puzzle for connecting theory to data.

Traditional theory treats factivity as a discrete property. Predicates either trigger presuppositions about their complements or they don't. "Know" is factive - it presupposes its complement is true. "Think" is non-factive - it doesn't.

The classic diagnostic uses the "family of sentences" test. A predicate is factive if the inference about its complement survives under negation and questioning. "Love" appears factive because all three sentences suggest Mo left - the positive assertion, the negation, and the question.

This categorical view made sense given traditional methodology. Looking at clear cases like "know" vs. "think," the distinction seems sharp. But what happens when we test hundreds of predicates with hundreds of participants?

Spoiler: we find pervasive gradience that's difficult to explain under the discrete view.
:::

---

## Case Study 2: Factivity - Definition

A predicate is *factive* if it triggers inferences about its complement that project through entailment-canceling operators

Inference about complement truth survives:
- Negation
- Questions
- Other embeddings

::: {.notes}
Let's be precise about what factivity means. A predicate is factive if it triggers inferences about its complement that project through entailment-canceling operators.

What are entailment-canceling operators? Negation is the classic example - "Jo laughed" entails Jo exists, but "Jo didn't laugh" doesn't. Questions are another - "Did Jo laugh?" doesn't entail Jo laughed. 

But with factive predicates, complement inferences survive these operators. "Jo knows that Mo left" suggests Mo left. So does "Jo doesn't know that Mo left" and "Does Jo know that Mo left?" The inference projects through the operators.

This projection behavior is what makes factivity special and theoretically interesting. It seems to mark a natural class of predicates with shared semantic properties. But as we'll see, the empirical reality is more complex than this neat categorical picture suggests.
:::

---

## Case Study 2: Factivity - The puzzle

@white_role_2018 and @degen_are_2022 found continuous variation:
- No clear line between factive and non-factive predicates
- Mean projection ratings vary continuously
- *pretend* (lowest) to *be annoyed* (highest)

::: {style="text-align: center;"}
![](images/veridicality_factivity.pdf){width=400}
:::

::: {.smaller}
[Aggregate factivity measures from MegaVeridicality dataset]
:::

::: {.notes}
Here's where things get interesting. When White and colleagues tested hundreds of attitude predicates, they found continuous variation in projection strength - not the discrete categories theory predicted.

Look at this plot. Each point is a predicate's mean projection rating. If factivity were discrete, we'd expect clustering - a factive group with high projection, a non-factive group with low projection. Instead, we see a smooth gradient from "pretend" at the bottom to "be annoyed" at the top.

Where would you draw the line between factive and non-factive? At 0.5? Why not 0.6 or 0.4? The data doesn't suggest any natural boundary.

This is a serious puzzle. Either our theoretical understanding is wrong - maybe factivity isn't discrete - or something about the relationship between discrete semantic properties and gradient behavioral responses needs explaining. PDS lets us explore both possibilities formally.
:::

---

## Factivity: Fundamental Discreteness

**Hypothesis 1**: Factivity is discrete; gradience arises from:

- Multiple predicate senses (factive and non-factive variants)
- Structural ambiguity affecting projection
- Contextual variation in whether complements are at-issue

::: {.notes}
The Fundamental Discreteness hypothesis maintains that factivity is discrete, but gradience emerges from other sources. This preserves traditional theoretical insights while explaining the puzzling data.

First, predicates might have multiple senses - factive and non-factive variants. "Realize" might have a factive sense (coming to know) and a non-factive sense (understanding). Population-level gradience would reflect mixture across senses.

Second, structural ambiguity might affect projection. Different syntactic analyses of the same string might have different projection properties. The complement might be parsed as truly embedded under the attitude verb, or as a parenthetical, affecting whether presuppositions project.

Third, contextual factors like at-issueness might vary. When complements address the question under discussion, their content might not project as strongly. This would create gradience even with discrete underlying mechanisms.

Under this view, factivity remains a discrete semantic property, but various factors create gradient behavioral patterns.
:::

---

## Discreteness hypothesis: Evidence

Supporting literature:
::: {.smaller}
Structural ambiguity: @varlokosta_issues_1994; @giannakidou_polarity_1998; @giannakidou_affective_1999; @giannakidou_dependency_2009; @roussou_selecting_2010; @farudi_antisymmetric_2007; @abrusan_predicting_2011; @kastner_factivity_2015; @ozyildiz_attitude_2017

Contextual variation: @simons_best_2017; @roberts_preconditions_2024; @qing_rational_2016
:::

::: {.notes}
There's substantial theoretical and empirical support for aspects of the discreteness hypothesis.

The structural ambiguity approach has been developed by many researchers. They argue that different syntactic structures for attitude reports have different semantic properties. Factive complements might involve a different syntactic position than non-factive ones, or selection of different complement types (DPs vs. CPs).

The contextual variation approach, developed by Simons and colleagues, argues that projection depends on discourse structure. When complement content addresses the QUD, it's at-issue and doesn't project. When it's backgrounded, it projects. This predicts variation based on context rather than lexical properties alone.

These aren't mutually exclusive - both structural and contextual factors might contribute to the observed gradience. The key insight is that discrete underlying mechanisms can produce gradient surface patterns through various routes.
:::

---

## Factivity: Fundamental Gradience

**Hypothesis 2**: No discrete property exists

- Gradient degrees of complement truth support
- Continuous variation reflects semantic reality
::: {.smaller}
[@tonhauser_how_2018]
:::

::: {.notes}
The Fundamental Gradience hypothesis takes a more radical position: there's no discrete factivity property at all. Instead, predicates provide gradient degrees of support for their complement's truth.

Under this view, the continuous variation we observe in experiments directly reflects semantic reality. "Know" strongly supports its complement being true, "think" weakly supports it, and other predicates fall at various points between. There's no categorical distinction, just a continuum.

This aligns with Tonhauser and colleagues' Gradient Projection Principle - that projection strength varies continuously based on multiple factors including lexical semantics, world knowledge, and discourse structure.

This hypothesis is theoretically simpler in some ways - no need to explain away the gradience. But it's more radical, abandoning a distinction that seemed well-motivated by traditional data. It also raises questions about how gradient projection would work compositionally.

PDS lets us implement both hypotheses formally and test their predictions.
:::

---

## Factivity: Testing both hypotheses

PDS enables testing both hypotheses against fine-grained response distributions

Not just means, but entire patterns including multimodality

::: {.notes}
The power of PDS is that we can implement both hypotheses formally and test their predictions against fine-grained data patterns - not just average ratings but entire response distributions.

If factivity is discrete with gradience from mixture, we'd expect multimodal response distributions. Some participants interpret predicates as factive (high ratings), others as non-factive (low ratings), producing bimodality.

If factivity is fundamentally gradient, we'd expect unimodal distributions centered at different points for different predicates. The variation would be smooth and continuous.

PDS can generate predictions for both patterns. We can implement discrete factivity with probabilistic selection between variants, or gradient factivity with continuous parameters. The data can then adjudicate between theories.

As we'll see in the implementation section, the discrete hypothesis actually fits the data better - suggesting that despite surface gradience, factivity involves categorical distinctions. This vindicates the traditional view while explaining the puzzling experimental patterns.
:::

---

## Factivity: Testing paradigms

Various experimental paradigms:
::: {.smaller}
- Prosodic manipulations: @tonhauser_prosodic_2016; @djarv_prosodic_2017; @jeong_prosodically-conditioned_2021
- Prior beliefs: @degen_prior_2021
- Large-scale judgments: @white_role_2018; @white_lexicosyntactic_2018; @degen_are_2022; @kane_intensional_2022
:::

::: {.notes}
The factivity puzzle has motivated diverse experimental paradigms, each revealing different aspects of the phenomenon.

Prosodic manipulation studies show that focus and accent placement affect projection strength. This supports the view that information structure matters - focused content is less likely to project.

Large-scale judgment studies like MegaVeridicality test hundreds of predicates systematically. These reveal the full gradient landscape rather than just endpoints.

Prior belief manipulations tease apart semantic from world knowledge contributions. Even canonical factives show reduced projection when complements are implausible.

Each paradigm provides a different window into the phenomenon. PDS models can incorporate insights from all of them - prosodic effects through discourse parameters, cognitive effects through response noise, prior beliefs through world knowledge distributions.
:::

---

## Theoretical challenges from gradience

The gradience poses challenges:
::: {.smaller}
[@simons_observations_2007; @simons_what_2010; @simons_best_2017; @tonhauser_how_2018]
:::

::: {.fragment}
### Key questions:
- Is the gradience fundamental or derived?
- What mechanisms produce continuous variation?
- How do semantic and pragmatic factors interact?

PDS provides tools to address these formally
:::

::: {.notes}
The pervasive gradience in projection poses serious theoretical challenges that have motivated substantial recent work.

The fundamental question is whether gradience is a core property of projection or derived from other sources. If fundamental, we need gradient semantic representations - a major revision to standard theories. If derived, we need to identify the sources and show how they produce the observed patterns.

We also need mechanisms that produce continuous variation. Discrete categories usually produce discrete behaviors. How do we get smooth gradients? Through probabilistic selection? Continuous parameters? Multiple interacting factors?

Finally, how do semantic and pragmatic factors interact? Is projection computed by semantic mechanisms then modulated by pragmatics? Or is it pragmatic from the start?

PDS provides tools to address these questions formally. We can implement different theoretical positions, derive their predictions, and test them against data. This moves debates from intuition to empirical testing.
:::

---

## The need for new frameworks

Case studies illustrate four key requirements:

**1. Maintain Compositionality**
- Derive meanings compositionally
- Preserve formal semantic insights
- Can't abandon compositionality for gradience

**2. Model Uncertainty Explicitly**
- Represent resolved and unresolved uncertainty
- Show interaction during interpretation

::: {.notes}
Our case studies illustrate what we need from a framework connecting formal semantics to experimental data.

First, we must maintain compositionality. The meanings of complex expressions must be derived systematically from their parts. We can't abandon this core principle just because judgments are gradient. Decades of semantic research have shown the power of compositional analysis - we need to preserve these insights.

Second, we need to model uncertainty explicitly. The framework must represent both types we've discussed - resolved uncertainty from discrete choices and unresolved uncertainty from inherent gradience. Crucially, it must show how these interact during interpretation.

[Continue to next slide for requirements 3 and 4]
:::

---

## Framework requirements (cont.)

**3. Make Linking Hypotheses Precise**
- Explicit theories: representation → behavior
- What processes between meaning and slider?

**4. Enable Quantitative Evaluation**
- Testable predictions about distributions
- Compare theories using standard metrics

::: {.notes}
[Continuing from previous slide]

Third, we must make linking hypotheses precise. We need explicit theories of how semantic representations produce behavioral responses. What cognitive processes intervene between computing a meaning and moving a slider? Different assumptions lead to different predictions.

Fourth, the framework must enable quantitative evaluation. Theories should make testable predictions about response distributions, not just qualitative patterns. Different theories must be comparable using standard statistical metrics so we can determine which best explains the data.

These requirements are demanding. We need a framework that's both theoretically principled and empirically adequate. Existing approaches often excel at one or the other. PDS aims to satisfy all four requirements simultaneously.
:::

---

## Moving forward

Existing computational approaches (e.g., RSA) bridge formal semantics with probabilistic reasoning
::: {.smaller}
[@frank_predicting_2012; @goodman_knowledge_2013]
:::

::: {.fragment}
### Challenges with existing approaches:
- Difficulty maintaining modularity
- Often blur semantics/pragmatics distinction
- Connection to traditional theory somewhat opaque
:::

::: {.notes}
Before introducing PDS in detail, let's acknowledge existing computational approaches, particularly Rational Speech Act (RSA) models. These have made important strides in bridging formal semantics with probabilistic reasoning.

RSA models formalize Gricean reasoning - speakers choose utterances to convey information, listeners interpret utterances by reasoning about speaker intentions. This provides a principled way to derive pragmatic inferences from literal meanings.

But RSA faces challenges when scaling to the phenomena we've discussed. It's difficult to maintain modularity - semantic and pragmatic components become intertwined. The distinction between what's semantic and what's pragmatic often blurs.

Most importantly, the connection to traditional compositional semantics is somewhat opaque. RSA typically operates on complete utterance meanings rather than building them compositionally. This makes it hard to leverage existing semantic insights.

[Continue to next slide for PDS motivation]
:::

---

## Motivating PDS

This motivates Probabilistic Dynamic Semantics:
- Preserves semantic insights
- Adds probabilistic tools for gradient data
- Maintains theoretical commitments while enabling tests

::: {.fragment}
PDS provides the framework we need to bridge theory and data
:::

::: {.notes}
This brings us to Probabilistic Dynamic Semantics. PDS is designed to address the challenges we've identified while building on the successes of both traditional and computational approaches.

PDS preserves the insights of formal semantics - compositionality, precise truth conditions, systematic meaning derivation. But it adds probabilistic tools needed to model gradient behavioral data. Uncertainty is built into the semantic representations themselves, not added post-hoc.

Crucially, PDS maintains theoretical commitments while enabling quantitative tests. You can implement your favorite semantic theory within PDS and test its predictions. Different theories become comparable through their empirical adequacy.

The framework provides exactly what we need - a principled bridge from compositional semantic theory to gradient behavioral data. It's not a replacement for traditional semantics but an extension that enables new types of investigation.

In the next session, we'll see how PDS works in detail, starting with its relationship to RSA models.
:::

---

## Interim summary

- Traditional semantics achieved remarkable success
- Experimental methods open new opportunities
- Gradience poses theoretical challenges
- Need frameworks bridging theory and data

::: {.fragment}
**Next**: How Rational Speech Act models attempt this bridge, and what PDS can add
:::

::: {.notes}
Let's summarize where we are. Traditional semantics has achieved remarkable success in characterizing compositionality and inference patterns. These insights are too valuable to abandon.

But experimental methods open new opportunities. We can test theories at unprecedented scale, investigate entire lexical domains, and discover patterns invisible to traditional methods. The data is rich but challenging.

The pervasive gradience in this data poses theoretical challenges. We need to understand what produces gradience - discrete choices, continuous parameters, or both? And we need frameworks that can model these sources while maintaining theoretical clarity.

This motivates frameworks that bridge theory and data. We need approaches that preserve semantic insights while engaging with empirical complexity. They must be both principled and practical.

Next, we'll examine how Rational Speech Act models attempt this bridge. We'll see their successes and limitations, which will set the stage for understanding how PDS provides a complementary approach that maintains compositionality while adding probabilistic structure.

Thank you! Questions before we move to RSA?
:::