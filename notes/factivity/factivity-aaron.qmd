---
title: "Factivity"
---

Having explored how PDS handles expected gradience in adjectives, we now turn to factivity—where gradience poses a deeper theoretical puzzle. While the gradience in adjective meanings arises from well-understood sources like vagueness and contextual variation, the gradience observed in judgments aimed at measuring is more difficult to explain.

## Factivity and projection

Before diving into computational models, let's review what makes factivity special. Factivity describes the property of certain predicates that are associated with the presupposition that their complements are true, even when the predicate is embedded under various operators. Compare these sentences:

[]{@factive-projection-pos}
[]{@factive-projection-neg}
[]{@factive-projection-q}
[]{@factive-projection-cond}

(@factive-projection-pos) Jo loved that Mo left.
(@factive-projection-neg) Jo didn't love that Mo left.
(@factive-projection-q) Did Jo love that Mo left?
(@factive-projection-cond) If Jo loved that Mo left, she'll won't be upset that Bo left.

In all cases, there's a strong inference that Mo actually left. We say that this inference *projects* through negation, questions, and conditionals. Contrast this with non-factive predicates:

[]{@non-factive-pos}
[]{@non-factive-neg}

(@non-factive-pos) Jo {thinks, believes, said} that Mo left.
(@non-factive-neg) Jo doesn't {think, believe, say} that Mo left.

Here, using (@non-factive-neg) doesn't commit us to Mo having left. Given just these examples, the contrast feels sharp: predicates are either associated with factive presuppositions or not.

But this traditional picture has been challenged by experimental work showing substantial gradience in projection judgments. Some predicates (like *love*) are almost always associated with projection, others (like *think*) rarely trigger it, but many fall somewhere in between. For instance, @white_role_2018 measured veridicality inferences for more than 700 predicates in their MegaVeridicality dataset–visualized in @fig:veridicality-factivity.

![An aggregate measure of factivity, derived from the MegaVeridicality dataset of @white_role_2018. Points are jittered (0.05) to avoid overplotting. The x-axis corresponds to mean scores for affirmative contexts, the y-axis for negative contexts. Each point is a predicate. Note the continuous gradient rather than discrete clusters.](plots/veridicality_factivity.pdf){#fig:veridicality-factivity}

What explains this gradience?

## An importance distinction

There are two importantly distinct questions that we need to disentangle before moving on:

1. Whether the gradience in @fig:veridicality-factivity suggests that there is no *class* (or subclasses) of factive predicates.
2. Whether the veridicality inferences triggered by a particular expression containing a particular clause-embedding predicate are themselves gradient (whatever that would mean).

We're going to be concerned with the latter question (i) because it is something PDS, we believe, is uniquely well positioned to ask, and thus asking it provides a useful demonstration of the PDS's use; and (ii) because the former question has already been answered.

[As discussed on Day 1](../background/case-studies.qmd), @degen_are_2022 argue that there is no clear line separates factive from non-factive predicates. Mean projection ratings vary continuously from *pretend* (lowest) to *be annoyed* (highest).

![Aggregate factivity measures from @degen_are_2022, showing continuous variation in projection ratings across predicates under questioning.](plots/projection_no_fact_means.pdf){width=750}

@kane_intensional_2022 later showed that this gradience is likely due largely to task effects and measurement noise. They demonstrate that when one applies a clustering model to these data that accounts for noise due to various factors, many of the standard subclasses of factives pop out. 

To get a sense for how these clusters appear in the data, we can look back at @fig-derived-factivity, which [we discussed on Day 1](../background/understanding-gradience.qmd).

![One way of deriving a factivity measure from the MegaVeridicality dataset.](plots/derived_factivity_measure.png){#fig-derived-factivity width=750}

Remember that the idea behind this figure is that we could in principle derive a continuous measure of factivity by taking the minimum along the axes of @fig:veridicality-factivity and rectifying.

The thing to note is that (i) there are clearly at least two separate "bumps" in the histogram; but (ii) the peak of the right bump is not at 1, as we might have expected. This is because some of these subclasses–e.g. the cognitive factives, which @karttunen_observations_1971 observes to not always give rise factivity–appear to themselves be associated with non-necessary factive inferences, while other classes–e.g. the emtoive factives, such as *love*, *hate*, and *be annoyed*–always do (up to measurement error and other sources of noise). 

What explains the gradience *internal* to a class like the cognitive factives (which, again, we have known for more than 60 years *should* show this pattern)?

## Two competing hypotheses

Using PDS, we can precisely state two fundamentally different accounts of the observed gradience:

**Discrete-factivity hypothesis**: Factivity is a discrete property—speakers either interpret a predicate as triggering a factive presupposition on a given occasion of use or they don't. The observed gradience reflects uncertainty about which interpretation is intended, analogous to lexical ambiguity. Different uses of *discover* might involve different lexical entries or different syntactic structures, only some of which are associated with presupposition.

**Wholly-gradient hypothesis**: There is no discrete factivity property. Instead, predicates make gradient contributions to inferences about their complements' truth. This view, aligned with @tonhauser_how_2018's Gradient Projection Principle, treats projection as a continuous phenomenon where content projects "to the extent that it is not at-issue."

As we'll see, PDS allows us to implement both hypotheses as statistical models and compare their empirical predictions. But first, we need to understand the experimental paradigm that gives us the data to test these hypotheses.

## Experimental paradigms

### Separating world knowledge from factivity

A key challenge in studying factivity is that our judgments about whether someone is certain of a proposition depend on two factors:

1. The properties of the predicate (whether speakers interpret it as being associated with a presupposition)
2. Our world knowledge about how likely the proposition is

@degen_prior_2021 developed a two-stage experimental design to tease these apart:

**Stage 1: Norming study (Experiment 2a)**

First, they measured how world knowledge affects belief in various propositions. Participants saw items like:

:::{.callout-note title="Norming item example"}
**Fact**: Sophia is a hipster.  
**Question**: How likely is it that Sophia got a tattoo?  
**Response**: [slider from 0 to 1]
:::

For each proposition (e.g., *Sophia got a tattoo*), they created two contexts–one making it more likely (Sophia is a hipster) and one making it less likely (*Sophia is a Mormon*). This gives us a measure of world knowledge independent of any embedding predicate.

**Stage 2: Projection study (Experiment 2b)**

Next, they embedded these same propositions under various predicates and asked about the speaker's certainty:

:::{.callout-note title="Projection item example"}
**Context**: Isabella said that Sophia is a hipster.  
**Utterance**: Noah knows that Sophia got a tattoo.  
**Question**: Is Noah certain that Sophia got a tattoo?  
**Response**: [slider from 0 to 1]
:::

They reasoned that, if speakers categorically interpret *know* as triggering a factive presupposition, Noah should be certain regardless of the prior probability; but, they reasoned, if projection is gradient, his certainty should vary with world knowledge.

### Key experimental details

The experiments tested 20 predicates that theory suggests might pattern differently:

- **Canonical factives:** *be annoyed*, *discover*, *know*, *reveal*, *see*
- **Non-factives:**
  - **Non-veridical non-factive:** *pretend*, *say*, *suggest*, *think* 
  - **Veridical non-factive:** *be right*, *demonstrate*
- **Optionally factive:** *acknowledge*, *admit*, *announce*, *confess*, *confirm*, *establish*, *hear*, *inform*, *prove* 

Each participant saw 20 items (one per predicate) plus controls, with predicates paired with different complement clauses and prior contexts across participants. @fig:projection-verb-means shows the resulting distributions.

![Verb means from @degen_prior_2021's experiment 2b. Non-factive predicates are in red, "optionally factive" verbs are in teal, and "canonically factive" verbs are in green. Violin plots indicate the probability density of responses, showing the gradient nature of the empirical data.](plots/projection_no_fact_means.pdf){#fig:projection-verb-means}

## World knowledge: gradient or discrete?

Before examining factivity proper, let's understand how the norming study helps us model world knowledge. The norming data allows us to test whether world knowledge itself involves discrete or gradient uncertainty. (The structure of this model is very similar to the one we covered in the [adjectives section](../adjectives/adjectives-aaron.html#world-knowledge-gradient-or-discrete).)

![Left: ELPDs for the two models of the norming data from @degen_prior_2021's experiment 2a. Dotted line indicates estimated difference between the norming-discrete model and the norming-gradient model. Right: posterior predictive distributions for both models for the item *Sophia got a tattoo*, given either the fact *Sophia is a hipster* (5H) or *Sophia is a fashion model* (5L). Error bars indicate standard errors.](plots/norming_elpd.pdf){#fig:norming-elpds-pps}

As @fig:norming-elpds-pps shows, the gradient model of world knowledge substantially outperforms the discrete model (ΔELPD = 442.3 ± 23.1). The posterior predictive distributions reveal why: participants' judgments cluster away from the scale endpoints, a pattern the gradient model captures naturally but the discrete model cannot. This establishes that world knowledge contributes gradient uncertainty to factivity judgments.

## From PDS to Stan: implementing the theories

Building on [the compilation pipeline introduced for adjectives](../adjectives/norming-model.qmd), let's see how PDS handles the competing theories of factivity. Recall that PDS outputs **kernel models**—the semantic core corresponding directly to our theoretical commitments.
We'll focus mainly on the actual experimental items here, since the norming models for this task have a very similar character to the ones we introduced for gradable adjectives.

### Discrete-factivity in PDS

For the discrete-factivity hypothesis, we can derive projection judgments using:

```haskell
-- From Grammar.Parser and Grammar.Lexica.SynSem.Factivity
-- Define the discourse: "Jo knows that Bo is a linguist"
expr1 = ["jo", "knows", "that", "bo", "is", "a", "linguist"]
expr2 = ["how", "likely", "that", "bo", "is", "a", "linguist"]
s1 = getSemantics @Factivity 0 expr1
q1 = getSemantics @Factivity 0 expr2
discourse = ty tau $ assert s1 >>> ask q1

-- Compile to Stan using factivityPrior and factivityRespond
factivityExample = asTyped tau (betaDeltaNormal deltaRules . factivityRespond factivityPrior) discourse
```

This compilation process involves several key components. First, the lexical entry for *know* branches based on discourse state:

```haskell
-- From Grammar.Lexica.SynSem.Factivity
"knows" -> [ SynSem {
    syn = S :\: NP :/: S,
    sem = ty tau (lam s (purePP (lam p (lam x (lam i 
      (ITE (TauKnow s) 
           (And (epi i @@ x @@ p) (p @@ i))  -- factive: belief AND truth
           (epi i @@ x @@ p))))))            -- non-factive: belief only
      @@ s)
} ]
```

The `ITE` (if-then-else) creates a discrete choice: either the speaker interprets the predicate as requiring the complement to be true (factive interpretation) or only the belief component is required (non-factive interpretation). The `TauKnow` parameter, which can vary by context, determines which branch is taken.

To accommodate this contextual variation, the prior over states must be updated:

```haskell
-- From Grammar.Lexica.SynSem.Factivity  
factivityPrior = let' x (LogitNormal 0 1) (let' y (LogitNormal 0 1) (let' z (LogitNormal 0 1) (let' b (Bern x) (Return (UpdCG (let' c (Bern y) (let' d (Bern z) (Return (UpdLing (lam x c) (UpdEpi (lam x (lam p d)) _0))))) (UpdTauKnow b ϵ)))))
```

The delta rules must also be modified to handle the contextual factivity parameter:

```haskell
-- From Lambda.Delta: Computes functions on states
states :: DeltaRule
states = \case
  TauKnow (UpdTauKnow b _) -> Just b
  TauKnow (UpdCG _ s)      -> Just (TauKnow s)
  TauKnow (UpdQUD _ s)     -> Just (TauKnow s)
  -- ... other cases
```

PDS compiles this discrete-factivity theory to the following kernel model:^[Actual PDS output: `model { v ~ logit_normal(0.0, 1.0); w ~ logit_normal(0.0, 1.0); target += log_mix(v, truncated_normal_lpdf(y | 1.0, sigma, 0.0, 1.0), truncated_normal_lpdf(y | w, sigma, 0.0, 1.0)); }`]

```stan
model {
  // FIXED EFFECTS
  v ~ logit_normal(0.0, 1.0);  // probability of factive interpretation
  w ~ logit_normal(0.0, 1.0);  // world knowledge (from norming)
  
  // LIKELIHOOD
  target += log_mix(v, 
                    truncated_normal_lpdf(y | 1.0, sigma, 0.0, 1.0),     // factive branch
                    truncated_normal_lpdf(y | w, sigma, 0.0, 1.0));      // non-factive branch
}
```

This kernel captures the discrete branching: with probability `v`, the response is near 1.0 (factive interpretation); otherwise, it depends on world knowledge `w` (non-factive interpretation).

### Understanding the Stan implementation

Before augmenting this kernel to handle real data, let's briefly review the key additions for factivity.^[For a detailed introduction to Stan's blocks and syntax, see [the Stan introduction in the adjectives section](../adjectives/norming-model.qmd).]

The factivity-specific components are:
- Hierarchical structure for verb-specific and context-specific effects
- Integration with norming study priors via `mu_omega` and `sigma_omega`
- Mixture likelihood for discrete-factivity (highlighted lines 25-27 in the full model below)

Here's how the discrete-factivity kernel is augmented for real data:

```{.stan .line-numbers highlight="16-27"}
model {
  // PRIORS (analyst-added)
  verb_intercept_std ~ exponential(1);
  context_intercept_std ~ exponential(1);
  subj_verb_std ~ exponential(1);
  subj_context_std ~ exponential(1);
  sigma_e ~ beta(2, 10);  // mildly informative prior keeping sigma_e small
  
  // Hierarchical priors
  verb_logit_raw ~ std_normal();
  context_logit_raw ~ normal(mu_omega, sigma_omega);  // informed by norming
  to_vector(subj_verb_raw) ~ std_normal();
  to_vector(subj_context_raw) ~ std_normal();
  
  // DISCRETE FACTIVITY (PDS kernel structure)
  for (n in 1:N) {
    // Probability of factive interpretation for this verb/subject combo
    real verb_prob = inv_logit(verb_intercept[verb[n]] +
                               subj_intercept_verb[subj[n], verb[n]]);
    
    // World knowledge probability for this context/subject combo  
    real context_prob = inv_logit(context_intercept[context[n]] +
                                  subj_intercept_context[subj[n], context[n]]);
    
    // MIXTURE LIKELIHOOD (PDS kernel structure)
    target += log_mix(verb_prob,
                      truncated_normal_lpdf(y[n] | 1.0, sigma_e, 0.0, 1.0),
                      truncated_normal_lpdf(y[n] | context_prob, sigma_e, 0.0, 1.0));
  }
}
```

The highlighted lines represent the kernel model from PDS—encoding discrete factivity as a mixture of two response distributions. The unhighlighted portions add the statistical machinery needed for real data.

### Wholly-gradient factivity

The alternative wholly-gradient hypothesis treats factivity as continuously variable. The key modification to the PDS code is in how we encode the gradience. Instead of discrete branching, the gradient model computes a weighted combination.

PDS outputs this kernel for the wholly-gradient model:^[Actual PDS output after adding rendering hooks: `model { v ~ logit_normal(0.0, 1.0); w ~ logit_normal(0.0, 1.0); target += truncated_normal_lpdf(y | v + (1.0 - v) * w, sigma, 0.0, 1.0); }`]

```stan
model {
  // FIXED EFFECTS
  v ~ logit_normal(0.0, 1.0);  // degree of factivity
  w ~ logit_normal(0.0, 1.0);  // world knowledge
  
  // LIKELIHOOD
  target += truncated_normal_lpdf(y | v + (1.0 - v) * w, sigma, 0.0, 1.0);
}
```

Here, `v` represents the degree of factivity—it provides a "boost" to the world knowledge probability `w`, but never forces the response to 1.0. The response probability is computed as: `response = v + (1-v) * w`.

Let's trace through this computation:
- If `v = 0` (no factivity): `response = 0 + 1 * w = w` (pure world knowledge)
- If `v = 1` (full factivity): `response = 1 + 0 * w = 1` (certain)
- If `v = 0.5` (partial factivity): `response = 0.5 + 0.5 * w` (boosted world knowledge)

The full model augments this kernel with the same hierarchical structure as before:

```{.stan .line-numbers highlight="16-26"}
model {
  // PRIORS (analyst-added)
  verb_intercept_std ~ exponential(1);
  context_intercept_std ~ exponential(1);
  subj_verb_std ~ exponential(1);
  subj_context_std ~ exponential(1);
  sigma_e ~ beta(2, 10);
  
  // Hierarchical priors
  verb_logit_raw ~ std_normal();
  context_logit_raw ~ normal(mu_omega, sigma_omega);
  to_vector(subj_verb_raw) ~ std_normal();
  to_vector(subj_context_raw) ~ std_normal();
  
  // GRADIENT COMPUTATION (PDS kernel structure)
  for (n in 1:N) {
    // Degree of factivity for this verb/subject
    real verb_boost = inv_logit(verb_intercept[verb[n]] +
                                subj_intercept_verb[subj[n], verb[n]]);
    
    // World knowledge for this context/subject
    real context_prob = inv_logit(context_intercept[context[n]] +
                                  subj_intercept_context[subj[n], context[n]]);
    
    // GRADIENT LIKELIHOOD (PDS kernel computation)
    real response_prob = verb_boost + (1.0 - verb_boost) * context_prob;
    target += truncated_normal_lpdf(y[n] | response_prob, sigma_e, 0.0, 1.0);
  }
}
```

The highlighted lines show the PDS kernel—a continuous computation rather than discrete branching. The gradient contribution of factivity is clear in line 25: the verb provides a multiplicative boost to world knowledge rather than overriding it entirely.

### Response distributions and censoring

Both models use truncated normal distributions as response functions. As discussed in [the adjectives section](../adjectives/adjectives-aaron.html#censoring-and-boundary-effects), this handles the bounded nature of slider scales:

```stan
real truncated_normal_lpdf(real y | real mu, real sigma, real lower, real upper) {
  // Log probability of y under Normal(mu, sigma) truncated to [lower, upper]
  real lpdf = normal_lpdf(y | mu, sigma);
  real normalizer = log(normal_cdf(upper | mu, sigma) - normal_cdf(lower | mu, sigma));
  return lpdf - normalizer;
}
```

The truncation is crucial because many responses cluster at the scale boundaries (0 and 1), which standard distributions like Beta cannot handle directly.

### Generated quantities

Both models can include a generated quantities block to compute posterior predictions:

```stan
generated quantities {
  array[N] real y_pred;  // posterior predictive samples
  
  for (n in 1:N) {
    real verb_prob = inv_logit(verb_intercept[verb[n]] + 
                               subj_intercept_verb[subj[n], verb[n]]);
    real context_prob = inv_logit(context_intercept[context[n]] + 
                                  subj_intercept_context[subj[n], context[n]]);
    
    if (model_type == "discrete") {
      // Discrete: first sample branch, then response
      int branch = bernoulli_rng(verb_prob);
      if (branch == 1) {
        y_pred[n] = truncated_normal_rng(1.0, sigma_e, 0.0, 1.0);
      } else {
        y_pred[n] = truncated_normal_rng(context_prob, sigma_e, 0.0, 1.0);
      }
    } else {
      // Gradient: compute blended probability
      real response_prob = verb_prob + (1.0 - verb_prob) * context_prob;
      y_pred[n] = truncated_normal_rng(response_prob, sigma_e, 0.0, 1.0);
    }
  }
}
```

These posterior predictions let us visualize how well each model captures the empirical patterns.

## Additional models

For completeness, we also consider two models created by directly manipulating the Stan code rather than deriving them from linguistic hypotheses. These computational experiments serve as useful comparisons but don't correspond to theoretical proposals in the linguistics literature:

### Wholly-discrete model

This model treats both factivity and world knowledge as discrete:

```stan
// In model block
real verb_prob = inv_logit(verb_intercept[verb[n]] + subj_intercept_verb[subj[n], verb[n]]);
real context_prob = inv_logit(context_intercept[context[n]] + subj_intercept_context[subj[n], context[n]]);

// Both components are discrete
int verb_branch = bernoulli_rng(verb_prob);      // factive or not
int context_branch = bernoulli_rng(context_prob); // world knowledge true or not

// Response is certain (1) if either component is true
real response = (verb_branch == 1 || context_branch == 1) ? 1.0 : 0.0;
target += truncated_normal_lpdf(y[n] | response, sigma_e, 0.0, 1.0);
```

### Discrete-world model

This model has discrete world knowledge but gradient factivity:

```stan
// World knowledge is discrete
int context_branch = bernoulli_rng(context_prob);

if (context_branch == 1) {
  // If world knowledge says "true", response is certain
  target += truncated_normal_lpdf(y[n] | 1.0, sigma_e, 0.0, 1.0);
} else {
  // Otherwise, factivity provides gradient boost
  target += truncated_normal_lpdf(y[n] | verb_prob, sigma_e, 0.0, 1.0);
}
```

As noted, these models result mainly from us exploring all the options available within the frame, rather than linguistic theory; but they provide useful sanity checks.

## Model comparison

To evaluate these competing hypotheses, we can examine their empirical predictions. Using [the same model comparison techniques from adjectives](../adjectives/adjectives-aaron.html#comparing-models), we compute expected log pointwise predictive densities (ELPDs).

### Posterior predictive checks

First, let's visualize how well each model captures the distribution of responses:

![Posterior predictive distributions (with simulated participant intercepts) of all four models for six predicates from @degen_prior_2021's projection experiment 2b, for all contexts combined. Empirical distributions are represented by density histograms of data from @degen_prior_2021.](plots/contentful_all_6_pp.pdf){#fig:factivity-posteriors}

@fig:factivity-posteriors reveals striking differences between the models:

- **Discrete-factivity** (top left): Captures the characteristic dips in response frequency mid-scale—reflecting its mixture of factive (response ≈ 1) and non-factive (response varies) interpretations
- **Wholly-gradient** (bottom left): Produces smoother, unimodal distributions, unable to capture the multi-modal patterns in the data
- **Wholly-discrete** (top right): Forces responses to extremes, missing the intermediate values
- **Discrete-world** (bottom right): Shows some bimodality but in the wrong direction

The discrete-factivity model's ability to capture the non-monotonic response patterns is particularly clear for predicates like *announce* and *confirm*, where responses cluster both near 1 (factive interpretation) and at intermediate values (non-factive interpretation modulated by world knowledge).

### Quantitative comparison

Looking at the expected log pointwise predictive densities reveals a clear winner. The discrete-factivity model substantially outperforms all alternatives across the board. Compared to the wholly-gradient model, it achieves a ΔELPD of 834.5 ± 55.4—a massive improvement in predictive accuracy. The advantages over discrete-world (ΔELPD = 766.1 ± 53.8) and wholly-discrete (ΔELPD = 295.1 ± 34.8) models are similarly impressive. These differences are not just statistically significant but practically large, indicating that the discrete-factivity model provides a dramatically better account of the data.

![ELPDs for the four models. Dotted lines indicate estimated differences between each model and the discrete-factivity model. Error bars indicate standard errors.](plots/fits_elpd.pdf){#fig:factivity-elpds}

## Exploring additional dimensions

### Predicate-specific patterns

Not all predicates show the same degree of discreteness. We can examine the posterior distributions of the factivity parameter by predicate:

![Density plots of the posterior log-odds of projection (with participant intercepts zeroed out) for all four models for six predicates from @degen_prior_2021's projection experiment 2b.](plots/fit_6_thetas.pdf){#fig:factivity-predicate-posteriors}

Several patterns emerge from these posterior distributions. Canonical factives like *know* and *discover* show high probability of being interpreted with factive presupposition across models, while non-factives like *think* and *say* show consistently low probability. The most interesting cases are variable predicates like *confirm* and *prove*, which show intermediate probabilities with high uncertainty. This variation suggests that while factivity is discrete at the token level (each use involves either a factive or non-factive interpretation), predicates differ systematically in their propensity to trigger factive interpretations.

For a complete view of all 20 predicates tested:

:::{.callout-tip collapse="true" title="Click to see posterior distributions for all predicates"}
![Density plots of the posterior log-odds of projection for all predicates from @degen_prior_2021's projection experiment 2b.](plots/fit_full_thetas.pdf){#fig:factivity-all-predicate-posteriors}
:::

### Context effects

The norming study reveals how world knowledge varies across contexts:

![Density plots of the posterior log-odds certainty (with participant intercepts zeroed out) for three items in @degen_prior_2021's norming task (experiment 2a). Low and high priors are for *Grace visited her sister*, given the facts *Grace hates her sister* and *Grace loves her sister*, respectively. Mid prior is for *Sophia got a tattoo*, given the fact *Sophia is a hipster*.](plots/norming_3_thetas.pdf){#fig:norming-contexts}

The separation between low and high prior contexts validates the experimental manipulation—participants genuinely use world knowledge when judging likelihood. This makes the discrete-factivity model's success more impressive: it must overcome this continuous variation to produce discrete projection patterns.

### Complete posterior predictive distributions

For researchers interested in the full pattern across all predicates:

:::{.callout-tip collapse="true" title="Click to see posterior predictive distributions for all 20 predicates"}
![Posterior predictive distributions (with simulated participant intercepts) of all four models for all predicates in @degen_prior_2021's projection experiment 2b, for all contexts combined. Empirical distributions are represented by density histograms.](plots/fits_all_full_pp.pdf){#fig:factivity-all-posteriors}
:::

## Implementation challenges and solutions

The transition from PDS kernels to full statistical models reveals several challenges:

### 1. Identifiability

In mixture models, the components can sometimes "trade off"—different parameter combinations yield identical predictions. We address this through informative priors (using norming data to constrain world knowledge parameters), hierarchical structure (partial pooling across predicates and contexts), and multiple contexts per predicate (each predicate appears with different world knowledge levels).

### 2. Computational efficiency

Mixture models can be slow to fit (or not fit at all). We improve efficiency through non-centered parameterizations (as shown in the transformed parameters blocks), vectorization (operating on arrays rather than loops where possible), and warm starts (initializing chains near reasonable values).

### 3. Model checking

Beyond posterior predictive checks, we validate models through prior predictive checks (ensuring priors generate reasonable data), residual analysis (checking for systematic deviations), and cross-validation (using held-out data to assess generalization).

### 4. Alternative response distributions

To ensure our results aren't artifacts of the truncated normal distribution, we can consider alternative likelihoods–e.g. ordered beta distributions:

:::{.callout-note collapse="true" title="Robustness to response distribution choice"}
We also fit models using ordered beta distributions, which handle bounded responses differently:

![Left: ELPDs for all four ordered beta models. Right: ELPDs for truncated normal vs. ordered beta models on non-endpoint responses. Dotted lines indicate differences from discrete-factivity model.](plots/ordered_beta_elpd.pdf){#fig:ordered-beta-comparison}

The discrete-factivity model maintains its advantage regardless of response distribution, confirming the robustness of our findings.
:::

