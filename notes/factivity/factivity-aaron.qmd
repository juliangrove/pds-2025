# Factivity and presupposition projection: From PDS to Stan

Now that we've explored how PDS handles expected gradience in vagueness, we turn to a phenomenon where gradience is unexpected: factivity. This case study demonstrates how discrete semantic knowledge—expressed as branching computations in PDS—translates into mixture models in Stan. The translation process reveals both the power and challenges of compiling high-level semantic theories into statistical models.

## The phenomenon and the data

A predicate is said to be *factive* if it triggers inferences about the truth of its complement that persist even under entailment-canceling operators. When someone asserts "Jo knows that Mo left," they seem to presuppose—rather than assert—that Mo did indeed leave. This inference projects through various operators:

```
(1) a. Jo knows that Mo left.          ⇒ Mo left
    b. Jo doesn't know that Mo left.    ⇒ Mo left  
    c. Does Jo know that Mo left?       ⇒ Mo left
```

### The Degen & Tonhauser (2021) paradigm

To investigate how world knowledge influences these inferences, Degen and Tonhauser developed a two-stage experimental paradigm:

**Stage 1: Norming experiment** - Measure prior beliefs about propositions in context:
```
Context: Zoe is a kindergartener
How likely is it that Zoe can tie her shoes?
[slider: "very unlikely" to "very likely"]
```

**Stage 2: Projection experiment** - Measure inference strength with clause-embedding predicates:
```
Context: Zoe is a kindergartener
Edward discovered that Zoe can tie her shoes.
How certain is Edward that Zoe can tie her shoes?
[slider: "not certain" to "certain"]
```

They tested 20 predicates across theoretical categories (canonically factive, non-factive, optionally factive) with 20 complement clauses in high/low probability contexts. The results showed systematic gradience: mean certainty ratings varied continuously from 0.2 (pretend) to 0.85 (be annoyed), with paradigmatically factive "know" at 0.75.

## Two hypotheses in PDS

This gradience admits two interpretations that PDS can precisely formalize:

### The Fundamental Discreteness Hypothesis

Under this hypothesis, factivity is a discrete property—predicates have distinct factive and non-factive uses. Here's the actual PDS implementation for "know":

```haskell
-- From Grammar.Lexica.SynSem.Factivity
"knows" -> [ SynSem {
  syn = S :\: NP :/: S,
  sem = ty tau (lam s (purePP (lam p (lam x (lam i 
    (ITE (TauKnow s) 
         (And (epi i @@ x @@ p) (p @@ i))  -- Factive branch
         (epi i @@ x @@ p))))))            -- Non-factive branch
    @@ s))
} ]
```

Let's trace through this computation step by step:

1. **Input state**: The meaning takes a discourse state `s` containing factivity information
2. **Branching point**: `ITE (TauKnow s) ...` checks whether this use is factive
3. **Factive branch**: `And (epi i @@ x @@ p) (p @@ i)` - both epistemic state AND truth
4. **Non-factive branch**: `epi i @@ x @@ p` - only the epistemic state
5. **Result**: A proposition whose content depends on the discourse state

The prior distribution controls the branching probability:

```haskell
factivityPrior :: Term
factivityPrior = 
  let' x (LogitNormal 0 1)  -- Sample base rate
  (let' b (Bern x)           -- Sample factivity parameter
  (Return (... UpdTauKnow b ...)))  -- Update state
```

### The Fundamental Gradience Hypothesis

Under this hypothesis, predicates encode continuous degrees of commitment. While not implemented in the current PDS code, it would look like:

```haskell
-- Hypothetical gradient implementation
"knows" -> [ SynSem {
  sem = ty tau (lam s (
    let' strength (Beta 0.8 0.1) (    -- Sample commitment
      let' i (CG s) (                  -- Get common ground
        factor (strength * (p @@ i)) >>>  -- Soft presupposition
        purePP (epi i @@ x @@ p)))))
} ]
```

Instead of discrete branching, this uses continuous weighting through the `factor` operation.

## The compilation challenge: From branching to mixtures

The key challenge in translating factivity to Stan is that Stan doesn't support discrete parameters. We can't directly translate:

```haskell
ITE (TauKnow s) factive_meaning non_factive_meaning
```

Into:
```stan
if (tau_know[s]) {
  // Factive computation
} else {
  // Non-factive computation  
}
```

Because `tau_know` would need to be a discrete parameter, which Stan's HMC sampler cannot handle. Instead, we must *marginalize* over the discrete choice, transforming the branching computation into a mixture model.

### The marginalization transformation

Consider the response distribution for a single judgment. In PDS, we have:

```haskell
respond :: Term -> Term -> Term
respond bg update = 
  let' s bg (                          -- Sample initial state
    let' s' (update @@ s) (            -- Update with utterance
      let' answer (computeAnswer s') ( -- Compute answer
        sampleResponse answer)))       -- Generate response
```

For "know" with discrete factivity, `computeAnswer` branches:

```haskell
computeAnswer s = 
  if (TauKnow s) 
    then 1.0                    -- Factive: certain
    else prior_probability       -- Non-factive: follows prior
```

To marginalize, we weight each branch by its probability:

```
P(response | verb, context) = 
  P(factive | verb) × P(response | factive, context) +
  P(non-factive | verb) × P(response | non-factive, context)
```

This is exactly what `log_mix` implements in Stan:

```stan
real log_lik_lpdf(real resp, real verb_prob, real context_prob, real sigma) {
  return log_mix(
    verb_prob,                                         // P(factive)
    truncated_normal_lpdf(resp | 1, sigma, 0, 1),     // Factive response
    truncated_normal_lpdf(resp | context_prob, sigma, 0, 1)  // Non-factive response
  );
}
```

### Compiling the discrete-factivity model

Let's trace through the complete compilation from PDS to Stan:

#### Step 1: Identify the probabilistic structure

From the PDS lexical entry and prior, we extract:
- **Discrete choice**: Factive vs non-factive (per verb)
- **Continuous parameters**: Prior probability (per context), response noise
- **Hierarchical structure**: Verb-level, context-level, participant-level variation

#### Step 2: Transform to first-order parameters

PDS higher-order functions become arrays indexed by integers:

```haskell
-- PDS: Functions from verbs to probabilities
TauKnow :: State -> Verb -> Bool

-- Stan: Arrays indexed by verb number
vector[N_verb] verb_intercept;  // Log-odds of factivity
```

#### Step 3: Implement the likelihood

The key insight is that the PDS branching:

```haskell
ITE (TauKnow s) 
  (observe True >>> return 1.0)      -- Factive
  (return context_prob)               -- Non-factive
```

Becomes a mixture in Stan:

```stan
// In transformed parameters
verb_prob_by_resp[n] = inv_logit(
  verb_intercept[verb[n]] + subj_intercept_verb[subj[n]]
);

// In model block via log_lik function
target += log_mix(
  verb_prob_by_resp[n],
  truncated_normal_lpdf(resp[n] | 1, sigma, 0, 1),
  truncated_normal_lpdf(resp[n] | context_prob_by_resp[n], sigma, 0, 1)
);
```

#### Step 4: Add hierarchical structure

The PDS prior samples parameters hierarchically. This translates to Stan's non-centered parameterization:

```stan
parameters {
  // Population-level
  vector[N_verb] verb_intercept_z;      // Standardized
  
  // Participant-level variation
  real<lower=0> subj_intercept_verb_std;
  vector[N_subj] subj_intercept_verb_z;
  
  // Response noise
  real<lower=0,upper=1> sigma;
}

transformed parameters {
  // Convert from standardized
  vector[N_verb] verb_intercept = verb_std .* verb_intercept_z + verb_mean;
  vector[N_subj] subj_intercept_verb = subj_intercept_verb_std * subj_intercept_verb_z;
}
```

### Compiling the wholly-gradient model

The gradient hypothesis requires a different compilation strategy. Instead of marginalizing over discrete choices, we directly implement continuous combination:

```haskell
-- PDS gradient computation
let' verb_strength (sampleVerbStrength verb) (
  let' context_strength (sampleContextStrength context) (
    return (1 - (1 - verb_strength) * (1 - context_strength))))
```

This naturally translates to Stan without mixtures:

```stan
real log_lik_lpdf(real resp, real verb_prob, real context_prob, real sigma) {
  real prob_or = 1.0 - (1.0 - verb_prob) * (1.0 - context_prob);
  return truncated_normal_lpdf(resp | prob_or, sigma, 0, 1);
}
```

The formula implements probabilistic disjunction: the complement is likely true if either the verb suggests it OR world knowledge suggests it.

## Complete Stan implementation

Here's the full discrete-factivity model, annotated to show its connection to PDS:

```stan
functions {
  // Truncated normal for bounded responses (like adjectives Day 3)
  real truncated_normal_lpdf(real x, real mu, real sigma, real a, real b) {
    return normal_lpdf(x | mu, sigma) - 
           log_diff_exp(normal_lcdf(b | mu, sigma), 
                        normal_lcdf(a | mu, sigma));
  }
  
  // Mixture likelihood implementing PDS branching
  real log_lik_lpdf(real resp, real verb_prob, real context_prob, real sigma) {
    return log_mix(
      verb_prob,                                          // P(TauKnow = true)
      truncated_normal_lpdf(resp | 1, sigma, 0, 1),      // Factive branch
      truncated_normal_lpdf(resp | context_prob, sigma, 0, 1)  // Non-factive branch
    );
  }
}

data {
  // Experimental structure
  int<lower=0> N_resp;      // Total responses
  int<lower=0> N_verb;      // Number of verbs (20)
  int<lower=0> N_context;   // Number of contexts (40)
  int<lower=0> N_subj;      // Number of participants
  
  // Prior information from norming (implements world knowledge)
  vector[N_verb] verb_mean;         // From previous model or uniform
  vector[N_verb] verb_std;          // Uncertainty about verb factivity
  vector[N_context] context_mean;   // From norming experiment
  vector[N_context] context_std;    // Uncertainty about context
  
  // Indexing (implements function application via arrays)
  int<lower=1,upper=N_verb> verb[N_resp];
  int<lower=1,upper=N_context> context[N_resp];
  int<lower=1,upper=N_subj> subj[N_resp];
  
  // Responses
  vector<lower=0,upper=1>[N_resp] resp;
}

parameters {
  // Verb factivity (implements TauKnow per verb)
  vector[N_verb] verb_intercept_z;
  
  // Context effects (implements world knowledge)
  real<lower=0> context_intercept_std;
  vector[N_context] context_intercept_z;
  
  // Participant variation (implements individual differences)
  real<lower=0> subj_intercept_verb_std;
  vector[N_subj] subj_intercept_verb_z;
  real<lower=0> subj_intercept_context_std;
  vector[N_subj] subj_intercept_context_z;
  
  // Response noise (implements imperfect measurement)
  real<lower=0,upper=1> sigma;
}

transformed parameters {
  // Transform standardized parameters (non-centered parameterization)
  vector[N_verb] verb_intercept = verb_std .* verb_intercept_z + verb_mean;
  vector[N_context] context_intercept = context_intercept_std * context_intercept_z;
  vector[N_subj] subj_intercept_verb = subj_intercept_verb_std * subj_intercept_verb_z;
  vector[N_subj] subj_intercept_context = subj_intercept_context_std * subj_intercept_context_z;
  
  // Compute response-level probabilities
  vector[N_resp] log_lik;
  vector[N_resp] verb_prob_by_resp;
  vector[N_resp] context_prob_by_resp;
  
  for (n in 1:N_resp) {
    // Verb probability: P(TauKnow = true | verb, participant)
    verb_prob_by_resp[n] = inv_logit(
      verb_intercept[verb[n]] + subj_intercept_verb[subj[n]]
    );
    
    // Context probability: P(complement true | context, participant)
    context_prob_by_resp[n] = inv_logit(
      context_intercept[context[n]] + subj_intercept_context[subj[n]]
    );
    
    // Likelihood: implements marginalized PDS computation
    log_lik[n] = log_lik_lpdf(
      resp[n] | verb_prob_by_resp[n], context_prob_by_resp[n], sigma
    );
  }
}

model {
  // Priors (implement PDS prior distributions)
  verb_intercept_z ~ std_normal();
  context_intercept_std ~ exponential(1);
  context_intercept_z ~ std_normal();
  subj_intercept_verb_std ~ exponential(1);
  subj_intercept_verb_z ~ std_normal();
  subj_intercept_context_std ~ exponential(1);
  subj_intercept_context_z ~ std_normal();
  
  // Likelihood
  for (n in 1:N_resp)
    target += log_lik[n];
}

generated quantities {
  // Extract interpretable parameters
  vector[N_verb] verb_prob = inv_logit(verb_intercept);
  vector[N_context] context_prob = inv_logit(context_intercept);
}
```

## Compilation patterns and challenges

Several patterns emerge when compiling factivity models:

### Pattern 1: Discrete choices become mixtures

Every PDS branching on discrete parameters:
```haskell
ITE discrete_param option1 option2
```

Becomes a mixture in Stan:
```stan
log_mix(prob_param, likelihood1, likelihood2)
```

This transformation is systematic but changes the model's interpretation—from sampling discrete values to marginalizing over them.

### Pattern 2: State dependencies become array indexing

PDS tracks factivity in discourse state:
```haskell
TauKnow :: State -> Bool
```

Stan implements this as arrays:
```stan
vector[N_verb] tau_know_logit;  // Per-verb factivity
```

The state threading in PDS becomes implicit in Stan's array structure.

### Pattern 3: Hierarchical sampling becomes non-centered parameterization

PDS samples hierarchically:
```haskell
let' verb_base (Normal 0 1) (
  let' participant_offset (Normal 0 sigma_participant) (
    return (verb_base + participant_offset)))
```

Stan uses non-centered parameterization for efficiency:
```stan
// Sample standardized
verb_z ~ std_normal();
participant_z ~ std_normal();

// Transform with scales
verb = verb_std * verb_z;
participant = participant_std * participant_z;
```

### Pattern 4: Response functions become truncated distributions

PDS response function:
```haskell
respond (lam x (Truncate (Normal x sigma) 0 1))
```

Becomes Stan likelihood:
```stan
resp ~ normal(predicted, sigma) T[0, 1];
// Or equivalently:
target += truncated_normal_lpdf(resp | predicted, sigma, 0, 1);
```

## Model comparison results

The four models make different predictions about response distributions:

- **Discrete-factivity**: Bimodal mixtures (factive mode at 1, non-factive follows context)
- **Wholly-gradient**: Smooth unimodal distributions
- **Wholly-discrete**: Mixtures with modes only at 0 and 1
- **Discrete-world**: Gradient factivity with discrete world knowledge

Comparing via ELPD on Degen & Tonhauser's data:

```
Model               ELPD (SE)    Interpretation
Discrete-factivity   2917 (44)   Factivity discrete, world knowledge gradient
Wholly-discrete      2739 (42)   Both discrete
Wholly-gradient      2698 (41)   Both gradient
Discrete-world       2689 (41)   Factivity gradient, world knowledge discrete
```

The discrete-factivity model's superior fit suggests that the PDS branching structure—rather than continuous combination—better captures how people process factive inferences.

## Theoretical implications for compilation

This case study reveals several important aspects of the PDS-to-Stan pipeline:

1. **Branching vs. weighting**: Discrete semantic choices in PDS become probability weightings in Stan. This preserves the intuition while enabling inference.

2. **Marginalization is key**: We can't sample discrete parameters in Stan, but we can marginalize over them. This transformation is systematic and preserves the model's predictions.

3. **Mixture emergence**: The bimodal response patterns that support discrete factivity emerge naturally from compiling branching PDS programs to mixture models.

4. **Hierarchical structure preserved**: The compositional structure in PDS (predicates → contexts → participants) maps cleanly to Stan's hierarchical models.

## Looking ahead: Questions and discourse dynamics

Our treatment of factivity has abstracted away from a crucial component: the role of questions in inference tasks. Recall that participants judge "How certain is Edward that Zoe can tie her shoes?" This implicitly involves:

1. **Question semantics**: What does "how certain" mean?
2. **QUD structure**: How do questions under discussion affect interpretation?
3. **Response pragmatics**: How do participants map internal certainty to slider positions?

In the next session, we'll see how PDS handles these discourse-dynamic phenomena. As shown in the PDS code, questions can be explicitly modeled:

```haskell
ask κ = κ >>>= Lam "q" (getPP >>>= Lam "s" (putPP (upd_QUD q s)))
```

This machinery will prove essential for modeling:
- How question type affects inference judgments
- The role of QUDs in projection phenomena
- Conventional implicatures and parentheticals
- Evidential markers across languages
- Socio-semantic inferences

By integrating question semantics into our models, we can move beyond simple assertion-based tasks to capture the full richness of discourse-based reasoning that underlies linguistic inference.