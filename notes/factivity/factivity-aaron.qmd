---
title: "Factivity"
---

Having explored how PDS handles expected gradience in adjectives, we now turn to factivity—where gradience poses a deeper theoretical puzzle. While the gradience in adjective meanings arises from well-understood sources like vagueness and contextual variation, the gradience observed in factivity judgments challenges our basic understanding of what it means for speakers to presuppose the truth of a complement when using certain predicates.

## Factivity and projection

Before diving into computational models, let's review what makes factivity special. Factivity describes the property of certain predicates that are associated with the presupposition that their complements are true, even when the predicate is embedded under various operators. Compare these sentences:

[]{@factive-projection-pos}
[]{@factive-projection-neg}
[]{@factive-projection-q}
[]{@factive-projection-cond}

(@factive-projection-pos) Jo loved that Mo left.
(@factive-projection-neg) Jo didn't love that Mo left.
(@factive-projection-q) Did Jo love that Mo left?
(@factive-projection-cond) If Jo loved that Mo left, she'll won't be upset that Bo left.

In all cases, there's a strong inference that Mo actually left. This inference **projects** through negation, questions, and conditionals—the hallmark of presupposition. Contrast this with non-factive predicates:

[]{@non-factive-pos}
[]{@non-factive-neg}

(@non-factive-pos) Jo thinks/believes/said that Mo left.
(@non-factive-neg) Jo doesn't think/believe/say that Mo left.

Here, using (@non-factive-neg) doesn't commit us to Mo having left. Given just these examples, the contrast feels charp: predicates are either associated with factive presuppositions or not.

But this traditional picture has been challenged by experimental work showing substantial gradience in projection judgments. Some predicates (like *know*) are almost always associated with projection, others (like *think*) rarely trigger it, and many fall somewhere in between. What explains this gradience? For instance, @white_role_2018 measured veridicality inferences for more than 700 predicates in their MegaVeridicality dataset–visualized in @fig:veridicality-factivity.

![An aggregate measure of factivity, derived from the MegaVeridicality dataset of @white_role_2018. Points are jittered (0.05) to avoid overplotting. The x-axis corresponds to mean scores for affirmative contexts, the y-axis for negative contexts. Each point is a predicate. Note the continuous gradient rather than discrete clusters.](plots/veridicality_factivity.pdf){#fig:veridicality-factivity}

As @fig:veridicality-factivity shows, the empirical landscape reveals a continuum of projection behavior rather than the discrete categories traditional theories would predict. This gradience is what our models must explain.

## Two competing hypotheses

Using PDS, we can precisely state two fundamentally different accounts of the observed gradience:

**Discrete-factivity hypothesis**: Factivity is a discrete property—speakers either interpret a predicate as triggering a factive presupposition on a given occasion of use or they don't. The observed gradience reflects uncertainty about which interpretation is intended, analogous to lexical ambiguity. Different uses of *discover* might involve different lexical entries or different syntactic structures, only some of which are associated with presupposition.

**Wholly-gradient hypothesis**: There is no discrete factivity property. Instead, predicates make gradient contributions to inferences about their complements' truth. This view, aligned with @tonhauser_how_2018's Gradient Projection Principle, treats projection as a continuous phenomenon where content projects "to the extent that it is not at-issue."

As we'll see, PDS allows us to implement both hypotheses as statistical models and compare their empirical predictions. But first, we need to understand the experimental paradigm that gives us the data to test these hypotheses.

## Experimental paradigms

### Separating world knowledge from factivity

A key challenge in studying factivity is that our judgments about whether someone is certain of a proposition depend on two factors:

1. The properties of the predicate (whether speakers interpret it as being associated with a presupposition)
2. Our world knowledge about how likely the proposition is

@degen_prior_2021 developed a two-stage experimental design to tease these apart:

**Stage 1: Norming study (Experiment 2a)**

First, they measured how world knowledge affects belief in various propositions. Participants saw items like:

:::{.callout-note title="Norming item example"}
**Fact**: Sophia is a hipster.  
**Question**: How likely is it that Sophia got a tattoo?  
**Response**: [slider from 0 to 1]
:::

For each proposition (e.g., *Sophia got a tattoo*), they created two contexts–one making it more likely (Sophia is a hipster) and one making it less likely (*Sophia is a Mormon*). This gives us a measure of world knowledge independent of any embedding predicate.

**Stage 2: Projection study (Experiment 2b)**

Next, they embedded these same propositions under various predicates and asked about the speaker's certainty:

:::{.callout-note title="Projection item example"}
**Context**: Isabella said that Sophia is a hipster.  
**Utterance**: Noah knows that Sophia got a tattoo.  
**Question**: Is Noah certain that Sophia got a tattoo?  
**Response**: [slider from 0 to 1]
:::

They reasoned that, if speakers categorically interpret *know* as triggering a factive presupposition, Noah should be certain regardless of the prior probability; but, they reasoned, if projection is gradient, his certainty should vary with world knowledge.

### Key experimental details

The experiments tested 20 predicates that theory suggests might pattern differently:

- **Canonical factives**: *know*, *discover*, *realize*, *be aware*, *reveal*
- **Non-factives**: *think*, *believe*, *say*, *suggest*, *pretend*  
- **"Semifactives"**: *see*, *hear*, *notice*, *find out*
- **Others with variable behavior**: *confirm*, *admit*, *be right*

Each participant saw 20 items (one per predicate) plus controls, with predicates paired with different complement clauses and prior contexts across participants.

![Verb means from @degen_prior_2021's experiment 2b. "Non-factive" verbs are in red, "optionally factive" verbs are in teal, and "canonically factive" verbs are in green. Violin plots indicate the probability density of responses, showing the gradient nature of the empirical data.](plots/projection_no_fact_means.pdf){#fig:projection-verb-means}

@fig:projection-verb-means reveals the empirical challenge: rather than clustering into discrete categories, predicates show a (qualitatively) continuous gradient of projection behavior. This is precisely what our competing hypotheses must explain.

## World knowledge: gradient or discrete?

Before examining factivity proper, let's understand how the norming study helps us model world knowledge. The norming data allows us to test whether world knowledge itself involves discrete or gradient uncertainty. (The structure of this model is very similar to the one we covered in the [adjectives section](../adjectives/adjectives-aaron.html#world-knowledge-gradient-or-discrete).)

![Left: ELPDs for the two models of the norming data from @degen_prior_2021's experiment 2a. Dotted line indicates estimated difference between the norming-discrete model and the norming-gradient model. Right: posterior predictive distributions for both models for the item *Sophia got a tattoo*, given either the fact *Sophia is a hipster* (5H) or *Sophia is a fashion model* (5L). Error bars indicate standard errors.](plots/norming_elpd.pdf){#fig:norming-elpds-pps}

As @fig:norming-elpds-pps shows, the gradient model of world knowledge substantially outperforms the discrete model (ΔELPD = 442.3 ± 23.1). The posterior predictive distributions reveal why: participants' judgments cluster away from the scale endpoints, a pattern the gradient model captures naturally but the discrete model cannot. This establishes that world knowledge contributes gradient uncertainty to factivity judgments.

## From PDS to Stan: implementing the theories

Building on [the compilation pipeline introduced for adjectives](../adjectives/adjectives-aaron.html#the-pds-to-stan-pipeline), let's see how PDS handles the competing theories of factivity. Recall that PDS outputs **kernel models**—the semantic core corresponding directly to our theoretical commitments.

### Discrete-factivity in PDS

For the discrete-factivity hypothesis, we can derive projection judgments using:

```haskell
-- Define the discourse: "Jo knows that Bo is a linguist"
s1         = termOf $ getSemantics @Factivity 1 ["jo", "knows", "that", "bo", "is", "a", "linguist"]
q1         = termOf $ getSemantics @Factivity 1 ["how", "likely", "that", "bo", "is", "a", "linguist"]
discourse  = ty tau $ assert s1 >>> ask q1

-- Compile to Stan using factivityPrior and factivityRespond
factivityExample = asTyped tau (betaDeltaNormal deltaRules . factivityRespond factivityPrior) discourse
```

This compilation process involves several key components. First, the lexical entry for *know* branches based on discourse state:

```haskell
"knows" -> [ SynSem {
    syn = S :\: NP :/: S,
    sem = ty tau (lam s (purePP (lam p (lam x (lam i 
      (ITE (TauKnow i) 
           (And (epi i @@ x @@ p) (p @@ i))  -- factive: belief AND truth
           (epi i @@ x @@ p))))))            -- non-factive: belief only
      @@ s)
} ]
```

The `ITE` (if-then-else) creates a discrete choice: either the speaker interprets the predicate as requiring the complement to be true (factive interpretation) or only the belief component is required (non-factive interpretation). The `TauKnow` parameter, which can vary by context, determines which branch is taken.

To accommodate this contextual variation, the prior over states must be updated:

```haskell
factivityPrior = let' x (LogitNormal 0 1) 
                 (let' y (LogitNormal 0 1) 
                 (let' z (LogitNormal 0 1) 
                 (Return (UpdCG (let' b (Bern x) 
                               (let' c (Bern y) 
                               (let' d (Bern z) 
                               (Return (UpdTauKnow b 
                                       (UpdLing (lam x c) 
                                       (UpdEpi (lam x (lam p d)) _0))))))) 
                         ϵ))))
```

The delta rules must also be modified to handle the contextual factivity parameter:

```haskell
-- Computes functions on indices
indices :: DeltaRule
indices = \case
  TauKnow (UpdTauKnow b _) -> Just b
  TauKnow (UpdEpi _ s)     -> Just (TauKnow s)
  TauKnow (UpdLing _ s)    -> Just (TauKnow s)
  -- ... other cases
```

PDS compiles this discrete-factivity theory to the following kernel model:^[Actual PDS output: `model { v ~ logit_normal(0.0, 1.0); w ~ logit_normal(0.0, 1.0); target += log_mix(v, truncated_normal_lpdf(y | 1.0, sigma, 0.0, 1.0), truncated_normal_lpdf(y | w, sigma, 0.0, 1.0)); }`]

```stan
model {
  // FIXED EFFECTS
  v ~ logit_normal(0.0, 1.0);  // probability of factive interpretation
  w ~ logit_normal(0.0, 1.0);  // world knowledge (from norming)
  
  // LIKELIHOOD
  target += log_mix(v, 
                    truncated_normal_lpdf(y | 1.0, sigma, 0.0, 1.0),     // factive branch
                    truncated_normal_lpdf(y | w, sigma, 0.0, 1.0));      // non-factive branch
}
```

This kernel captures the discrete branching: with probability `v`, the response is near 1.0 (factive interpretation); otherwise, it depends on world knowledge `w` (non-factive interpretation).

### Understanding the Stan implementation

Before augmenting this kernel to handle real data, let's briefly review the key additions for factivity.^[For a detailed introduction to Stan's blocks and syntax, see [the Stan introduction in the adjectives section](../adjectives/adjectives-aaron.html#stan-a-language-for-statistical-modeling).]

The factivity-specific components are:
- Hierarchical structure for verb-specific and context-specific effects
- Integration with norming study priors via `mu_omega` and `sigma_omega`
- Mixture likelihood for discrete-factivity (highlighted lines 25-27 in the full model below)

Here's how the discrete-factivity kernel is augmented for real data:

```{.stan .line-numbers}
model {
  // PRIORS (analyst-added)
  verb_intercept_std ~ exponential(1);
  context_intercept_std ~ exponential(1);
  subj_verb_std ~ exponential(1);
  subj_context_std ~ exponential(1);
  sigma_e ~ beta(2, 10);  // mildly informative prior keeping sigma_e small
  
  // Hierarchical priors
  verb_logit_raw ~ std_normal();
  context_logit_raw ~ normal(mu_omega, sigma_omega);  // informed by norming
  to_vector(subj_verb_raw) ~ std_normal();
  to_vector(subj_context_raw) ~ std_normal();
  
  // DISCRETE FACTIVITY (PDS kernel structure)
  for (n in 1:N) {
    // Probability of factive interpretation for this verb/subject combo
    real verb_prob = inv_logit(verb_intercept[verb[n]] + // |highlight: 17|
                               subj_intercept_verb[subj[n], verb[n]]); // |highlight: 18|
    
    // World knowledge probability for this context/subject combo  
    real context_prob = inv_logit(context_intercept[context[n]] + // |highlight: 21|
                                  subj_intercept_context[subj[n], context[n]]); // |highlight: 22|
    
    // MIXTURE LIKELIHOOD (PDS kernel structure)
    target += log_mix(verb_prob, // |highlight: 25|
                      truncated_normal_lpdf(y[n] | 1.0, sigma_e, 0.0, 1.0), // |highlight: 26|
                      truncated_normal_lpdf(y[n] | context_prob, sigma_e, 0.0, 1.0)); // |highlight: 27|
  }
}
```

The highlighted lines (17-18, 21-22, 25-27) represent the kernel model from PDS—encoding discrete factivity as a mixture of two response distributions. The unhighlighted portions add the statistical machinery needed for real data.

### Wholly-gradient factivity

The alternative wholly-gradient hypothesis treats factivity as continuously variable. The key modification to the PDS code is in how we encode the gradience. Instead of discrete branching, the gradient model computes a weighted combination.

PDS outputs this kernel for the wholly-gradient model:^[Actual PDS output after adding rendering hooks: `model { v ~ logit_normal(0.0, 1.0); w ~ logit_normal(0.0, 1.0); target += truncated_normal_lpdf(y | v + (1.0 - v) * w, sigma, 0.0, 1.0); }`]

```stan
model {
  // FIXED EFFECTS
  v ~ logit_normal(0.0, 1.0);  // degree of factivity
  w ~ logit_normal(0.0, 1.0);  // world knowledge
  
  // LIKELIHOOD
  target += truncated_normal_lpdf(y | v + (1.0 - v) * w, sigma, 0.0, 1.0);
}
```

Here, `v` represents the degree of factivity—it provides a "boost" to the world knowledge probability `w`, but never forces the response to 1.0. The response probability is computed as: `response = v + (1-v) * w`.

Let's trace through this computation:
- If `v = 0` (no factivity): `response = 0 + 1 * w = w` (pure world knowledge)
- If `v = 1` (full factivity): `response = 1 + 0 * w = 1` (certain)
- If `v = 0.5` (partial factivity): `response = 0.5 + 0.5 * w` (boosted world knowledge)

The full model augments this kernel with the same hierarchical structure as before:

```{.stan .line-numbers}
model {
  // PRIORS (analyst-added)
  verb_intercept_std ~ exponential(1);
  context_intercept_std ~ exponential(1);
  subj_verb_std ~ exponential(1);
  subj_context_std ~ exponential(1);
  sigma_e ~ beta(2, 10);
  
  // Hierarchical priors
  verb_logit_raw ~ std_normal();
  context_logit_raw ~ normal(mu_omega, sigma_omega);
  to_vector(subj_verb_raw) ~ std_normal();
  to_vector(subj_context_raw) ~ std_normal();
  
  // GRADIENT COMPUTATION (PDS kernel structure)
  for (n in 1:N) {
    // Degree of factivity for this verb/subject
    real verb_boost = inv_logit(verb_intercept[verb[n]] + // |highlight: 17|
                                subj_intercept_verb[subj[n], verb[n]]); // |highlight: 18|
    
    // World knowledge for this context/subject
    real context_prob = inv_logit(context_intercept[context[n]] + // |highlight: 21|
                                  subj_intercept_context[subj[n], context[n]]); // |highlight: 22|
    
    // GRADIENT LIKELIHOOD (PDS kernel computation)
    real response_prob = verb_boost + (1.0 - verb_boost) * context_prob; // |highlight: 25|
    target += truncated_normal_lpdf(y[n] | response_prob, sigma_e, 0.0, 1.0); // |highlight: 26|
  }
}
```

Lines 17-18, 21-22, and 25-26 (highlighted) show the PDS kernel—a continuous computation rather than discrete branching. The gradient contribution of factivity is clear in line 25: the verb provides a multiplicative boost to world knowledge rather than overriding it entirely.

### Response distributions and censoring

Both models use truncated normal distributions as response functions. As discussed in [the adjectives section](../adjectives/adjectives-aaron.html#censoring-and-boundary-effects), this handles the bounded nature of slider scales:

```stan
real truncated_normal_lpdf(real y | real mu, real sigma, real lower, real upper) {
  // Log probability of y under Normal(mu, sigma) truncated to [lower, upper]
  real lpdf = normal_lpdf(y | mu, sigma);
  real normalizer = log(normal_cdf(upper | mu, sigma) - normal_cdf(lower | mu, sigma));
  return lpdf - normalizer;
}
```

The truncation is crucial because many responses cluster at the scale boundaries (0 and 1), which standard distributions like Beta cannot handle directly.

### Generated quantities

Both models can include a generated quantities block to compute posterior predictions:

```stan
generated quantities {
  array[N] real y_pred;  // posterior predictive samples
  
  for (n in 1:N) {
    real verb_prob = inv_logit(verb_intercept[verb[n]] + 
                               subj_intercept_verb[subj[n], verb[n]]);
    real context_prob = inv_logit(context_intercept[context[n]] + 
                                  subj_intercept_context[subj[n], context[n]]);
    
    if (model_type == "discrete") {
      // Discrete: first sample branch, then response
      int branch = bernoulli_rng(verb_prob);
      if (branch == 1) {
        y_pred[n] = truncated_normal_rng(1.0, sigma_e, 0.0, 1.0);
      } else {
        y_pred[n] = truncated_normal_rng(context_prob, sigma_e, 0.0, 1.0);
      }
    } else {
      // Gradient: compute blended probability
      real response_prob = verb_prob + (1.0 - verb_prob) * context_prob;
      y_pred[n] = truncated_normal_rng(response_prob, sigma_e, 0.0, 1.0);
    }
  }
}
```

These posterior predictions let us visualize how well each model captures the empirical patterns.

## Additional models

For completeness, we also consider two models created by directly manipulating the Stan code rather than deriving them from linguistic hypotheses. These computational experiments serve as useful comparisons but don't correspond to theoretical proposals in the linguistics literature:

### Wholly-discrete model

This model treats both factivity and world knowledge as discrete:

```stan
// In model block
real verb_prob = inv_logit(verb_intercept[verb[n]] + subj_intercept_verb[subj[n], verb[n]]);
real context_prob = inv_logit(context_intercept[context[n]] + subj_intercept_context[subj[n], context[n]]);

// Both components are discrete
int verb_branch = bernoulli_rng(verb_prob);      // factive or not
int context_branch = bernoulli_rng(context_prob); // world knowledge true or not

// Response is certain (1) if either component is true
real response = (verb_branch == 1 || context_branch == 1) ? 1.0 : 0.0;
target += truncated_normal_lpdf(y[n] | response, sigma_e, 0.0, 1.0);
```

### Discrete-world model

This model has discrete world knowledge but gradient factivity:

```stan
// World knowledge is discrete
int context_branch = bernoulli_rng(context_prob);

if (context_branch == 1) {
  // If world knowledge says "true", response is certain
  target += truncated_normal_lpdf(y[n] | 1.0, sigma_e, 0.0, 1.0);
} else {
  // Otherwise, factivity provides gradient boost
  target += truncated_normal_lpdf(y[n] | verb_prob, sigma_e, 0.0, 1.0);
}
```

As noted, these models result mainly from us exploring all the options available within the frame, rather than linguistic theory; but they provide useful sanity checks.

## Model comparison

To evaluate these competing hypotheses, we can examine their empirical predictions. Using [the same model comparison techniques from adjectives](../adjectives/adjectives-aaron.html#comparing-models), we compute expected log pointwise predictive densities (ELPDs).

### Posterior predictive checks

First, let's visualize how well each model captures the distribution of responses:

![Posterior predictive distributions (with simulated participant intercepts) of all four models for six predicates from @degen_prior_2021's projection experiment 2b, for all contexts combined. Empirical distributions are represented by density histograms of data from @degen_prior_2021.](plots/contentful_all_6_pp.pdf){#fig:factivity-posteriors}

@fig:factivity-posteriors reveals striking differences between the models:

- **Discrete-factivity** (top left): Captures the characteristic dips in response frequency mid-scale—reflecting its mixture of factive (response ≈ 1) and non-factive (response varies) interpretations
- **Wholly-gradient** (bottom left): Produces smoother, unimodal distributions, unable to capture the multi-modal patterns in the data
- **Wholly-discrete** (top right): Forces responses to extremes, missing the intermediate values
- **Discrete-world** (bottom right): Shows some bimodality but in the wrong direction

The discrete-factivity model's ability to capture the non-monotonic response patterns is particularly clear for predicates like *announce* and *confirm*, where responses cluster both near 1 (factive interpretation) and at intermediate values (non-factive interpretation modulated by world knowledge).

### Quantitative comparison

Looking at the expected log pointwise predictive densities reveals a clear winner. The discrete-factivity model substantially outperforms all alternatives across the board. Compared to the wholly-gradient model, it achieves a ΔELPD of 834.5 ± 55.4—a massive improvement in predictive accuracy. The advantages over discrete-world (ΔELPD = 766.1 ± 53.8) and wholly-discrete (ΔELPD = 295.1 ± 34.8) models are similarly impressive. These differences are not just statistically significant but practically large, indicating that the discrete-factivity model provides a dramatically better account of the data.

![ELPDs for the four models. Dotted lines indicate estimated differences between each model and the discrete-factivity model. Error bars indicate standard errors.](plots/fits_elpd.pdf){#fig:factivity-elpds}

### Robustness across paradigms

To ensure these results aren't artifacts of the particular experimental design, we can examine how the models perform on variations:

![ELPDs for the four models with an anti-veridicality component added for each predicate.](plots/fits_a-v_elpd.pdf){#fig:factivity-av-elpds}

Even when we add an anti-veridicality component (allowing predicates like *pretend* to actively signal falsity), the discrete-factivity model maintains its advantage. The relative ordering of models remains unchanged.

### Cross-experimental validation

The discrete-factivity advantage persists across different experimental contexts:

![ELPDs for the four model evaluations on our replication experiment data.](plots/contentful_elpd.pdf){#fig:factivity-replication-elpds}

In a direct replication with new participants, the same pattern emerges. The discrete-factivity model's ELPD advantage remains substantial, confirming that the results reflect genuine properties of factivity rather than experimental artifacts.

### Replication success

To verify that our replication captured the same empirical patterns as the original study:

![Left: item means from @degen_prior_2021 vs. our replication (Spearman's r = 0.68, p ≤ 0.001). Right: verb means (Spearman's r = 0.98, p ≤ 0.001). "Non-factive" verbs are in red, "optionally factive" verbs are in teal, and "canonically factive" verbs are in green.](plots/projection_item_means.pdf){#fig:replication-comparison}

The near-perfect correlation at the verb level (r = 0.98) confirms that our replication successfully captured the same factivity patterns as the original study, validating our model comparisons.

### Non-contentful experiments

To test whether factivity patterns depend on rich lexical content, we conducted two additional experiments with minimal contexts:

![Left: @degen_prior_2021's projection data vs. bleached contexts (Spearman's r = 0.97, p ≤ 0.001). Right: templatic contexts (Spearman's r = 0.87, p ≤ 0.001). "Non-factive" verbs are in red, "optionally factive" verbs are in teal, and "canonically factive" verbs are in green.](plots/bleached_verb_means.pdf){#fig:non-contentful-comparison}

Remarkably, even when complement clauses are reduced to "a particular thing happened" (bleached) or "X happened" (templatic), the factivity patterns persist with high correlations to the original data.

![ELPDs for the four model evaluations on the bleached data (left) and the templatic data (right). Dotted lines indicate estimated differences between each model and the discrete-factivity model. Error bars indicate standard errors.](plots/non-contentful_elpds.pdf){#fig:non-contentful-elpds}

Crucially, the discrete-factivity model maintains its advantage even in these minimal contexts. This suggests that discrete factivity is a robust property of how speakers interpret predicates, not dependent on rich contextual information.

## Exploring additional dimensions

### Predicate-specific patterns

Not all predicates show the same degree of discreteness. We can examine the posterior distributions of the factivity parameter by predicate:

![Density plots of the posterior log-odds of projection (with participant intercepts zeroed out) for all four models for six predicates from @degen_prior_2021's projection experiment 2b.](plots/fit_6_thetas.pdf){#fig:factivity-predicate-posteriors}

Several patterns emerge from these posterior distributions. Canonical factives like *know* and *discover* show high probability of being interpreted with factive presupposition across models, while non-factives like *think* and *say* show consistently low probability. The most interesting cases are variable predicates like *confirm* and *prove*, which show intermediate probabilities with high uncertainty. This variation suggests that while factivity is discrete at the token level (each use involves either a factive or non-factive interpretation), predicates differ systematically in their propensity to trigger factive interpretations.

For a complete view of all 20 predicates tested:

:::{.callout-tip collapse="true" title="Click to see posterior distributions for all predicates"}
![Density plots of the posterior log-odds of projection for all predicates from @degen_prior_2021's projection experiment 2b.](plots/fit_full_thetas.pdf){#fig:factivity-all-predicate-posteriors}
:::

### Context effects

The norming study reveals how world knowledge varies across contexts:

![Density plots of the posterior log-odds certainty (with participant intercepts zeroed out) for three items in @degen_prior_2021's norming task (experiment 2a). Low and high priors are for *Grace visited her sister*, given the facts *Grace hates her sister* and *Grace loves her sister*, respectively. Mid prior is for *Sophia got a tattoo*, given the fact *Sophia is a hipster*.](plots/norming_3_thetas.pdf){#fig:norming-contexts}

The separation between low and high prior contexts validates the experimental manipulation—participants genuinely use world knowledge when judging likelihood. This makes the discrete-factivity model's success more impressive: it must overcome this continuous variation to produce discrete projection patterns.

### Complete posterior predictive distributions

For researchers interested in the full pattern across all predicates:

:::{.callout-tip collapse="true" title="Click to see posterior predictive distributions for all 20 predicates"}
![Posterior predictive distributions (with simulated participant intercepts) of all four models for all predicates in @degen_prior_2021's projection experiment 2b, for all contexts combined. Empirical distributions are represented by density histograms.](plots/fits_all_full_pp.pdf){#fig:factivity-all-posteriors}
:::

## Implementation challenges and solutions

The transition from PDS kernels to full statistical models reveals several challenges:

### 1. Identifiability

In mixture models, the components can sometimes "trade off"—different parameter combinations yield identical predictions. We address this through informative priors (using norming data to constrain world knowledge parameters), hierarchical structure (partial pooling across predicates and contexts), and multiple contexts per predicate (each predicate appears with different world knowledge levels).

### 2. Computational efficiency

Mixture models can be slow to fit (or not fit at all). We improve efficiency through non-centered parameterizations (as shown in the transformed parameters blocks), vectorization (operating on arrays rather than loops where possible), and warm starts (initializing chains near reasonable values).

### 3. Model checking

Beyond posterior predictive checks, we validate models through prior predictive checks (ensuring priors generate reasonable data), residual analysis (checking for systematic deviations), and cross-validation (using held-out data to assess generalization).

### 4. Alternative response distributions

To ensure our results aren't artifacts of the truncated normal distribution, we can consider alternative likelihoods–e.g. ordered beta distributions:

:::{.callout-note collapse="true" title="Robustness to response distribution choice"}
We also fit models using ordered beta distributions, which handle bounded responses differently:

![Left: ELPDs for all four ordered beta models. Right: ELPDs for truncated normal vs. ordered beta models on non-endpoint responses. Dotted lines indicate differences from discrete-factivity model.](plots/ordered_beta_elpd.pdf){#fig:ordered-beta-comparison}

The discrete-factivity model maintains its advantage regardless of response distribution, confirming the robustness of our findings.
:::

## Theoretical implications

The strong empirical support for discrete-factivity has important theoretical consequences:

### For semantic theory

The results support views where factivity involves categorical semantic properties. These might manifest as lexical ambiguity (predicates having distinct factive and non-factive entries), structural ambiguity (different syntactic structures triggering or not triggering presupposition), or pragmatic ambiguity (context determining whether speakers presuppose the complement). The key insight is that gradience emerges from uncertainty about which categorical option speakers adopt, not from gradient truth conditions.

### For projection theories

Even theories that derive projection from pragmatic principles must explain the discrete patterns. Whether projection is driven by Questions Under Discussion (with complements being categorically included or excluded), at-issueness (with content being categorically at-issue or not-at-issue), or speaker commitment (with speakers being discretely committed or uncommitted to the truth of the complement), the underlying mechanism operates discretely, not continuously.

### For experimental methodology

The success of model-based analysis highlights important methodological points. First, aggregate gradience does not imply individual gradience—population-level continuous patterns can emerge from individual-level discrete processes. Second, theory-driven models matter—the PDS framework maintains theoretical commitments while enabling quantitative tests. Finally, multiple paradigms strengthen conclusions—convergent results across experiments provide robust evidence for theoretical claims.

## Connections across phenomena

The contrast between factivity and adjectives is theoretically illuminating. Adjectives show gradient uncertainty from vagueness—an expected source of unresolved indeterminacy. In contrast, factivity shows discrete uncertainty from interpretation selection—resolved indeterminacy. This difference emerges naturally from the PDS compilation: vague adjectives yield continuous threshold parameters while factive predicates yield mixture model components. The framework thus maintains theoretical distinctions while enabling quantitative comparison.

## Summary

The discrete nature of factivity emerges clearly from our model comparison. Empirically, discrete-factivity models dramatically outperform gradient alternatives. Theoretically, factivity involves categorical interpretation selection rather than continuous meaning modulation—speakers either interpret predicates as triggering presupposition or not. Methodologically, the PDS framework enables principled theory comparison through compilation from compositional semantics to statistical models. These findings vindicate the traditional view of factivity as a discrete phenomenon while explaining gradient patterns through probabilistic interpretation selection. The framework developed here—from compositional semantics through PDS compilation to statistical models—provides a template for investigating similar questions across semantic phenomena.