---
title: "Probabilistic dynamic semantics"
bibliography: ../pds.bib
---

# Outline

The recent advent of linguistic datasets and their associated statistical models have given rise to two major kinds of questions bearing on linguistic theory and methodology:

* How can semanticists *use* such datasets?
  That is, how can the statistical properties of a dataset inform semantic theory directly, and what guiding principles regulate the link between such properties and semantic theory?
* How should semantic theories themselves be *modified* so that they may characterize not only informally collected acceptability and inference judgments, but statistical generalizations observed from datasets?

This course brings the compositional, algebraic view of meaning employed by semanticists into contact with linguistic datasets by introducing and applying the framework of Probabilistic Dynamic Semantics [@grove_factivity_2024;@grove_modeling_2025;@grove_probabilistic_2024].
PDS seamlessly integrates theories of semantic competence with accounts of linguistic behavior in experimental settings by taking a modular approach:
given a dataset involving some semantic phenomenon, and which exhibits certain statistical properties, this course offers an approach to developing both (a) *theories* of the meanings assigned to the expressions present in the dataset, and (b) *linking hypotheses* that directly relate these theories to linguistic behavior.

# Existing probabilistic approaches to meaning

The ideas developed in this course build on and respond to existing probabilistic approaches to semantics and pragmatics, including those which use computational modeling to characterize inference [@zeevat_bayesian_2015;@franke_probabilistic_2016;@brasoveanu_computational_2020;@bernardy_bayesian_2022;@goodman_probabilistic_2016].
Such models are motivated, in part, by the observation that linguistic inference tends to display substantial *gradience*, giving rise to quantitative patterns that traditional semantic theory has difficulty capturing.
Meanwhile, they often aim to explain Gricean linguistic behavior [@grice_logic_1975] by regarding humans as Bayesian reasoners.
Indeed, due to this emphasis on pragmatic principles, much modeling work blurs the semantics/pragmatics distinction, rendering the connection to traditional semantic theory somewhat opaque.

To take a paradigm case, models within the Rational Speech Act (RSA) framework consider human interpreters as inferring meanings for an utterance which maximize the utterance's utility relative to a set of possible alternative utterances [@frank_predicting_2012;@lassiter_vagueness_2011;@goodman_knowledge_2013;@goodman_pragmatic_2016;@lassiter_adjectival_2017;@degen_rational_2023].
Probabilistic models of linguistic inference, including RSA, tend to encode Bayesian principles of probabilistic update in terms of *Bayes' theorem*, which states that the posterior probability of an event given an observation is proportional to the prior probability of the event, multiplied by the likelihood of the observation given the event.
RSA models give an explicit operational interpretation to Bayes' theorem by assuming that prior distributions over inferences encode world knowledge, and that likelihoods represent the utility-maximizing behavior of a pragmatic speaker.

Despite their success in modeling a wide variety of semantic and pragmatic phenomena, probabilistic models of linguistic data remain largely divorced from semantic and pragmatic practice, both in theory and in implementation.
RSA models, for example, regard the semantic interpretations which humans pragmatically reason about as being provided by a *literal listener* that determines a distribution over inferences, given an utterance [@degen_rational_2023].
But aside from the constraint that the literal listener's posterior distribution is proportional to its prior distribution (i.e., that it acts as a filter), the semantic components of RSA models are generally designed by researchers on an *ad hoc* basis:
on the one hand, the space \(I\) of possible inferences must be decided by individual researchers in a way that depends on the task being modeled;
on the other hand, the relation \(⟦·⟧\) between utterances and inferences is often assumed without a justified connection to any semantic theory using, e.g., an explicit grammar fragment in the style of @montague_proper_1973.

# PDS as a bridge between probabilistic models and semantic theory

Given this background, this course introduces a novel approach to probabilistic meaning which integrates traditional Montague semantics, as well as ideas in compositional dynamic semantics, with probabilistic computational models in a completely seamless fashion.
The theoretical framework and methodology we introduce retain the beneficial features of both kinds of approach to meaning:
PDS may be used to construct probabilistic models of human inference data, and it is in principle compatible with existing probabilistic modeling paradigms such as RSA;
meanwhile, it seamlessly connects probabilistic models to compositional dynamic semantics in the Montagovian tradition by providing a setting to write full-fledged grammar fragments.

PDS additionally provides a theory of dynamic discourse update, integrating aspects of discourse such as the common ground, the question under discussion [@ginzburg_dynamics_1996;@roberts_information_2012;@farkas_reacting_2010], and uncertainty about lexical meaning.
Crucially, given a semantic theory of some discourse phenomenon couched with PDS, one may obtain a probabilistic model of some linguistic dataset, given a particular *response function* [@grove_factivity_2024;@grove_modeling_2025;@grove_probabilistic_2024].
We introduce PDS in the context empirical datasets studying factivity, gradable adjectives, and the question under discussion.
