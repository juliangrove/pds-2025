---
title: "Two case studies"
bibliography: ../../pds.bib
format:
  html:
    css: ../styles.css
    html-math-method: mathjax
    mathjax-config:
      loader: {load: ['[tex]/bussproofs','[tex]/bbox','[tex]/colorbox']}
      tex:
        packages: {'[+]': ['bussproofs','bbox','colorbox']}
---

::: {.hidden}
$$
\newcommand{\expr}[3]{\begin{array}{c}
#1 \\
\bbox[lightblue,5px]{#2}
\end{array} ⊢ #3}
\newcommand{\ct}[1]{\bbox[font-size: 0.8em]{\mathsf{#1}}}
\newcommand{\updct}[1]{\ct{upd\_#1}}
\newcommand{\abbr}[1]{\bbox[transform: scale(0.95)]{\mathtt{#1}}}
\newcommand{\pure}[1]{\bbox[border: 1px solid orange]{\bbox[border: 4px solid transparent]{#1}}}
\newcommand{\return}[1]{\bbox[border: 1px solid black]{\bbox[border: 4px solid transparent]{#1}}}
\def\P{\mathtt{P}}
\def\Q{\mathtt{Q}}
\def\True{\ct{T}}
\def\False{\ct{F}}
\def\ite{\ct{if\_then\_else}}
\def\Do{\abbr{do}}
$$
:::

To illustrate how PDS bridges formal semantics and experimental data, we'll examine two case studies that exemplify different aspects of the framework.

### Case Study 1: Gradable Adjectives

Vague predicates provide an ideal starting point because effectively everyone agrees we need to incorporate into our theories of their meanings something representation that drives gradience. Expressions like *tall*, *expensive*, and *old* lack sharp boundaries—there's no precise height at which someone becomes tall [@lakoff_hedges_1973; @sadock_truth_1977; @lasersohn_pragmatic_1999; @krifka_approximate_2007; @solt_vagueness_2015].

Formal semantic theories have long recognized this gradience. Degree-based approaches [@klein_semantics_1980; @bierwisch_semantics_1989; @kamp_two_1975; @kennedy_projecting_1999; @kennedy_scale_2005; @kennedy_vagueness_2007; @barker_dynamics_2002] analyze gradable adjectives as expressing relations to contextual thresholds:

- *tall* is true of $x$ if $\ct{height}(x) \geq d_\text{tall}$ (context)

The threshold $d_\text{tall}$ varies with context—what counts as tall for a basketball player differs from tall for a child. But even within a fixed context, speakers show gradient judgments about borderline cases.

This makes gradable adjectives ideal for demonstrating how PDS works. They allow us to show how the framework maintains the compositional degree-based analysis from formal semantics while adding probability distributions over thresholds to capture gradient judgments and how it can support modeling how context shifts these distributions and link threshold distributions to slider scale responses.
It also allows us to show how PDS is well-poisitioned to help us understand the additional complexity beyond this basic picture that recent experimental work reveals. Different adjective types show distinct patterns: relative adjectives like *tall* and *wide* show maximum gradience in their positive form, while absolute adjectives like *clean* and *dry* exhibit different threshold distributions altogether. Furthermore, the distinction between minimum and maximum standard adjectives reveals asymmetric patterns of imprecision that challenge simple threshold-based accounts. How do we account for these differences and relate them to judgment data?

Recent years have seen partial integration into computational models [@lassiter_context_2013; @qing_gradable_2014; @kao_nonliteral_2014; @lassiter_adjectival_2017; @bumford_rationalizing_2021]. We'll show that PDS allows us to synthesize and compare these different partial approaches.

### Case Study 2: Factivity

While vagueness involves expected gradience, factivity presents a puzzle. Traditional theory treats factivity as discrete—predicates either trigger presuppositions or they don't [@kiparsky_fact_1970; @karttunen_observations_1971].^[We'll spend a lot of time on Day 4 saying exactly what we mean by discrete here. @karttunen_observations_1971, of course, classically argues that there are predicates that sometimes trigger presuppositions and sometimes don't. For our purposes, we'll say that this behavior is discrete in the sense that it's more like ambiguity than vagueness. That is, we'll show that uncertainty around factivity displays the hallmarks of resolved uncertainty.] Yet experimental data reveals pervasive gradience.

A predicate is *factive* if it triggers inferences about its complement that project through entailment-canceling operators. []{#exm-love-positive} []{#exm-love-negative} []{#exm-love-question} *Love* appears factive because *Mo left* is inferrable from the standard family of sentences in (@exm-love-positive)–(@exm-love-question):

(@exm-love-positive) Jo loves that Mo left.
(@exm-love-negative) Jo doesn't love that Mo left.  
(@exm-love-question) Does Jo love that Mo left?

But when @white_role_2018 (discussed above) and @degen_are_2022 collected projection judgments at scale, they found continuous variation [@xue_correlation_2011; @smith_projection_2011; @djarv_prosodic_2017 also observe similar patterns]. Qualitatively, @degen_are_2022 argue that there is no clear line separates factive from non-factive predicates. Mean projection ratings vary continuously from *pretend* (lowest) to *be annoyed* (highest).

![Aggregate factivity measures from @degen_are_2022, showing continuous variation in projection ratings across predicates under questioning.](plots/projection_no_fact_means.pdf){width=750}

This gradience poses a theoretical challenge [@simons_observations_2007; @simons_what_2010; @simons_best_2017; @tonhauser_how_2018]. 

@kane_intensional_2022 later showed that this gradience is likely due largely to task effects and measurement noise. They demonstrate that when one applies a clustering model to these data that accounts for noise due to various factors, many of the standard subclasses of factives pop out. 

To get a sense for how these clusters appear in the data, we can look back at [this figure](understanding-gradience#fig-derived-factivity). Note that (i) there are clearly at least two separate "bumps" in the histogram; but (ii) the peak of the right bump is not at 1, as we might have expected. This is because some of these subclasses–e.g. the cognitive factives, which @karttunen_observations_1971 observes to not always give rise factivity–appear to themselves be associated with non-necessary factive inferences. 

In this case study, we'll focus on understanding what gives rise to *this* gradience. We'll take for granted that there are in fact discrete subclasses of factives, as @kane_intensional_2022 show, but that it remains an open question what kind of uncertainty drives the gradience internal to the subclasses. We'll consider two hypotheses that PDS allows us to state precisely and test against the data collected by @degen_prior_2021, which uses the same experimental paradigm as @degen_are_2022:

**The Fundamental Discreteness Hypothesis**: Factivity remains discrete; gradience reflects:
- Multiple predicate senses [see @spector_uniform_2015]
- Structural ambiguity affecting projection [@varlokosta_issues_1994; @giannakidou_polarity_1998; @giannakidou_affective_1999; @giannakidou_dependency_2009; @roussou_selecting_2010; @farudi_antisymmetric_2007; @abrusan_predicting_2011; @kastner_factivity_2015; @ozyildiz_attitude_2017]
- Contextual variation in whether or not complements are at-issue [@simons_best_2017; @roberts_preconditions_2024; @qing_rational_2016]

**The Fundamental Gradience Hypothesis**: No discrete factivity property exists. Gradient patterns reflect different degrees to which predicates support complement truth inferences–e.g. by viewing at-issueness as itself fundamentally continuous [@tonhauser_how_2018].

PDS allows us to implement both hypotheses formally and test their predictions against fine-grained response distributions—not just means, but entire judgment patterns including multimodality that might indicate mixture distributions. We'll show how this approach can be applied to judgment data aimed at capturing factivity using various experimental paradigms [@tonhauser_prosodic_2016; @djarv_prosodic_2017; @djarv_cognitive_2018; @white_role_2018; @white_lexicosyntactic_2018; @white_believing_2021; @degen_prior_2021; @degen_are_2022; @jeong_prosodically-conditioned_2021; @kane_intensional_2022].
