---
bibliography: ../../pds.bib
format:
  html:
    css: ../styles.css
---

### A theoretically-oriented approach: RSA models

Rational speech act (RSA) models are a very popular approach to modeling pragmatic inference that integrates ideas from formal semantics into mathematically explicit models of Gricean reasoning [@grice_logic_1975].
Crucially, these models aim for a certain kind of *modularity*:
they allow one to provide separate accounts of the literal semantics of expressions, on the one hand, and the inferences that people make when they encounter utterances of these expressions, on the other.
They achieve this kind of modularity, essentially, by allowing one to state a theory of literal meaning and then to use a systematic recipe for turning it into a theory of pragmatic inference.

Here we describe what is sometimes called *vanilla* RSA.
Vanilla RSA is RSA more or less as it was originally formulated @frank_predicting_2012 and @goodman_knowledge_2013 (see @degen_rational_2023 for a recent comprehensive overview of the RSA literature).
The basic idea is that there are two sets of models, *listener* models, and *speaker* models, which are kind of mirror images of each other.
In particular, any given listener model $L_{i}$ characterizes a probability distribution over possible worlds $w$, given some utterance $u$.
$$
\begin{aligned}
P_{L_0}(w | u) &= \frac{\begin{cases}
P_{L_0}(w) & ‚ü¶u‚üß^w = \mathtt{T} \\
0 & ‚ü¶u‚üß^w = \mathtt{F}
\end{cases}}{‚àë_{w^\prime}\begin{cases}
P_{L_0}(w^\prime) & ‚ü¶u‚üß^{w^\prime} = \mathtt{T} \\
0 & ‚ü¶u‚üß^{w^\prime} = \mathtt{F}
\end{cases}} \\[2mm]
P_{L_i}(w | u) &= \frac{P_{L_i}(u | w) * P_{L_i}(w)}{‚àë_{w^\prime}P_{L_i}(u | w^\prime) *
P_{L_i}(w^\prime)}\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,(i > 0) \\[2mm]
& = \frac{P_{L_i}(u | w) * P_{L_i}(w)}{P_{L_i}(u)}
\end{aligned}
$$
In words, $P_{L_0}(w | u)$ depends only on whether or not $u$ and $w$ are compatible.
It thus acts as a *filter*, eliminating possible worlds from the prior in which the utterance $u$ is false.

#### Listener models

The definition of $P_{L_i}(w | u)$ for $i > 0$ uses [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem).
To state that this definition *uses* Bayes' theorem is kind of a tautology when viewed simply as a mathematical description.
Thus what this statement really means is something more operational:
RSA models make distinguishing choices about the definitions of $P_{L_i}(u | w)$ and $P_{L_i}(w)$, and it is these latter
choices which are used, in turn, to compute $P_{L_i}(w | u)$.

In general, the choice $P_{L_i}(w)$ of a prior distribution over $w$ is made once and for all, regardless of the particular model, so we can just call this choice $P(w)$.
$P(w)$ can be seen to give a representation of the *context set* in a given discourse;
that is, the distribution over possible worlds known in common among the interlocutors, before anything is uttered.
	
#### Speaker models

The definition of $P_{L_i}(u | w)$, on the other hand, is chosen to reflect the model $S_i$ of the *speaker*, which brings us to the *other* set of
models.
Thus $P_{L_i}(u | w) = P_{S_i}(u | w)$, where
$$\begin{aligned}
P_{S_i}(u | w) &= \frac{e^{Œ± * ùïå_{S_i}(u; w)}}{‚àë_{u^\prime}e^{Œ± *
ùïå_{S_i}(u^\prime; w)}}
\end{aligned}$$
$ùïå_{S_i}(u; w)$ is the *utility* $S_i$ assigns to the utterance $u$, given its intention to communicate the world $w$. Utility for $S_i$ is typically defined as
$$ùïå_{S_i}(u; w) = ln(P_{L_{i-1}}(w | u)) - C(u)$$
that is, the natural log of the probability that $L_{i-1}$ assigns to $w$ (given $u$), minus $u$'s cost ($C(u)$).
$Œ±$ is known as the *temperature* (or the  *rationality parameter*) associated with $S_i$.
When $Œ± = 0$, $S_i$ chooses utterances randomly (from a uniform distribution), without attending to their utility in communicating $w$.
When $Œ±$ tends toward $‚àû$, $S_i$ becomes more and more deterministic in its choice of utterance, assigning more and more probability mass to the utterance that maximizes utility in communicating $w$.
A little more formally,
$$\lim_{Œ± ‚Üí ‚àû}\frac{e^{Œ± * ùïå_{S_i}(u; w)}}{‚àë_{u^\prime}e^{Œ± *
ùïå_{S_i}(u^\prime; w)}} = \begin{cases}
1 & u = \arg\max_{u^\prime}(ùïå_{S_i}(u^\prime; w)) \\
0 & u ‚â† \arg\max_{u^\prime}(ùïå_{S_i}(u^\prime; w))
\end{cases}$$
Because the cost $C(u)$ only depends on $u$, it is nice to view $e^{Œ± * ùïå_{S_i}(u; w)}$ as factored into a prior and (something like) a likelihood, so that $P_{S_i}(u | w)$ has a formulation symmetrical to that of $P_{L_i}(w | u)$ (when $i > 0$); that is, it can be formulated in the following way:
$$\begin{aligned}
e^{Œ± * ùïå_{S_i}(u; w)} &= P_{L_{i - 1}}(w | u)^Œ± * \frac{1}{e^{Œ± * C(u)}}
\\[2mm] &‚àù P_{S_i}(w | u) * P_{S_i}(u) \\[2mm]
&= P_{S_i}(w | u) * P(u)
\end{aligned}$$
In effect, we can define $P_{S_i}(w | u)$, viewed as a function of $u$, to be proportional to $P_{L_{i - 1}}(w | u)^Œ±$; meanwhile, we can define $P(u)$, the prior probability over utterances, to be proportional to $\frac{1}{e^{Œ± * C(u)}}$. (Note that if we ignore cost altogether, so that $C(u)$ is always, say, 0, then $P(u)$ just becomes a uniform distribution.)
	
Taking these points into consideration, we may reformulate our speaker model, $S_i$, as follows:
$$\begin{aligned}
P_{S_i}(u | w) &= \frac{P_{S_i}(w | u) * P(u)}{‚àë_{u^\prime}P_{S_i}(w |
u^\prime) * P(u^\prime)} \\[2mm]
&= \frac{P_{S_i}(w | u) * P(u)}{P_{S_i}(w)}
\end{aligned}$$
In words, the speaker model, just like the listener model, may be viewed operationally in terms of Bayes' theorem.
Note that $P_{S_i}(w)$, in general, defines a different distribution from $P(w)$, the listener's prior distribution over worlds (i.e., the context set).
The former represents, not prior knowledge about the *context*, but rather something more like the relative "communicability" of a given possible world, given the distribution $P(u)$ over utterances;
that is, how likely a random utterance makes $w$, though with the exponential $Œ±$ applied.
	

### An example

An example helps illustrate how the probability distributions determined by RSA speaker and listener models are computed in practice.
Let's say there are seven cookies, as depicted in the image below.

![Cookies (7 of them)](images/7cookies.jpg){width=300 fig-align="center"}

Further, say someone utters the sentence *Jo ate five cookies*.
We'll assume that the literal meaning of such a sentence is lower bounded:
it is true just in case the number cookies Jo ate is at least five, i.e., 
$$
n_{\textit{cookies}} ‚â• 5
$$
Let's now consider the probability distributions computed by the models $L_{0}$, $S_{1}$, and $L_{1}$, following the definitions given earlier.

#### The literal listener $L_{0}$

Recall that the literal listener is a *filter*:
$$
P_{L_{0}}(w ‚à£ u) ‚àù ùüô(w ‚â• n) √ó P (w)
$$
Let's also assume that $P(w)$, the *prior* distribution over the number of cookies Jo ate is uniform.
Then, the literal listener is simply zeroing out the portion of this prior distribution in which Jo ate *less than five* cookies and renormalizing the resulting distribution.
The following table illustrates this for *Jo ate five cookies*, as well as two other utterances.
Here, $w$ (the world) is identified with a possible inference;
i.e., about how many cookies Jo actually ate.

| $w =$ | 5 | 6  |7 |
| - | - | - | - |
| $u = \textit{Jo ate 5 cookies}$ | 1/3 | 1/3 | 1/3 |
| $u = \textit{Jo ate 6 cookies}$ | 0 | 1/2 | 1/2 |
| $u = \textit{Jo ate 7 cookies}$ | 0 | 0 | 1 |

Thus if *Jo ate five cookies* is uttered, $L_{0}$ assigns a probability of 1/3 to each of the possible inferences compatible with the utterance's lower-bounded literal meaning.

#### The pragmatic speaker $S_{1}$

Here is the pragmatic speaker model again, reformulated (i) as a proportionality statement, and (ii) by moving the cost term into the denominator:
$$
P_{S_{1}}(u ‚à£ w) ‚àù \frac{P_{L_{0}}(w ‚à£ u)^{Œ±}}{e^{Œ± √ó C(u)}}
$$
For the purposes of the example, let's assume that the rationality parameter $Œ± = 4$, and that the cost $C(u)$ of an utterance is constant across utterances.
Let's further assume that the speaker is only considering the utterances listed in the following table;
i.e., its prior distribution---given the constant cost function---is uniform over these alternatives.
Then, we  obtain the following distributions over *utterances* for three possible worlds corresponding to the inference which the speaker intends to communicate.

| $w =$ | 5 | 6  |7 |
| - | - | - | - |
| $u = \textit{Jo ate 5 cookies}$ | 1 | 0.16 | 0.01 |
| $u = \textit{Jo ate 6 cookies}$ | 0 | 0.84 | 0.06 |
| $u = \textit{Jo ate 7 cookies}$ | 0 | 0 | 0.93 |

Thus if $S_{1}$ wishes to convey that Jo ate exactly five cookies, it chooses the first utterance with a probability of 1.
This is because the literal listener assigns 5 cookies a probability of 0 if one of the other two stronger sentences is uttered.
Meanwhile, if it wishes to convey that Jo ate exactly six cookies, it chooses the first utterance with probability $\frac{(1/3)^4}{(1/3)^4 + (1/2)^4} ‚âà 0.16$ and the second utterance with probability $\frac{(1/2)^4}{(1/3)^4 + (1/2)^4} ‚âà 0.84$.
Crucially, we see that probabilities are normalized within *columns* of the table, rather than rows, as is the case for the listener models.

#### The pragmatic listener $L_{1}$

Finally, recall that the pragmatic listener model uses the pragmatic speaker model as a representation of the likelihood of some utterance, given an intended inference.
$$
P_{L_{1}}(w ‚à£ u) ‚àù P_{S_{1}}(u ‚à£ w) √ó P (w)
$$
Given that there is a uniform prior distribution over numbers of cookies, we may obtain probability distributions for the same three utterances by taking the table in the previous subsection and renormalizing its probabilities within rows.

| $w =$ | 5 | 6  |7 |
| - | - | - | - |
| $u = \textit{Jo ate 5 cookies}$ | 0.85 | 0.14 | 0.01 |
| $u = \textit{Jo ate 6 cookies}$ | 0 | 0.93 | 0.07 |
| $u = \textit{Jo ate 7 cookies}$ | 0 | 0 | 1 |

Note that if we had a non-uniform prior over numbers of cookies---e.g., if 6 is more probable than 5 (classic Jo)---we can simply multiply the entries of this table by their prior probabilities and renormalize them within rows once again.

### RSA discussion

As noted earlier, RSA models come with a very appealing feature:
that they provide a modular separation between semantic and pragmatic concerns.
In particular, the $L_{0}$ model can be seen as instantiating a semantic analysis of some utterance (which is, ideally, provided by some external theory of the semantics of utterances), while the $L_{1}$ model can be seen as instantiating a pragmatic theory that is built up from the semantic theory in a fairly deterministic way (once, e.g., cost parameters, rationality parameters, and prior distributions over utterance alternatives are fixed).
Indeed, such a separation can be methodologically useful, since it allows one to *test* particular semantic theories in the face of human inference data that arises from pragmatic (as well as other) factors (see, e.g., @waldon_modeling_2020 for discussion)

#### Challenges

There is a particular set of challenges for RSA models, as they are typically stated, that we aim to address in this course.
Namely, it is not super obvious what role the Montagovian notion of semantic compositionality can play.
Note that the account of the literal listener $L_{0}$ must come "from outside":
RSA models are typically defined on top of analyses of sentence meaning, as opposed to the meanings of basic expressions, though the latter are presumably implicated in deriving the former.
Thus there are certain questions about semantic compositionality which such models don't address:

- How may the semantics of individual expressions be studied in tandem with their pragmatic effects? How should such pragmatic effects be formally encoded in lexical meaning representations?
- How do pragmatic effects *compose*, in order to yield the global pragmatic effects associated with entire utterances? 

One of the aims of this course is to provide a framework in which the pragmatic effects of individual expressions may be stated and composed, and then tested against human inference data.

### Setting the stage

To round out this set of notes, we give a brief overview of the main desiderata which we aim to have PDS satisfy.
These are the following:

- [compositionality]{.gb-orange}: models of inference should be derived compositionally from semantic grammar fragments.
- [modularity]{.gb-orange}: factors affecting inference judgments should be theorized about independently and combined.
- [abstraction]{.gb-orange}: models of meaning and inference should be statable *abstractly*, without reference to implementation.

We say a little more about these here, in turn.

#### Compositionality for models

What could it mean for models of linguistic inference (e.g., as represented in a judgment dataset) to be *compositional*?
Our basic basic strategy is to build the distributional assumptions associated with, e.g., a mixed-effects model, *into the semantics*.
This way, when basic meanings compose, so do these distributional assumptions.

Indeed, it is reasonable to ask what such distributional assumptions represent.
Our answer is *uncertainty*;
specifically, uncertainty about which particular inferences are licensed on any given occasion of language use.
Importantly, these kinds of assumptions may be combined when determining the meanings of complex expressions from the meanings of the more basic expressions they contain. 
Thus while the meaning of a sentence such as *Jo laughs* might be determined compositionally as 

$$
‚ü¶\textit{jo laughs}‚üß = ‚ü¶\textit{laughs}‚üß ‚ñπ ‚ü¶\textit{jo}‚üß = laughs(j)
$$

within a traditional semantic framework, PDS, instead, compositionally associates this sentence with a *probability distribution*.

$$‚ü¶\textit{jo laughs}‚üß = ‚ü¶\textit{laughs}‚üß ‚ñπ ‚ü¶\textit{jo}‚üß = \begin{array}[t]{l}
j ‚àº JoDistr \\
laugh ‚àº LaughDistr \\
Return (laugh(j))
\end{array}
$$

This distribution is a distribution over truth values:
it assigns some probability $p$ to True and $1 - p$ to False.
Moreover, it is determined by certain *sampling statements* (whose interpretations we will formally define tomorrow).
Informally, the meaning of *Jo* takes some distribution over entities, while the meaning of *laughs* takes some distribution over functions from entities to truth values.
These distributions may then be *combined* to yield a distribution over values gotten by applying such functions applied to such entities; i.e., a distribution over truth values.
Remarkably, the distributions over the meanings of such basic expressions end up corresponding exactly, within PDS, to the parameters of some hierarchical Bayesian (e.g., mixed-effects) model which may be used to fit human inference judgment data.

#### Modularity

We also want theories constructed within PDS to be modular.
Specifically, we want the *factors affecting inference* to be able to be theorized about independently and combined.
These include:

- lexical and compositional semantics
- world knowledge
- response behavior: how does someone use a testing instrument (e.g., slider scale)?

An upshot of this feature is that PDS can have different uses.
For example, one could swap out a model of response behavior for a model of likely utterances (perhaps, $S_{1}$).

#### Abstraction

Finally, we want such theories to display a certain amount of abstraction.
That is, we should be able to state models of inference judgment data that:

- *describe* probability distributions,
- do not concern themselves with how distributions are computed.

There are a couple useful consequences of this feature.
First, it allows traditional semantic theories to be plugged into PDS rather seamlessly.
Second, it allows separation between theories stated within PDS and model stated within those thoeries.
This second consequence allows:

- Allows flexibility about implementation.
- Allows the theory to be simpler.
- Allows seamless integration between formal semantics and probabilistic semantics. (More tomorrow!)

### References
