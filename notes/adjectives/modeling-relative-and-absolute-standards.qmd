---
title: "Modeling relative and absolute standards"
bibliography: ../../pds.bib
format:
  html:
    css: ../styles.css
    html-math-method: mathjax
    mathjax-config:
      loader: {load: ['[tex]/bussproofs','[tex]/bbox','[tex]/colorbox']}
      tex:
        packages: {'[+]': ['bussproofs','bbox','colorbox']}
---

::: {.hidden}
$$
\newcommand{\expr}[3]{\begin{array}{c}
#1 \\
\bbox[lightblue,5px]{#2}
\end{array} ⊢ #3}
\newcommand{\ct}[1]{\bbox[font-size: 0.8em]{\mathsf{#1}}}
\newcommand{\updct}[1]{\ct{upd\_#1}}
\newcommand{\abbr}[1]{\bbox[transform: scale(0.95)]{\mathtt{#1}}}
\newcommand{\pure}[1]{\bbox[border: 1px solid orange]{\bbox[border: 4px solid transparent]{#1}}}
\newcommand{\return}[1]{\bbox[border: 1px solid black]{\bbox[border: 4px solid transparent]{#1}}}
\def\P{\mathtt{P}}
\def\Q{\mathtt{Q}}
\def\True{\ct{T}}
\def\False{\ct{F}}
\def\ite{\ct{if\_then\_else}}
\def\Do{\abbr{do}}
$$
:::

Our most sophisticated model implements the theoretical distinction between relative and absolute gradable adjectives. Some adjectives like *tall* use relative standards that shift with the comparison class, while others like *full* use absolute standards tied to scale endpoints.

#### Theoretical motivation

@kennedy_scale_2005 argues that gradable adjectives fall into two classes:

- **Relative adjectives** (tall, expensive): Standards shift dramatically with context
- **Absolute adjectives** (full, empty): Standards are anchored to scale endpoints

This distinction has been supported experimentally by @syrett_meaning_2010. Our mixture model allows the data to reveal which interpretation strategy participants use.

While PDS doesn't currently output mixture models directly, this represents a natural extension where semantic theory guides statistical model structure. The mixture implements the idea that the relative/absolute distinction might not be categorical but graded—a hypothesis that emerges naturally from the compositional framework.

### Encoding relative and absolute standards in PDS

The theoretical distinction between relative and absolute adjectives can be encoded directly in their lexical entries. Following the implementation in [`Grammar.Lexica.SynSem.Adjectives`](https://juliangrove.github.io/pds/Grammar-Lexica-SynSem-Adjectives.html), we can represent these two classes with different semantic structures.

For relative adjectives, the degree depends on the context item:

```haskell
-- Relative adjective: degree varies by comparison class
"tall" -> [ SynSem {
    syn = AP,
    sem = ty tau (lam s (purePP (lam x (lam i 
      (sCon "(≥)" @@ (sCon "height" @@ i @@ x) @@ 
       (sCon "d_tall" @@ s @@ (sCon "context" @@ i)))))) @@ s))
} ]
```

For absolute adjectives, the degree is fixed:

```haskell
-- Absolute adjective: degree anchored to scale endpoint
"full" -> [ SynSem {
    syn = AP,
    sem = ty tau (lam s (purePP (lam x (lam i 
      (sCon "(≥)" @@ (sCon "fullness" @@ i @@ x) @@ 
       (sCon "d_full" @@ s))))) @@ s))
} ]
```

The key difference lies in whether the threshold extractor takes a context argument. This semantic distinction motivates the mixture model structure—each response could result from either interpretation strategy.

### Delta rules for relative vs absolute interpretations

The delta rules must handle both types differently:

```haskell
-- | Context-dependent thresholds (relative)
contextual :: DeltaRule
contextual = \case
  D_tall ctx (UpdD_tall f s) -> Just (f @@ ctx)
  D_tall ctx s               -> Just (lookupD_tall s ctx)
  _                          -> Nothing

-- | Fixed thresholds (absolute)  
fixed :: DeltaRule
fixed = \case
  D_full (UpdD_full d s) -> Just d
  D_full s               -> Just (lookupD_full s)
  _                      -> Nothing
```

These different extraction patterns lead to different dependencies in the statistical model:

1. **Relative interpretation**: Degree varies with context
   - Basketball players: $\ct{d\_tall}(s, \ct{basketball}) = 0.9$
   - Children: $\ct{d\_tall}(s, \ct{children}) = 0.3$

2. **Absolute interpretation**: Degree is constant
   - All contexts: $\ct{d\_full}(s) = 0.95$

### From discrete semantics to gradient behavior

While the semantic distinction is categorical, the observed behavior is gradient. The mixture model captures this through uncertainty about which interpretation is active:

$$
P(\text{response} | \text{data}) = \pi \cdot P(\text{response} | \text{relative}) + (1-\pi) \cdot P(\text{response} | \text{absolute})
$$

where $\pi$ represents the probability of using the relative interpretation.

This compilation strategy shows how discrete theoretical distinctions can generate gradient empirical patterns—a key idea in the PDS framework.

::: {.callout-note title="Hypothetical PDS Compilation"}
While PDS doesn't currently output mixture models directly, the compilation would follow this pattern:

**Relative component:**
```haskell
relativeKernel = do
  d_r <- lookupDegree item  -- item-specific degree
  threshold <- lookupThreshold adjective context
  return (d_r >= threshold)
```

**Absolute component:**
```haskell  
absoluteKernel = do
  d_a <- lookupDegree adjective  -- adjective-specific degree
  threshold <- lookupThreshold adjective
  return (d_a >= threshold)
```

**Mixture:**
```haskell
mixtureKernel = do
  which <- bernoulli pi
  if which 
    then relativeKernel
    else absoluteKernel
```

This would compile to the Stan mixture model shown in the full implementation.
:::

#### Complete Stan code for the mixture model

```stan {.line-numbers highlight="45-47,59-61"}
data {
  // Same basic structure as Model 2
  int<lower=1> N_item;
  int<lower=1> N_adjective;
  int<lower=1> N_participant;
  int<lower=1> N_data;
  int<lower=1> N_0;
  int<lower=1> N_1;
  
  vector<lower=0, upper=1>[N_data] y;
  array[N_item] int<lower=1, upper=N_adjective> item_adj;
  
  // Indexing arrays (same structure as Model 2)
  array[N_data] int<lower=1, upper=N_item> item;
  array[N_0] int<lower=1, upper=N_item> item_0;
  array[N_1] int<lower=1, upper=N_item> item_1;
  array[N_data] int<lower=1, upper=N_adjective> adjective;
  array[N_0] int<lower=1, upper=N_adjective> adjective_0;
  array[N_1] int<lower=1, upper=N_adjective> adjective_1;
  array[N_data] int<lower=1, upper=N_participant> participant;
  array[N_0] int<lower=1, upper=N_participant> participant_0;
  array[N_1] int<lower=1, upper=N_participant> participant_1;
}

parameters {
  // MIXTURE COMPONENT (semantic theory: two interpretation strategies)
  
  // Global probability of using relative (vs absolute) interpretation
  real<lower=0, upper=1> which;
  
  // TWO SETS OF DEGREE PARAMETERS (PDS kernel structure)
  
  // Absolute interpretation: degrees are fixed per adjective
  vector<lower=0, upper=1>[N_adjective] d_a;
  
  // Relative interpretation: degrees vary by item (adjective × context)
  vector<lower=0, upper=1>[N_item] d_r;
  
  // THRESHOLD PARAMETERS (same as Model 2)
  
  // Global vagueness
  real<lower=0> sigma_guess;
  
  // Context sensitivity per adjective
  vector<lower=0>[N_adjective] spread;
  
  // PARTICIPANT EFFECTS
  real<lower=0> sigma_epsilon_mu_guess;
  vector[N_participant] z_epsilon_mu_guess;
  
  // RESPONSE NOISE
  real<lower=0, upper=1> sigma_e;
  
  // CENSORED DATA
  array[N_0] real<upper=0> y_0;
  array[N_1] real<lower=1> y_1;
}

transformed parameters {
  // Participant effects (same as Model 2)
  vector[N_participant] epsilon_mu_guess = sigma_epsilon_mu_guess * z_epsilon_mu_guess;
  
  // MIXTURE WEIGHTS ON LOG SCALE
  // This prevents numerical underflow when probabilities get very small
  vector[2] log_which;
  log_which[1] = log(which);      // log P(relative)
  log_which[2] = log1m(which);    // log P(absolute) = log(1 - which)
  
  // BASE THRESHOLDS (same structure as Model 2)
  vector[N_item] mu_guess0;
  for (i in 0:(N_adjective-1)) {
    mu_guess0[3 * i + 1] = spread[i + 1];   // High condition
    mu_guess0[3 * i + 2] = -spread[i + 1];  // Low condition
    mu_guess0[3 * i + 3] = 0;                // Mid condition
  }
  
  // COMPUTE PREDICTIONS UNDER BOTH INTERPRETATIONS
  
  // Thresholds on probability scale
  vector<lower=0, upper=1>[N_data] mu_guess;
  vector<lower=0, upper=1>[N_0] mu_guess_0;
  vector<lower=0, upper=1>[N_1] mu_guess_1;
  
  // Predictions under relative interpretation
  vector<lower=0, upper=1>[N_data] response_rel;
  vector<lower=0, upper=1>[N_0] response_rel_0;
  vector<lower=0, upper=1>[N_1] response_rel_1;
  
  // Predictions under absolute interpretation
  vector<lower=0, upper=1>[N_data] response_abs;
  vector<lower=0, upper=1>[N_0] response_abs_0;
  vector<lower=0, upper=1>[N_1] response_abs_1;
  
  // For each response
  for (i in 1:N_data) {
    // Threshold (same for both interpretations)
    mu_guess[i] = inv_logit(mu_guess0[item[i]] + epsilon_mu_guess[participant[i]]);
    
    // RELATIVE: degree varies by item
    // "tall for a basketball player" vs "tall for a jockey"
    response_rel[i] = 1 - normal_cdf(d_r[item[i]] | mu_guess[i], sigma_guess);
    
    // ABSOLUTE: degree fixed per adjective
    // "tall" has same degree regardless of context
    response_abs[i] = 1 - normal_cdf(d_a[adjective[i]] | mu_guess[i], sigma_guess);
  }
  
  // Repeat for censored data at 0
  for (i in 1:N_0) {
    mu_guess_0[i] = inv_logit(mu_guess0[item_0[i]] + epsilon_mu_guess[participant_0[i]]);
    response_rel_0[i] = 1 - normal_cdf(d_r[item_0[i]] | mu_guess_0[i], sigma_guess);
    response_abs_0[i] = 1 - normal_cdf(d_a[adjective_0[i]] | mu_guess_0[i], sigma_guess);
  }
  
  // Repeat for censored data at 1
  for (i in 1:N_1) {
    mu_guess_1[i] = inv_logit(mu_guess0[item_1[i]] + epsilon_mu_guess[participant_1[i]]);
    response_rel_1[i] = 1 - normal_cdf(d_r[item_1[i]] | mu_guess_1[i], sigma_guess);
    response_abs_1[i] = 1 - normal_cdf(d_a[adjective_1[i]] | mu_guess_1[i], sigma_guess);
  }
}

model {
  // PRIORS
  
  // Vagueness and context effects
  sigma_guess ~ exponential(5);
  spread ~ exponential(1);
  
  // Participant variation
  sigma_epsilon_mu_guess ~ exponential(1);
  z_epsilon_mu_guess ~ std_normal();
  
  // MIXTURE LIKELIHOOD (PDS kernel structure)
  
  // For each observed response
  for (i in 1:N_data) {
    // Log probability under relative interpretation
    real lps_r = log_which[1] + normal_lpdf(y[i] | response_rel[i], sigma_e);
    
    // Log probability under absolute interpretation
    real lps_a = log_which[2] + normal_lpdf(y[i] | response_abs[i], sigma_e);
    
    // Add log of sum of probabilities (marginalizing over interpretations)
    // log_sum_exp(a, b) = log(exp(a) + exp(b)) but numerically stable
    target += log_sum_exp(lps_r, lps_a);
  }
  
  // Same for censored at 0
  for (i in 1:N_0) {
    real lps_r = log_which[1] + normal_lpdf(y_0[i] | response_rel_0[i], sigma_e);
    real lps_a = log_which[2] + normal_lpdf(y_0[i] | response_abs_0[i], sigma_e);
    target += log_sum_exp(lps_r, lps_a);
  }
  
  // Same for censored at 1
  for (i in 1:N_1) {
    real lps_r = log_which[1] + normal_lpdf(y_1[i] | response_rel_1[i], sigma_e);
    real lps_a = log_which[2] + normal_lpdf(y_1[i] | response_abs_1[i], sigma_e);
    target += log_sum_exp(lps_r, lps_a);
  }
}

generated quantities {
  // OVERALL LOG-LIKELIHOOD
  vector[N_data] ll;
  
  // COMPONENT LOG-LIKELIHOODS
  vector[N_data] ll_r;  // relative component
  vector[N_data] ll_a;  // absolute component
  
  // POSTERIOR PROBABILITY OF EACH INTERPRETATION
  vector[N_data] prob_relative;
  
  for (i in 1:N_data) {
    // Component likelihoods
    ll_r[i] = log_which[1] + normal_lpdf(y[i] | response_rel[i], sigma_e);
    ll_a[i] = log_which[2] + normal_lpdf(y[i] | response_abs[i], sigma_e);
    
    // Overall likelihood
    ll[i] = log_sum_exp(ll_r[i], ll_a[i]);
    
    // Posterior probability this response used relative interpretation
    prob_relative[i] = exp(ll_r[i] - ll[i]);
  }
  
  // We could also compute:
  // - Which adjectives are more likely relative vs absolute
  // - Individual differences in interpretation preference
  // - Predicted classifications for new adjectives
}
```

#### How the mixture model works: A concrete example

Let's trace through how this model handles the adjective "tall" in different conditions:

1. **Under relative interpretation**:
   - "tall" + "basketball player" (high condition): `d_r["tall_high"] = 0.9`
   - "tall" + "average person" (mid condition): `d_r["tall_mid"] = 0.6`
   - "tall" + "child" (low condition): `d_r["tall_low"] = 0.3`
   
   The degree changes with context—what counts as tall depends on the comparison class, exactly as the discourse state mechanism predicts.

2. **Under absolute interpretation**:
   - All conditions: `d_a["tall"] = 0.7`
   
   The degree is fixed—"tall" means the same thing regardless of context.

3. **Threshold computation** (same for both):
   - High condition: threshold shifts up (`spread["tall"]` added)
   - Low condition: threshold shifts down (`spread["tall"]` subtracted)
   - But the shift might be small if "tall" is absolute-like

4. **Mixture computation**:
   ```
   P(response | data) = which × P(response | relative) + (1-which) × P(response | absolute)
   ```
   
   If `which = 0.8`, then 80% of responses come from relative interpretation.

5. **What we learn**:
   - Global `which`: Overall tendency toward relative vs absolute interpretation
   - `spread` values: Which adjectives show more context sensitivity
   - Generated `prob_relative`: Which specific responses likely used which interpretation

#### Key innovations of the mixture model

1. **Theory-neutral**: The model doesn't assume which adjectives are relative or absolute—it learns from data
2. **Graded membership**: Adjectives can be partially relative/absolute rather than categorical
3. **Individual differences**: Different participants might use different interpretation strategies
4. **Response-level inference**: We can probabilistically assess which specific responses reflect which interpretation

This mixture approach implements the idea that the relative/absolute distinction might not be categorical but graded, with some adjectives showing mixed behavior [@kennedy_scale_2005]. It demonstrates how semantic theory can guide statistical model structure even when PDS doesn't yet output such complex models directly.

## Conclusion: From theory to implementation

Through these three models, we've seen how PDS translates abstract semantic theories into concrete statistical models:

1. **Model 1 (Norming)**: Established the basic Stan architecture and censoring approach for degree questions
2. **Model 2 (Vagueness)**: Showed how degree-based semantics with likelihood judgments becomes probabilistic computation
3. **Model 3 (Mixture)**: Demonstrated how theoretical distinctions become statistical hypotheses

The progression from PDS's kernel models to full Stan implementations encodes several key ideas:

- **Kernel models capture semantics**: The highlighted portions of our Stan code directly implement semantic computations from compositional theory
- **Augmentations handle reality**: Random effects, hierarchical structure, and measurement noise aren't semantic but are necessary for real data
- **Theory guides implementation**: Different theoretical commitments (discrete vs. gradient, relative vs. absolute) lead to different model structures
- **Compilation is theoretical**: The choices made in translating λ-terms to Stan embody linking hypotheses about semantic processing