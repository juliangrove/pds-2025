---
bibliography: ../../pds.bib
format:
  html:
    css: ../styles.css
filters:
  - line-highlight
---

# From PDS to Stan: Implementing models of gradable adjectives

Having seen how gradable adjectives are represented in PDS's compositional semantics, we now turn to the practical challenge of implementing statistical models that test these semantic theories against experimental data. The translation from abstract semantic theory to concrete statistical models requires careful attention to how theoretical commitments manifest as computational procedures. This section demonstrates this translation through three increasingly sophisticated models of gradable adjectives.

## PDS outputs and kernel models

Before diving into the implementation details, it's important to understand what PDS produces and how it relates to the full statistical models we'll develop. PDS outputs what we term a **kernel model**—the semantic core that corresponds directly to the lexical and compositional semantics. This kernel can in principle be augmented with random effects, hierarchical priors, and other statistical machinery within the Haskell code itself, but the current implementation focuses on producing just the semantic kernel. 

When we present Stan models in these notes, highlighted lines show the kernel model output by PDS (the semantic core), while non-highlighted lines show statistical augmentations added by the analyst (random effects, priors, etc.).

This distinction is crucial: PDS automates the translation from compositional semantics to the core statistical model, while leaving room for analysts to add domain-specific statistical structure. As we'll see, this separation allows us to maintain theoretical clarity while building models sophisticated enough to handle real experimental data.

We'll be showing prettified versions of what the system actually outputs. For every model block, we'll also provide the current system output in a footnote.

## Stan: A language for statistical modeling

The translation from abstract semantic theory to concrete statistical models requires a computational intermediary—a language that can express both the probabilistic structures we theorize and the inference procedures we need to learn from data. Stan has emerged as the de facto standard for this role in computational semantics and psycholinguistics [@burkner_brms_2017;@stan_development_team_stan_2024].

Stan is a probabilistic programming language designed for statistical inference. Unlike general-purpose programming languages, Stan is specialized for defining probability distributions and performing Bayesian inference. When we write a Stan program, we're not writing procedures to execute but rather declaring the structure of a probability model. Stan then uses sophisticated algorithms (Hamiltonian Monte Carlo) to sample from the posterior distribution of our model parameters given the observed data.

This approach aligns perfectly with our semantic goals: just as formal semantics declares truth conditions rather than computational procedures, Stan declares probabilistic relationships rather than sampling algorithms. The parallel is not accidental—both frameworks separate the "what" (semantic content, statistical structure) from the "how" (compositional computation, inference algorithms).

## Case study: Modeling gradable adjectives

To understand how Stan works and how we can use it to test semantic theories, let's work through three models of increasing complexity. We'll start with a baseline norming model that will serve as our introduction to Stan's structure and syntax. Each model will demonstrate how PDS translates semantic theory into statistical kernels, which we then augment for real-world data analysis.

## The PDS implementation: Gradable adjectives

Before diving into Stan code, let's examine how gradable adjectives are represented in PDS. From the Haskell codebase ([`Grammar.Lexica.SynSem.Adjectives`](https://juliangrove.github.io/pds/Grammar-Lexica-SynSem-Adjectives.html)), here's the lexical entry for "tall":

```haskell
"tall" -> [ SynSem {
    syn = AP,
    sem = ty tau (lam s (purePP (lam x (lam i 
      (sCon "(≥)" @@ (sCon "height" @@ i @@ x) @@ 
       (sCon "d_tall" @@ s))))) @@ s))
} ]
```

This Haskell implementation directly encodes the formal semantics $⟦\text{tall}⟧$ presented in the previous section. The key components map as follows:
- `sCon "height"` implements the measure function
- `sCon "d_tall"` extracts the contextual threshold from the discourse state
- `sCon "(≥)"` implements the comparison relation
- The lambda abstractions mirror the semantic type $(e → (i → t))$

This lexical entry encodes several key theoretical insights:

1. **Syntactic type**: `AP` indicates this is an adjective phrase
2. **Semantic computation**: The meaning is a function that:
   - Takes a discourse state `s` (containing threshold information)
   - Returns a function from entities `x` to propositions (functions from indices `i` to truth values)
   - The proposition is true when the entity's height exceeds the contextual threshold

3. **Semantic components**:
   - $\ct{height}$: A function from indices to entity-to-degree mappings (type: $\iota \to e \to r$)
   - $\ct{d\_tall}$: Extracts the threshold for "tall" from the discourse state (type: $\sigma \to r$)
   - $\ct{(≥)}$: Comparison operator (type: $r \to r \to t$)

This implements @kennedy_vagueness_2007's degree-based semantics, where gradable adjectives denote relations between degrees and contextually determined thresholds. The use of the discourse state for threshold storage captures the context-sensitivity of standards.

### Model 1: Baseline norming

Let's start with the simplest possible model for understanding how participants judge items on gradable scales. This model will serve as our foundation for understanding Stan's structure and syntax, while also showing how PDS translates semantic theory into statistical code. We'll build it up block by block, explaining every line.

#### Understanding the experimental setup

Before diving into the Stan code, let's understand what data we're modeling. In a norming experiment, participants see items paired with adjectives and conditions, then provide slider responses from 0 to 1. Here's a sample of the data:

```csv
"participant","item","item_number","adjective","adjective_number","condition","condition_number","scale_type","scale_type_number","response"
1,"closed_mid",6,"closed",2,"mid",3,"absolute",1,0.66
1,"old_mid",24,"old",8,"mid",3,"relative",2,0.51
1,"expensive_mid",15,"expensive",5,"mid",3,"relative",2,0.62
1,"full_high",16,"full",6,"high",1,"absolute",1,1
1,"deep_low",8,"deep",3,"low",2,"relative",2,0.22
```

Each row represents one judgment:
- `participant`: Which person made this judgment (participant 1, 2, etc.)
- `item`: A unique identifier combining adjective and condition (e.g., "tall_high")
- `item_number`: Numeric ID for the item (used in Stan)
- `adjective`: The gradable adjective being tested
- `condition`: Whether this is a high/mid/low standard context
- `response`: The participant's slider response (0 = "definitely not X", 1 = "definitely X")

Our simplest model asks: What degree does each item have on its scale, and how do participants map these degrees to slider responses? This connects directly to the semantic theory—we're measuring the degrees that our formal analysis posits.

#### The structure of a Stan program

Every Stan program follows a particular architecture with blocks that appear in a specific order. Each block serves a specific purpose in defining our statistical model. Let's build up our norming model block by block to understand how Stan works. This structure parallels the modular architecture of PDS itself—each block handles a distinct aspect of the modeling problem.

#### The data block: Declaring what we observe

Every Stan program begins with a `data` block that tells Stan what information will be provided from the outside world—our experimental observations. Let's build this up piece by piece:

```stan
data {
  int<lower=1> N_item;        // number of items
  int<lower=1> N_participant; // number of participants  
  int<lower=1> N_data;        // number of data points
```

These first lines declare basic counts. The syntax breaks down as:
- `int`: This will be an integer (whole number)
- `<lower=1>`: This integer must be at least 1 (no negative counts!)
- `N_item`: The variable name (we'll use this throughout our program)
- `// number of items`: A comment explaining what this represents

Why do we need these constraints? Stan uses them to:
1. Catch data errors early (if we accidentally pass 0 items, Stan will complain)
2. Optimize its algorithms (knowing bounds helps the sampler work efficiently)

Next, we handle a subtle but important issue—boundary responses:

```stan
  int<lower=1> N_0;           // number of 0s
  int<lower=1> N_1;           // number of 1s
```

Why separate these out? Slider responses of exactly 0 or 1 are "censored"—they might represent even more extreme judgments that the scale can't capture. We'll handle these specially, implementing insights from measurement theory.

Now for the actual response data:

```stan
  vector<lower=0, upper=1>[N_data] y; // response in (0, 1)
```

This declares a vector (like an array) of length `N_data`, where each element must be between 0 and 1. Notice this is for responses *between* 0 and 1, not including the boundaries.

Finally, we need to map responses to items and participants:

```stan
  array[N_data] int<lower=1, upper=N_item> item;        // which item for each response
  array[N_0] int<lower=1, upper=N_item> item_0;         // which item for each 0
  array[N_1] int<lower=1, upper=N_item> item_1;         // which item for each 1
  array[N_data] int<lower=1, upper=N_participant> participant;     
  array[N_0] int<lower=1, upper=N_participant> participant_0;
  array[N_1] int<lower=1, upper=N_participant> participant_1;
}
```

These arrays work like lookup tables. If `item[5] = 3`, then the 5th response in our data was about item #3. This indexing structure connects our flat data file to the hierarchical structure of our experiment.

Looking back at our CSV data, when Stan reads it, it will:
1. Count unique items → `N_item` (e.g., 36 if we have 12 adjectives × 3 conditions)
2. Count unique participants → `N_participant` 
3. Extract all responses between 0 and 1 → `y` vector
4. Build index arrays mapping each response to its item and participant

#### The parameters block: What we want to learn

After declaring our data, we declare the parameters—the unknown quantities we want to infer. This is where semantic theory meets statistical inference:

```stan
parameters {
  // Fixed effects
  vector[N_item] mu_guess;
```

This declares a vector of "guesses" (degrees) for each item. Why `mu_guess`? In statistics, μ (mu) traditionally denotes a mean or central tendency. These represent our best guess about each item's true degree on its scale—the theoretical degrees that the semantic analysis posits.

But people differ! We need random effects to capture individual variation:

```stan
  // Random effects
  real<lower=0> sigma_epsilon_guess;     // how much people vary
  vector[N_participant] z_epsilon_guess; // each person's deviation
```

This uses a clever trick called "non-centered parameterization":
- `sigma_epsilon_guess`: The overall amount of person-to-person variation
- `z_epsilon_guess`: Standardized (z-score) deviations for each person

We'll combine these later to get each person's actual adjustment. Why not just use `vector[N_participant] epsilon_guess` directly? This separation often helps Stan's algorithms converge much faster—a practical consideration that doesn't affect the semantic theory but matters for implementation.

Next, measurement noise:

```stan
  real<lower=0,upper=1> sigma_e;  // response variability
```

Even if two people agree on an item's degree, their slider responses might differ slightly. This parameter captures that noise.

Finally, those boundary responses:

```stan
  // Censored data
  array[N_0] real<upper=0> y_0;  // true values for observed 0s
  array[N_1] real<lower=1> y_1;  // true values for observed 1s
}
```

This is subtle but important. When someone gives a 0 response, their "true" judgment might be -0.1 or -0.5—we just can't see below 0. These parameters let Stan infer what those true values might have been @stevens_psychophysics_1975.

#### The transformed parameters block: Building predictions

Now we combine our basic parameters to build what we actually need. This block serves as a bridge between abstract parameters and concrete predictions:

```stan
transformed parameters {
  vector[N_participant] epsilon_guess;
  vector[N_data] guess;
  vector[N_0] guess_0;
  vector[N_1] guess_1;
```

First, we convert those z-scores to actual participant adjustments:

```stan
  // Non-centered parameterization
  epsilon_guess = sigma_epsilon_guess * z_epsilon_guess;
```

If `sigma_epsilon_guess = 0.2` and participant 3 has `z_epsilon_guess[3] = 1.5`, then participant 3 tends to give responses 0.3 units higher than average.

Now we can compute predicted responses:

```stan
  for (i in 1:N_data) {
    guess[i] = mu_guess[item[i]] + epsilon_guess[participant[i]];
  }
```

Let's trace through one prediction:
- Response i is about item 5 by participant 3
- `item[i] = 5`, so we look up `mu_guess[5]` (say it's 0.7)
- `participant[i] = 3`, so we add `epsilon_guess[3]` (say it's 0.1)
- `guess[i] = 0.7 + 0.1 = 0.8`

We repeat this for the boundary responses:

```stan
  for (i in 1:N_0) {
    guess_0[i] = mu_guess[item_0[i]] + epsilon_guess[participant_0[i]];
  }
  
  for (i in 1:N_1) {
    guess_1[i] = mu_guess[item_1[i]] + epsilon_guess[participant_1[i]];
  }
}
```

#### The model block: Putting it all together

The model block is where we specify our statistical assumptions—both our prior beliefs and how the data was generated. This is where PDS's semantic computations meet statistical reality:

```stan
model {
  // Priors on random effects
  sigma_epsilon_guess ~ exponential(1);
  z_epsilon_guess ~ std_normal();
```

These priors encode mild assumptions:
- `exponential(1)`: We expect person-to-person variation to be moderate (not huge)
- `std_normal()`: By construction, z-scores have a standard normal distribution

Notice we don't specify priors for `mu_guess`—Stan treats this as an implicit uniform prior over the real numbers. Since our responses are bounded, the data will naturally constrain these values.

Now the likelihood—how data relates to parameters:

```stan
  // Likelihood
  for (i in 1:N_data) {
    y[i] ~ normal(guess[i], sigma_e);
  }
```

This says: each response is drawn from a normal distribution centered at our prediction with standard deviation `sigma_e`. The `~` symbol means "is distributed as."

For boundary responses, we use the latent values:

```stan
  for (i in 1:N_0) {
    y_0[i] ~ normal(guess_0[i], sigma_e);
  }
  
  for (i in 1:N_1) {
    y_1[i] ~ normal(guess_1[i], sigma_e);
  } 
}
```

Remember, we're inferring `y_0` and `y_1` as parameters! Stan will sample plausible values that are consistent with both the model and the fact that we observed 0s and 1s.

#### The generated quantities block: After-the-fact calculations

Finally, we compute quantities that help us understand and evaluate our model:

```stan
generated quantities {
  vector[N_data] ll; // log-likelihoods
  
  for (i in 1:N_data) {
    if (y[i] >= 0 && y[i] <= 1)
      ll[i] = normal_lpdf(y[i] | guess[i], sigma_e);
    else
      ll[i] = negative_infinity();
  }
}
```

The log-likelihood tells us how probable each observation is under our model. We'll use these for model comparison—models that assign higher probability to the actual data are better.

#### The complete norming model

Here's our complete model with consistent naming:

```stan
data {
  int<lower=1> N_item;              // number of items
  int<lower=1> N_participant;       // number of participants
  int<lower=1> N_data;              // number of data points in (0, 1)
  int<lower=1> N_0;                 // number of 0s
  int<lower=1> N_1;                 // number of 1s
  vector<lower=0, upper=1>[N_data] y; // response in (0, 1)
  array[N_data] int<lower=1, upper=N_item> item;
  array[N_0] int<lower=1, upper=N_item> item_0;
  array[N_1] int<lower=1, upper=N_item> item_1;
  array[N_data] int<lower=1, upper=N_participant> participant;
  array[N_0] int<lower=1, upper=N_participant> participant_0;
  array[N_1] int<lower=1, upper=N_participant> participant_1;
}

parameters {
  vector[N_item] mu_guess;
  real<lower=0> sigma_epsilon_guess;
  vector[N_participant] z_epsilon_guess;
  real<lower=0,upper=1> sigma_e;
  array[N_0] real<upper=0> y_0;
  array[N_1] real<lower=1> y_1;
}

transformed parameters {
  vector[N_participant] epsilon_guess = sigma_epsilon_guess * z_epsilon_guess;
  vector[N_data] guess;
  vector[N_0] guess_0;
  vector[N_1] guess_1;

  for (i in 1:N_data) {
    guess[i] = mu_guess[item[i]] + epsilon_guess[participant[i]];
  }
  for (i in 1:N_0) {
    guess_0[i] = mu_guess[item_0[i]] + epsilon_guess[participant_0[i]];
  }
  for (i in 1:N_1) {
    guess_1[i] = mu_guess[item_1[i]] + epsilon_guess[participant_1[i]];
  }
}

model {
  sigma_epsilon_guess ~ exponential(1);
  z_epsilon_guess ~ std_normal();

  for (i in 1:N_data) {
    y[i] ~ normal(guess[i], sigma_e);
  }
  for (i in 1:N_0) {
    y_0[i] ~ normal(guess_0[i], sigma_e);
  }
  for (i in 1:N_1) {
    y_1[i] ~ normal(guess_1[i], sigma_e);
  } 
}

generated quantities {
  vector[N_data] ll;
  for (i in 1:N_data) {
    if (y[i] >= 0 && y[i] <= 1)
      ll[i] = normal_lpdf(y[i] | guess[i], sigma_e);
    else
      ll[i] = negative_infinity();
  }
}
```

This baseline model treats each item as having an inherent degree along the relevant scale, with participants providing noisy measurements of these degrees. The censoring approach handles the common issue of responses at the boundaries (0 and 1) of the slider scale—implementing @lassiter_measurement_2013's approach to gradable adjective semantics.

#### What we've learned about Stan

Through building this model, we've seen that Stan programs have a specific structure:
1. **data block**: Declares observed quantities with constraints
2. **parameters block**: Declares unknown quantities to infer
3. **transformed parameters block**: Builds derived quantities
4. **model block**: Specifies priors and likelihood
5. **generated quantities block**: Computes post-inference quantities

We've also learned key Stan concepts:
- **Constraints**: `<lower=0>` ensures valid parameter ranges
- **Indexing**: Arrays like `item[i]` connect responses to structure
- **Sampling notation**: `y ~ normal(mu, sigma)` specifies distributions
- **Non-centered parameterization**: Improves sampling efficiency
- **Censoring**: Handles boundary responses theoretically

With this foundation in Stan, we can now understand how to translate semantic theories into Stan code. The structure of Stan programs mirrors the modular design of PDS—each component has a clear role in the overall system.

### PDS to Stan: Scale-norming with degree questions

Our baseline norming model asks participants "how tall is Jo?"—a degree question that directly elicits scalar judgments. Let's see how PDS compiles this semantic theory to Stan models. Degree questions use a special version of adjective lexical entries that expose the degree argument.

The PDS code for generating a scale-norming model is:

```haskell
s1'        = termOf $ getSemantics @Adjectives 1 ["jo", "is", "a", "soccer player"]
q1'        = termOf $ getSemantics @Adjectives 0 ["how", "tall", "jo", "is"]
discourse' = ty tau $ assert s1' >>> ask q1'
scaleNormingExample = asTyped tau (betaDeltaNormal deltaRules . adjectivesRespond scaleNormingPrior) discourse'
```

This code:
1. Asserts that Jo is a soccer player (establishing context)
2. Asks "how tall is Jo?" using the degree-argument version of the adjective
3. Applies beta and delta reduction rules via `betaDeltaNormal`
4. Uses `scaleNormingPrior` to generate prior distributions
5. Applies `adjectivesRespond` to specify the response function

The PDS system outputs the following kernel model:^[Actual PDS output: `model { w ~ normal(0.0, 1.0); target += normal_lpdf(y | w, sigma); }`]

```stan
model {
  // FIXED EFFECTS
  w ~ normal(0.0, 1.0);
  
  // LIKELIHOOD
  target += normal_lpdf(y | w, sigma);
}
```

This is the semantic core—it captures the essential degree-based semantics where `w` represents the degree on the height scale. But reality is complicated... we need random effects, the ability to model censored data, and proper indexing for multiple items and participants. This gap between the kernel model and a full statistical implementation represents ongoing research: how to get from here (PDS output) to here (actual implementation).

The full model with analyst augmentations looks like:

```stan {.line-numbers highlight="6,13"}
model {
  // PRIORS (analyst-added)
  sigma_epsilon_guess ~ exponential(1);
  sigma_e ~ beta(2, 10);
  
  // FIXED EFFECTS (PDS kernel)
  mu_guess ~ normal(0.0, 1.0);
  
  // RANDOM EFFECTS (analyst-added)
  z_epsilon_guess ~ std_normal();
  
  // LIKELIHOOD (PDS kernel with modifications)
  y[i] ~ normal(mu_guess[item[i]] + epsilon_guess[participant[i]], sigma_e);
}
```

Lines 6 and 13 (highlighted) show the kernel model from PDS. The unhighlighted portions add statistical machinery for real data: hierarchical priors, random effects, and indexed parameters. The kernel captures the core semantic computation—degrees on scales—while the augmentations handle the realities of experimental data.

This opens up the prospect of getting people involved in contributing functionality to PDS—the system automates the theory-to-model translation, but there's rich territory in developing principled ways to add statistical structure.

## From PDS lambda-terms to semantic values

Now that we understand Stan's structure through a concrete example, we can address the heart of our enterprise: translating semantic theories expressed as typed λ-terms into Stan programs that can be fit to data. Let's work through the degree question "how tall is Jo?" to see how PDS processes this construction.

### Working through degree questions

Degree questions like "how tall is Jo?" use a special lexical entry for adjectives that exposes the degree argument. From [`Grammar.Lexica.SynSem.Adjectives`](https://juliangrove.github.io/pds/Grammar-Lexica-SynSem-Adjectives.html):

```haskell
-- Degree-question version of "tall"
"tall" -> [ SynSem {
    syn = (S :\: NP) :/: (S :/: NP :/: Deg),
    sem = ty tau (lam s (purePP (lam f (lam x (lam i 
      (f @@ (sCon "height" @@ i @@ x) @@ x @@ i))))) @@ s))
} ]
```

This entry takes a degree-to-proposition function and applies it to the entity's height. The question word "how" provides this function:

```haskell
"how" -> [ SynSem {
    syn = S :/: (S :\: Deg),
    sem = ty tau (purePP (lam p (lam i (Return (iota (lam d (p @@ d @@ i)))))))
} ]
```

The $\iota$ operator extracts the unique degree satisfying the predicate—implementing the semantics of degree questions.

### Delta rules and semantic computation

PDS applies delta rules to simplify these complex λ-terms. As discussed in [the implementation section](../pds-intro/implementation.html), delta rules enable different semantic computations while preserving semantic equivalence. The formalism is strongly normalizing and confluent, so the order of rule application doesn't affect the final result—a crucial property that ensures our semantic theory remains consistent.

Key delta rules for adjectives include:
- **Arithmetic operations**: Simplifying comparisons like $\ct{(≥)}$ when applied to constants
- **State/index extraction**: Rules for $\ct{height}$, $\ct{d\_tall}$, etc.
- **Beta reduction**: Standard λ-calculus reduction

These rules transform the complex compositional semantics into simpler forms suitable for compilation to Stan. The transformation preserves the semantic content while making it computationally tractable.

### From lambda terms to Stan parameters

The challenge is translating abstract semantic computations into Stan's parameter space. This translation embodies our linking hypothesis between semantic competence and performance. Key transformations:

1. **Degree extraction becomes parameter inference**:
   - $\iota(\lambda d[\ct{height}(i)(j) = d])$ → Infer parameter `height_jo`
   - The unique degree satisfying the equation becomes a parameter to estimate

2. **Functions become data structures**:
   - $\ct{height} : \iota \to e \to r$ → Array `height[person]`
   - Function application → Array indexing

3. **Propositions become probabilities**:
   - Truth values → Real numbers in [0,1]
   - Logical operations → Probabilistic operations

4. **The monad becomes Stan's target**:
   - The $\Do$-notation structures sequential computation:
   ```
   \begin{array}[t]{rl}
   \Do & x ← \ct{return}(5) \\
       & y ← \ct{normal}(x, 1) \\
       & \ct{return}(y)
   \end{array}
   ```
   - This becomes contributions to Stan's log probability

This translation embodies our linking hypothesis: semantic computations generate behavioral data through a noisy measurement process captured by `adjectivesRespond`.

### Challenges in the translation

Several theoretical challenges arise when moving from abstract semantics to concrete implementation:

1. **Higher-order functions**: Degree questions involve functions over functions. Stan's first-order language requires "defunctionalization"—representing these as data structures.

2. **Dynamic effects**: Some semantic values contain free variables resolved dynamically. We must either:
   - Marginalize over possible resolutions
   - Fix resolution as data
   - Expand the parameter space

3. **Anaphora resolution**: Similar to dynamic effects, anaphoric dependencies create challenges for static compilation.

Each approach has trade-offs. Marginalization can be computationally intractable. Fixing resolution requires additional annotation. Parameter expansion can lead to combinatorial explosion @grove_probabilistic_2024.

### From semantics to psychosemantics

The translation from λ-terms to Stan forces us to be explicit about the relationship between semantic competence and performance. Our λ-terms express speakers' semantic knowledge—what they know about meaning. Our Stan models express how this knowledge generates behavioral data. The translation between them embodies our linking hypotheses @jasbi_linking_2019.

Different compilation strategies correspond to different psychological assumptions about semantic processing. By making these choices explicit in our Stan models, we commit to testable hypotheses about how speakers compute meanings. This connection between formal theory and psychological reality is what makes the PDS approach powerful—it forces us to be explicit about our assumptions at every level.

With this understanding of how PDS translates to Stan, we can now examine more sophisticated models that capture the theoretical distinctions between vagueness and imprecision.

### Model 2: Vagueness and imprecision

Now that we understand Stan's structure and how to translate from PDS, let's implement a more sophisticated model that captures the theoretical distinction between vagueness (uncertainty about standards) and imprecision (tolerance around thresholds).

#### Theoretical motivation

The key insight from @kennedy_vagueness_2007 and @lasersohn_pragmatic_1999: gradable adjectives involve comparing an item's degree to a threshold, where both the threshold location and the comparison precision can vary. This implements @barker_dynamics_2002's claim that vague predicates have metalinguistic effects—they don't just describe the world but negotiate standards. This negotiation happens through the discourse state mechanism in PDS.

#### PDS to Stan: Likelihood judgments

For the likelihood judgment "how likely is it that Jo is tall?", we use the bare adjective entry (type AP) rather than the degree-question version. The PDS code is:

```haskell
q1''        = termOf $ getSemantics @Adjectives 0 ["how", "likely", "that", "jo", "is", "tall"]
discourse'' = ty tau $ assert s1' >>> ask q1''
likelihoodExample = asTyped tau (betaDeltaNormal deltaRules . adjectivesRespond likelihoodPrior) discourse''
```

This uses `likelihoodPrior` to set up distributions over thresholds and degrees, capturing the uncertainty inherent in vague predicates. The prior encodes our theoretical commitment to gradience in standards.

PDS outputs this kernel model:^[Actual PDS output: `model { v ~ normal(0.0, 1.0); target += normal_lpdf(y | normal_cdf(v, -0.0, 1.0), sigma); }`]

```stan
model {
  // FIXED EFFECTS
  v ~ normal(0.0, 1.0);
  
  // LIKELIHOOD
  target += normal_lpdf(y | normal_cdf(v, -0.0, 1.0), sigma);
}
```

This kernel captures the semantic computation: the likelihood that Jo is tall depends on where Jo's height `v` falls relative to the threshold (here normalized to 0). The `normal_cdf` implements the vagueness—there's no sharp cutoff but a gradual transition, exactly as the semantic theory predicts.

But reality is complicated... we need to handle multiple adjectives, context effects, participant variation, and censored data. The full model with analyst augmentations:

```stan {.line-numbers highlight="7,15"}
model {
  // PRIORS (analyst-added)
  sigma_guess ~ exponential(5);
  spread ~ exponential(1);
  
  // DEGREE PARAMETERS (PDS kernel)
  // Each item has a degree on its scale
  vector<lower=0, upper=1>[N_item] d;
  
  // PARTICIPANT EFFECTS (analyst-added)
  sigma_epsilon_mu_guess ~ exponential(1);
  z_epsilon_mu_guess ~ std_normal();
  
  // LIKELIHOOD
  // Core semantic computation from PDS
  response_rel[i] = 1 - normal_cdf(d[item[i]] | mu_guess[i], sigma_guess);
  // Measurement model (analyst-added modification)
  y[i] ~ normal(response_rel[i], sigma_e);
}
```

The highlighted lines show PDS's semantic computation—comparing degrees to thresholds with vagueness. The unhighlighted code adds the statistical machinery needed for real experimental data. The point here is clear: ongoing research involves figuring out how to get from here (PDS output) to here (actual implementation) in a principled way.

#### Data structure for vagueness experiments

Our vagueness/imprecision data looks similar to the norming data but now tracks adjectives separately:

```csv
"participant","item","item_number","adjective","adjective_number","scale_type","scale_type_number","condition","condition_number","response"
1,"9_high",25,"quiet",9,"absolute",1,"high",1,0.82
1,"4_low",11,"wide",4,"relative",2,"low",2,0.34
1,"5_mid",15,"deep",5,"relative",2,"mid",3,0.77
```

The key difference: we now distinguish between items (specific adjective-condition pairs) and adjectives themselves. For example:
- Item "9_high" = adjective "quiet" in the "high" condition
- Item "4_low" = adjective "wide" in the "low" condition

This structure lets us model properties that belong to adjectives (like how context-sensitive they are) separately from properties of specific items—a distinction motivated by the semantic theory.

#### Complete Stan code for the vagueness model

Here's the complete model with detailed explanations:

```stan
data {
  // Basic counts
  int<lower=1> N_item;         // number of items (adjective × condition)
  int<lower=1> N_adjective;    // number of unique adjectives
  int<lower=1> N_participant;  // number of participants
  int<lower=1> N_data;         // responses in (0,1)
  int<lower=1> N_0;            // boundary responses at 0
  int<lower=1> N_1;            // boundary responses at 1
  
  // Response data
  vector<lower=0, upper=1>[N_data] y;  // slider responses
  
  // NEW: Mapping structure
  array[N_item] int<lower=1, upper=N_adjective> item_adj;  // which adjective for each item
  
  // Indexing arrays for responses
  array[N_data] int<lower=1, upper=N_item> item;
  array[N_0] int<lower=1, upper=N_item> item_0;
  array[N_1] int<lower=1, upper=N_item> item_1;
  array[N_data] int<lower=1, upper=N_adjective> adjective;
  array[N_0] int<lower=1, upper=N_adjective> adjective_0;
  array[N_1] int<lower=1, upper=N_adjective> adjective_1;
  array[N_data] int<lower=1, upper=N_participant> participant;
  array[N_0] int<lower=1, upper=N_participant> participant_0;
  array[N_1] int<lower=1, upper=N_participant> participant_1;
}

parameters {
  // SEMANTIC PARAMETERS
  
  // Each item has a degree on its scale
  vector<lower=0, upper=1>[N_item] d;
  
  // Global vagueness: how fuzzy are threshold comparisons?
  real<lower=0> sigma_guess;
  
  // Adjective-specific context sensitivity
  vector<lower=0>[N_adjective] spread;
  
  // PARTICIPANT VARIATION
  
  // How much participants vary in their thresholds
  real<lower=0> sigma_epsilon_mu_guess;
  // Each participant's standardized deviation
  vector[N_participant] z_epsilon_mu_guess;
  
  // RESPONSE NOISE
  real<lower=0, upper=1> sigma_e;
  
  // CENSORED DATA
  array[N_0] real<upper=0> y_0;  // latent values for 0s
  array[N_1] real<lower=1> y_1;  // latent values for 1s
}

transformed parameters {
  // Convert standardized participant effects to natural scale
  vector[N_participant] epsilon_mu_guess = sigma_epsilon_mu_guess * z_epsilon_mu_guess;
  
  // STEP 1: Set up base thresholds for each item
  vector[N_item] mu_guess0;
  
  // This assumes our data has 3 conditions per adjective in order:
  // high (index 1), low (index 2), mid (index 3)
  for (i in 0:(N_adjective-1)) {
    // High condition: positive threshold shift
    mu_guess0[3 * i + 1] = spread[i + 1];
    // Low condition: negative threshold shift  
    mu_guess0[3 * i + 2] = -spread[i + 1];
    // Mid condition: no shift (baseline)
    mu_guess0[3 * i + 3] = 0;
  }
  
  // STEP 2: Transform thresholds to probability scale
  vector<lower=0, upper=1>[N_data] mu_guess;
  vector<lower=0, upper=1>[N_0] mu_guess_0;
  vector<lower=0, upper=1>[N_1] mu_guess_1;
  
  // STEP 3: Compute predicted responses
  vector<lower=0, upper=1>[N_data] response_rel;
  vector<lower=0, upper=1>[N_0] response_rel_0;
  vector<lower=0, upper=1>[N_1] response_rel_1;
  
  // For each response in (0,1)
  for (i in 1:N_data) {
    // Add participant adjustment to base threshold
    real threshold_logit = mu_guess0[item[i]] + epsilon_mu_guess[participant[i]];
    // Convert from logit scale to probability scale
    mu_guess[i] = inv_logit(threshold_logit);
    
    // KEY SEMANTIC COMPUTATION:
    // P(adjective applies) = P(degree > threshold)
    // Using normal CDF for smooth threshold crossing
    response_rel[i] = 1 - normal_cdf(d[item[i]] | mu_guess[i], sigma_guess);
  }
  
  // Repeat for censored data
  for (i in 1:N_0) {
    mu_guess_0[i] = inv_logit(mu_guess0[item_0[i]] + epsilon_mu_guess[participant_0[i]]);
    response_rel_0[i] = 1 - normal_cdf(d[item_0[i]] | mu_guess_0[i], sigma_guess);
  }
  
  for (i in 1:N_1) {
    mu_guess_1[i] = inv_logit(mu_guess0[item_1[i]] + epsilon_mu_guess[participant_1[i]]);
    response_rel_1[i] = 1 - normal_cdf(d[item_1[i]] | mu_guess_1[i], sigma_guess);
  }
}

model {
  // PRIORS
  
  // Vagueness: smaller values = more precise thresholds
  sigma_guess ~ exponential(5);
  
  // Context effects: how much standards shift
  spread ~ exponential(1);
  
  // Participant variation
  sigma_epsilon_mu_guess ~ exponential(1);
  z_epsilon_mu_guess ~ std_normal();
  
  // LIKELIHOOD
  
  // Observed responses are noisy measurements of semantic judgments
  for (i in 1:N_data) {
    y[i] ~ normal(response_rel[i], sigma_e);
  }
  
  // Censored responses
  for (i in 1:N_0) {
    y_0[i] ~ normal(response_rel_0[i], sigma_e);
  }
  
  for (i in 1:N_1) {
    y_1[i] ~ normal(response_rel_1[i], sigma_e);
  }
}

generated quantities {
  // Log-likelihood for model comparison
  vector[N_data] ll;
  
  for (i in 1:N_data) {
    ll[i] = normal_lpdf(y[i] | response_rel[i], sigma_e);
  }
  
  // We could also compute other quantities of interest:
  // - Average vagueness per adjective
  // - Predicted responses for new items
  // - Posterior predictive checks
}
```

#### How the model components map to semantic theory

Let's trace through a specific example to see how this model works:

1. **Item degree**: Suppose we're modeling "tall" in the high condition. The parameter `d[item["tall_high"]]` might be 0.85, representing that basketball players (high condition) have high degrees on the height scale.

2. **Adjective spread**: The parameter `spread["tall"]` might be 2.0, meaning "tall" is highly context-sensitive—its threshold shifts dramatically between conditions. This captures the context-dependency that the discourse state mechanism models.

3. **Threshold computation**: 
   - Base threshold (logit scale): `mu_guess0["tall_high"] = spread["tall"] = 2.0`
   - Participant adjustment: Say participant 5 has `epsilon_mu_guess[5] = -0.3`
   - Final threshold (logit): `2.0 + (-0.3) = 1.7`
   - Final threshold (probability): `inv_logit(1.7) ≈ 0.85`

4. **Semantic judgment**:
   - The crucial computation: `response_rel = 1 - normal_cdf(0.85 | 0.85, sigma_guess)`
   - If `sigma_guess = 0.1` (precise threshold), this gives ≈ 0.5
   - If `sigma_guess = 0.3` (vague threshold), the response is more variable

5. **Response generation**: The participant's actual slider response is a noisy measurement of this semantic judgment, with noise `sigma_e`.

This model implements several key theoretical insights:
- **Vagueness as threshold uncertainty**: The `sigma_guess` parameter captures how fuzzy the boundary is
- **Context sensitivity**: The `spread` parameters capture how standards shift across contexts
- **Individual differences**: Participants can have systematically different thresholds
- **Measurement error**: Slider responses are noisy measurements of semantic judgments

The model thus operationalizes the theoretical distinctions introduced earlier while adding the statistical machinery needed for real experimental data.

### Model 3: Mixture of relative and absolute standards

Our most sophisticated model implements the theoretical distinction between relative and absolute gradable adjectives. Some adjectives like "tall" use relative standards that shift with the comparison class, while others like "full" use absolute standards tied to scale endpoints.

#### Theoretical motivation

@kennedy_scale_2005 argues that gradable adjectives fall into two classes:
- **Relative adjectives** (tall, expensive): Standards shift dramatically with context
- **Absolute adjectives** (full, empty): Standards are anchored to scale endpoints

This distinction has been supported experimentally by @syrett_meaning_2010 and @toledo_absolute_2013. Our mixture model allows the data to reveal which interpretation strategy participants use.

While PDS doesn't currently output mixture models directly, this represents a natural extension where semantic theory guides statistical model structure. The mixture implements the insight that the relative/absolute distinction might not be categorical but graded—a hypothesis that emerges naturally from the compositional framework.

#### Complete Stan code for the mixture model

```stan {.line-numbers highlight="45-47,59-61"}
data {
  // Same basic structure as Model 2
  int<lower=1> N_item;
  int<lower=1> N_adjective;
  int<lower=1> N_participant;
  int<lower=1> N_data;
  int<lower=1> N_0;
  int<lower=1> N_1;
  
  vector<lower=0, upper=1>[N_data] y;
  array[N_item] int<lower=1, upper=N_adjective> item_adj;
  
  // Indexing arrays (same structure as Model 2)
  array[N_data] int<lower=1, upper=N_item> item;
  array[N_0] int<lower=1, upper=N_item> item_0;
  array[N_1] int<lower=1, upper=N_item> item_1;
  array[N_data] int<lower=1, upper=N_adjective> adjective;
  array[N_0] int<lower=1, upper=N_adjective> adjective_0;
  array[N_1] int<lower=1, upper=N_adjective> adjective_1;
  array[N_data] int<lower=1, upper=N_participant> participant;
  array[N_0] int<lower=1, upper=N_participant> participant_0;
  array[N_1] int<lower=1, upper=N_participant> participant_1;
}

parameters {
  // MIXTURE COMPONENT (semantic theory: two interpretation strategies)
  
  // Global probability of using relative (vs absolute) interpretation
  real<lower=0, upper=1> which;
  
  // TWO SETS OF DEGREE PARAMETERS (PDS kernel structure)
  
  // Absolute interpretation: degrees are fixed per adjective
  vector<lower=0, upper=1>[N_adjective] d_a;
  
  // Relative interpretation: degrees vary by item (adjective × context)
  vector<lower=0, upper=1>[N_item] d_r;
  
  // THRESHOLD PARAMETERS (same as Model 2)
  
  // Global vagueness
  real<lower=0> sigma_guess;
  
  // Context sensitivity per adjective
  vector<lower=0>[N_adjective] spread;
  
  // PARTICIPANT EFFECTS
  real<lower=0> sigma_epsilon_mu_guess;
  vector[N_participant] z_epsilon_mu_guess;
  
  // RESPONSE NOISE
  real<lower=0, upper=1> sigma_e;
  
  // CENSORED DATA
  array[N_0] real<upper=0> y_0;
  array[N_1] real<lower=1> y_1;
}

transformed parameters {
  // Participant effects (same as Model 2)
  vector[N_participant] epsilon_mu_guess = sigma_epsilon_mu_guess * z_epsilon_mu_guess;
  
  // MIXTURE WEIGHTS ON LOG SCALE
  // This prevents numerical underflow when probabilities get very small
  vector[2] log_which;
  log_which[1] = log(which);      // log P(relative)
  log_which[2] = log1m(which);    // log P(absolute) = log(1 - which)
  
  // BASE THRESHOLDS (same structure as Model 2)
  vector[N_item] mu_guess0;
  for (i in 0:(N_adjective-1)) {
    mu_guess0[3 * i + 1] = spread[i + 1];   // High condition
    mu_guess0[3 * i + 2] = -spread[i + 1];  // Low condition
    mu_guess0[3 * i + 3] = 0;                // Mid condition
  }
  
  // COMPUTE PREDICTIONS UNDER BOTH INTERPRETATIONS
  
  // Thresholds on probability scale
  vector<lower=0, upper=1>[N_data] mu_guess;
  vector<lower=0, upper=1>[N_0] mu_guess_0;
  vector<lower=0, upper=1>[N_1] mu_guess_1;
  
  // Predictions under relative interpretation
  vector<lower=0, upper=1>[N_data] response_rel;
  vector<lower=0, upper=1>[N_0] response_rel_0;
  vector<lower=0, upper=1>[N_1] response_rel_1;
  
  // Predictions under absolute interpretation
  vector<lower=0, upper=1>[N_data] response_abs;
  vector<lower=0, upper=1>[N_0] response_abs_0;
  vector<lower=0, upper=1>[N_1] response_abs_1;
  
  // For each response
  for (i in 1:N_data) {
    // Threshold (same for both interpretations)
    mu_guess[i] = inv_logit(mu_guess0[item[i]] + epsilon_mu_guess[participant[i]]);
    
    // RELATIVE: degree varies by item
    // "tall for a basketball player" vs "tall for a jockey"
    response_rel[i] = 1 - normal_cdf(d_r[item[i]] | mu_guess[i], sigma_guess);
    
    // ABSOLUTE: degree fixed per adjective
    // "tall" has same degree regardless of context
    response_abs[i] = 1 - normal_cdf(d_a[adjective[i]] | mu_guess[i], sigma_guess);
  }
  
  // Repeat for censored data at 0
  for (i in 1:N_0) {
    mu_guess_0[i] = inv_logit(mu_guess0[item_0[i]] + epsilon_mu_guess[participant_0[i]]);
    response_rel_0[i] = 1 - normal_cdf(d_r[item_0[i]] | mu_guess_0[i], sigma_guess);
    response_abs_0[i] = 1 - normal_cdf(d_a[adjective_0[i]] | mu_guess_0[i], sigma_guess);
  }
  
  // Repeat for censored data at 1
  for (i in 1:N_1) {
    mu_guess_1[i] = inv_logit(mu_guess0[item_1[i]] + epsilon_mu_guess[participant_1[i]]);
    response_rel_1[i] = 1 - normal_cdf(d_r[item_1[i]] | mu_guess_1[i], sigma_guess);
    response_abs_1[i] = 1 - normal_cdf(d_a[adjective_1[i]] | mu_guess_1[i], sigma_guess);
  }
}

model {
  // PRIORS
  
  // Vagueness and context effects
  sigma_guess ~ exponential(5);
  spread ~ exponential(1);
  
  // Participant variation
  sigma_epsilon_mu_guess ~ exponential(1);
  z_epsilon_mu_guess ~ std_normal();
  
  // MIXTURE LIKELIHOOD (PDS kernel structure)
  
  // For each observed response
  for (i in 1:N_data) {
    // Log probability under relative interpretation
    real lps_r = log_which[1] + normal_lpdf(y[i] | response_rel[i], sigma_e);
    
    // Log probability under absolute interpretation
    real lps_a = log_which[2] + normal_lpdf(y[i] | response_abs[i], sigma_e);
    
    // Add log of sum of probabilities (marginalizing over interpretations)
    // log_sum_exp(a, b) = log(exp(a) + exp(b)) but numerically stable
    target += log_sum_exp(lps_r, lps_a);
  }
  
  // Same for censored at 0
  for (i in 1:N_0) {
    real lps_r = log_which[1] + normal_lpdf(y_0[i] | response_rel_0[i], sigma_e);
    real lps_a = log_which[2] + normal_lpdf(y_0[i] | response_abs_0[i], sigma_e);
    target += log_sum_exp(lps_r, lps_a);
  }
  
  // Same for censored at 1
  for (i in 1:N_1) {
    real lps_r = log_which[1] + normal_lpdf(y_1[i] | response_rel_1[i], sigma_e);
    real lps_a = log_which[2] + normal_lpdf(y_1[i] | response_abs_1[i], sigma_e);
    target += log_sum_exp(lps_r, lps_a);
  }
}

generated quantities {
  // OVERALL LOG-LIKELIHOOD
  vector[N_data] ll;
  
  // COMPONENT LOG-LIKELIHOODS
  vector[N_data] ll_r;  // relative component
  vector[N_data] ll_a;  // absolute component
  
  // POSTERIOR PROBABILITY OF EACH INTERPRETATION
  vector[N_data] prob_relative;
  
  for (i in 1:N_data) {
    // Component likelihoods
    ll_r[i] = log_which[1] + normal_lpdf(y[i] | response_rel[i], sigma_e);
    ll_a[i] = log_which[2] + normal_lpdf(y[i] | response_abs[i], sigma_e);
    
    // Overall likelihood
    ll[i] = log_sum_exp(ll_r[i], ll_a[i]);
    
    // Posterior probability this response used relative interpretation
    prob_relative[i] = exp(ll_r[i] - ll[i]);
  }
  
  // We could also compute:
  // - Which adjectives are more likely relative vs absolute
  // - Individual differences in interpretation preference
  // - Predicted classifications for new adjectives
}
```

#### How the mixture model works: A concrete example

Let's trace through how this model handles the adjective "tall" in different conditions:

1. **Under relative interpretation**:
   - "tall" + "basketball player" (high condition): `d_r["tall_high"] = 0.9`
   - "tall" + "average person" (mid condition): `d_r["tall_mid"] = 0.6`
   - "tall" + "child" (low condition): `d_r["tall_low"] = 0.3`
   
   The degree changes with context—what counts as tall depends on the comparison class, exactly as the discourse state mechanism predicts.

2. **Under absolute interpretation**:
   - All conditions: `d_a["tall"] = 0.7`
   
   The degree is fixed—"tall" means the same thing regardless of context.

3. **Threshold computation** (same for both):
   - High condition: threshold shifts up (`spread["tall"]` added)
   - Low condition: threshold shifts down (`spread["tall"]` subtracted)
   - But the shift might be small if "tall" is absolute-like

4. **Mixture computation**:
   ```
   P(response | data) = which × P(response | relative) + (1-which) × P(response | absolute)
   ```
   
   If `which = 0.8`, then 80% of responses come from relative interpretation.

5. **What we learn**:
   - Global `which`: Overall tendency toward relative vs absolute interpretation
   - `spread` values: Which adjectives show more context sensitivity
   - Generated `prob_relative`: Which specific responses likely used which interpretation

#### Key innovations of the mixture model

1. **Theory-neutral**: The model doesn't assume which adjectives are relative or absolute—it learns from data
2. **Graded membership**: Adjectives can be partially relative/absolute rather than categorical
3. **Individual differences**: Different participants might use different interpretation strategies
4. **Response-level inference**: We can identify which specific responses reflect which interpretation

This mixture approach implements the theoretical insight that the relative/absolute distinction might not be categorical but graded, with some adjectives showing mixed behavior [@mcnally_scale_2013]. It demonstrates how semantic theory can guide statistical model structure even when PDS doesn't yet output such complex models directly.

## Conclusion: From theory to implementation

Through these three models, we've seen how PDS translates abstract semantic theories into concrete statistical models:

1. **Model 1 (Norming)**: Established the basic Stan architecture and censoring approach for degree questions
2. **Model 2 (Vagueness)**: Showed how degree-based semantics with likelihood judgments becomes probabilistic computation
3. **Model 3 (Mixture)**: Demonstrated how theoretical distinctions become statistical hypotheses

The progression from PDS's kernel models to full Stan implementations reveals several key insights:

- **Kernel models capture semantics**: The highlighted portions of our Stan code directly implement semantic computations from compositional theory
- **Augmentations handle reality**: Random effects, hierarchical structure, and measurement noise aren't semantic but are necessary for real data
- **Theory guides implementation**: Different theoretical commitments (discrete vs. gradient, relative vs. absolute) lead to different model structures
- **Compilation is theoretical**: The choices made in translating λ-terms to Stan embody linking hypotheses about semantic processing

### Future directions

This work opens several avenues for development:

1. **Automated augmentation**: Currently, analysts manually add statistical structure to kernel models. Future versions of PDS could automate common augmentations like random effects and hierarchical priors.

2. **Formally verified compilation**: Delta rules could be accompanied by formal proofs in Agda or Coq, ensuring soundness while enabling powerful transformations.

3. **Richer kernel models**: Extending PDS to output mixture models, censoring mechanisms, and other statistical structures that directly encode semantic theories.

4. **Community contributions**: The modular design of PDS invites researchers to contribute new lexical entries, delta rules, and compilation strategies.

Building on the PDS framework introduced in [previous sections](../pds-intro/overview.html), we've seen how computational tools can bridge the gap between formal semantic theory and experimental data. These models provide the foundation for understanding factivity—where gradience poses even deeper theoretical puzzles that we'll explore in the next section.