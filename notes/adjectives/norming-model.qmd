---
title: "Norming model"
bibliography: ../../pds.bib
format:
  html:
    css: ../styles.css
    html-math-method: mathjax
    mathjax-config:
      loader: {load: ['[tex]/bussproofs','[tex]/bbox','[tex]/colorbox']}
      tex:
        packages: {'[+]': ['bussproofs','bbox','colorbox']}
---

::: {.hidden}
$$
\newcommand{\expr}[3]{\begin{array}{c}
#1 \\
\bbox[lightblue,5px]{#2}
\end{array} ⊢ #3}
\newcommand{\ct}[1]{\bbox[font-size: 0.8em]{\mathsf{#1}}}
\newcommand{\updct}[1]{\ct{upd\_#1}}
\newcommand{\abbr}[1]{\bbox[transform: scale(0.95)]{\mathtt{#1}}}
\newcommand{\pure}[1]{\bbox[border: 1px solid orange]{\bbox[border: 4px solid transparent]{#1}}}
\newcommand{\return}[1]{\bbox[border: 1px solid black]{\bbox[border: 4px solid transparent]{#1}}}
\def\P{\mathtt{P}}
\def\Q{\mathtt{Q}}
\def\True{\ct{T}}
\def\False{\ct{F}}
\def\ite{\ct{if\_then\_else}}
\def\Do{\abbr{do}}
$$
:::

We'll start with a relativly simple model of the norming data that one might design as a means for analyzing that dataset. What we'll do is to build up the model block-by-block, explaining each line. Then, we'll turn to how we might analyze this experiment using PDS and show which components of this model correspond to the PDS kernel model and which are extensions by the analyst.

## Understanding the experimental setup

Before diving into the Stan code, let's consider how we'll represent the norming data, since this is important for understanding how we design Stan code. Here's a sample of the data:

| participant | item | item_number | adjective | adjective_number | condition | condition_number | scale_type | scale_type_number | response |
|-------------|------|-------------|-----------|------------------|-----------|------------------|------------|-------------------|----------|
| 1 | closed_mid | 6 | closed | 2 | mid | 3 | absolute | 1 | 0.66 |
| 1 | old_mid | 24 | old | 8 | mid | 3 | relative | 2 | 0.51 |
| 1 | expensive_mid | 15 | expensive | 5 | mid | 3 | relative | 2 | 0.62 |
| 1 | full_high | 16 | full | 6 | high | 1 | absolute | 1 | 1 |
| 1 | deep_low | 8 | deep | 3 | low | 2 | relative | 2 | 0.22 |

Each row represents one judgment:
- `participant`: Which person made this judgment (participant 1, 2, etc.)
- `item`: A unique identifier combining adjective and condition (e.g., "tall_high")
- `item_number`: Numeric ID for the item (used in Stan)
- `adjective`: The gradable adjective being tested
- `condition`: Whether this is a high/mid/low standard context
- `response`: The participant's slider response (0-1)

Our simplest model asks: What degree does each item have on its scale, and how do participants map these degrees to slider responses?

## The structure of a Stan program

Every Stan program follows a particular architecture with blocks that appear in a specific order. Each block serves a specific purpose in defining our statistical model. Let's build up our norming model block by block to understand how Stan works. This structure parallels the modular architecture of PDS itself—each block handles a distinct aspect of the modeling problem.

### The `data` block

Every Stan program begins with a `data` block that tells Stan what information will be provided from the outside world—our experimental observations. Let's build this up piece by piece:

```stan
data {
  int<lower=1> N_item;        // number of items
  int<lower=1> N_participant; // number of participants  
  int<lower=1> N_data;        // number of data points
}

These first lines declare basic counts. The syntax breaks down as:
- `int`: This will be an integer (whole number)
- `<lower=1>`: This integer must be at least 1 (no negative counts!)
- `N_item`: The variable name (we'll use this throughout our program)
- `// number of items`: A comment explaining what this represents

Why do we need these constraints? Stan uses them to:
1. Catch data errors early (if we accidentally pass 0 items, Stan will complain)
2. Optimize its algorithms (knowing bounds helps the sampler work efficiently)

Next, we handle a subtle but important issue—boundary responses:

```stan
  int<lower=1> N_0;           // number of 0s
  int<lower=1> N_1;           // number of 1s
}

Why separate these out? Slider responses of exactly 0 or 1 are "censored"—they might represent even more extreme judgments that the scale can't capture. We'll handle these specially, implementing insights from measurement theory.

Now for the actual response data:

```stan
  vector<lower=0, upper=1>[N_data] y; // response in (0, 1)
}

This declares a vector (like an array) of length `N_data`, where each element must be between 0 and 1. Notice this is for responses *between* 0 and 1, not including the boundaries.

Finally, we need to map responses to items and participants:

```stan
  array[N_data] int<lower=1, upper=N_item> item;        // which item for each response
  array[N_0] int<lower=1, upper=N_item> item_0;         // which item for each 0
  array[N_1] int<lower=1, upper=N_item> item_1;         // which item for each 1
  array[N_data] int<lower=1, upper=N_participant> participant;     
  array[N_0] int<lower=1, upper=N_participant> participant_0;
  array[N_1] int<lower=1, upper=N_participant> participant_1;
}

These arrays work like lookup tables. If `item[5] = 3`, then the 5th response in our data was about item #3. This indexing structure connects our flat data file to the hierarchical structure of our experiment.

Looking back at our CSV data, when Stan reads it, it will:
1. Count unique items → `N_item` (e.g., 36 if we have 12 adjectives × 3 conditions)
2. Count unique participants → `N_participant` 
3. Extract all responses between 0 and 1 → `y` vector
4. Build index arrays mapping each response to its item and participant

#### The parameters block: What we want to learn

After declaring our data, we declare the parameters—the unknown quantities we want to infer. This is where semantic theory meets statistical inference:

```stan
parameters {
  // Fixed effects
  vector[N_item] mu_guess;
}

This declares a vector of "guesses" (degrees) for each item. Why `mu_guess`? In statistics, μ (mu) traditionally denotes a mean or central tendency. These represent our best guess about each item's true degree on its scale—the theoretical degrees that the semantic analysis posits.

But people differ! We need random effects to capture individual variation:

```stan
  // Random effects
  real<lower=0> sigma_epsilon_guess;     // how much people vary
  vector[N_participant] z_epsilon_guess; // each person's deviation
}

This uses a clever trick called "non-centered parameterization":
- `sigma_epsilon_guess`: The overall amount of person-to-person variation
- `z_epsilon_guess`: Standardized (z-score) deviations for each person

We'll combine these later to get each person's actual adjustment. Why not just use `vector[N_participant] epsilon_guess` directly? This separation often helps Stan's algorithms converge much faster—a practical consideration that doesn't affect the semantic theory but matters for implementation.

Next, measurement noise:

```stan
  real<lower=0,upper=1> sigma_e;  // response variability
}

Even if two people agree on an item's degree, their slider responses might differ slightly. This parameter captures that noise.

Finally, those boundary responses:

```stan
  // Censored data
  array[N_0] real<upper=0> y_0;  // true values for observed 0s
  array[N_1] real<lower=1> y_1;  // true values for observed 1s
}

This is subtle but important. When someone gives a 0 response, their "true" judgment might be -0.1 or -0.5—we just can't see below 0. These parameters let Stan infer what those true values might have been @stevens_psychophysics_1975.

#### The transformed parameters block: Building predictions

Now we combine our basic parameters to build what we actually need. This block serves as a bridge between abstract parameters and concrete predictions:

```stan
transformed parameters {
  vector[N_participant] epsilon_guess;
  vector[N_data] guess;
  vector[N_0] guess_0;
  vector[N_1] guess_1;
}

First, we convert those z-scores to actual participant adjustments:

```stan
  // Non-centered parameterization
  epsilon_guess = sigma_epsilon_guess * z_epsilon_guess;
}

If `sigma_epsilon_guess = 0.2` and participant 3 has `z_epsilon_guess[3] = 1.5`, then participant 3 tends to give responses 0.3 units higher than average.

Now we can compute predicted responses:

```stan
  for (i in 1:N_data) {
    guess[i] = mu_guess[item[i]] + epsilon_guess[participant[i]];
  }
```

Let's trace through one prediction:
- Response i is about item 5 by participant 3
- `item[i] = 5`, so we look up `mu_guess[5]` (say it's 0.7)
- `participant[i] = 3`, so we add `epsilon_guess[3]` (say it's 0.1)
- `guess[i] = 0.7 + 0.1 = 0.8`

We repeat this for the boundary responses:

```stan
  for (i in 1:N_0) {
    guess_0[i] = mu_guess[item_0[i]] + epsilon_guess[participant_0[i]];
  }
  
  for (i in 1:N_1) {
    guess_1[i] = mu_guess[item_1[i]] + epsilon_guess[participant_1[i]];
  }
}
```

### The `model` block

The model block is where we specify our statistical assumptions—both our prior beliefs and how the data was generated. This is where most of the action in terms of how PDS relates to data.

```stan
model {
  // Priors on random effects
  sigma_epsilon_guess ~ exponential(1);
  z_epsilon_guess ~ std_normal();
```

These priors encode mild assumptions:
- `exponential(1)`: We expect person-to-person variation to be moderate (not huge)
- `std_normal()`: By construction, z-scores have a standard normal distribution

Notice we don't specify priors for `mu_guess`—Stan treats this as an implicit uniform prior over the real numbers. Since our responses are bounded, the data will naturally constrain these values.

Now the likelihood—how data relates to parameters:

```stan
  // Likelihood
  for (i in 1:N_data) {
    y[i] ~ normal(guess[i], sigma_e);
  }
```

This says: each response is drawn from a normal distribution centered at our prediction with standard deviation `sigma_e`. The `~` symbol means "is distributed as."

For boundary responses, we use the latent values:

```stan
  for (i in 1:N_0) {
    y_0[i] ~ normal(guess_0[i], sigma_e);
  }
  
  for (i in 1:N_1) {
    y_1[i] ~ normal(guess_1[i], sigma_e);
  } 
}
```

Remember, we're inferring `y_0` and `y_1` as parameters! Stan will sample plausible values that are consistent with both the model and the fact that we observed 0s and 1s.

### The `generated quantities` block

Finally, we compute quantities that help us understand and evaluate our model:

```stan
generated quantities {
  vector[N_data] ll; // log-likelihoods
  
  for (i in 1:N_data) {
    if (y[i] >= 0 && y[i] <= 1)
      ll[i] = normal_lpdf(y[i] | guess[i], sigma_e);
    else
      ll[i] = negative_infinity();
  }
}
```

The log-likelihood tells us how probable each observation is under our model. We'll use these for model comparison—models that assign higher probability to the actual data are better.

### The complete model

Here's our complete model with consistent naming:

```stan
data {
  int<lower=1> N_item;              // number of items
  int<lower=1> N_participant;       // number of participants
  int<lower=1> N_data;              // number of data points in (0, 1)
  int<lower=1> N_0;                 // number of 0s
  int<lower=1> N_1;                 // number of 1s
  vector<lower=0, upper=1>[N_data] y; // response in (0, 1)
  array[N_data] int<lower=1, upper=N_item> item;
  array[N_0] int<lower=1, upper=N_item> item_0;
  array[N_1] int<lower=1, upper=N_item> item_1;
  array[N_data] int<lower=1, upper=N_participant> participant;
  array[N_0] int<lower=1, upper=N_participant> participant_0;
  array[N_1] int<lower=1, upper=N_participant> participant_1;
}

parameters {
  vector[N_item] mu_guess;
  real<lower=0> sigma_epsilon_guess;
  vector[N_participant] z_epsilon_guess;
  real<lower=0,upper=1> sigma_e;
  array[N_0] real<upper=0> y_0;
  array[N_1] real<lower=1> y_1;
}

transformed parameters {
  vector[N_participant] epsilon_guess = sigma_epsilon_guess * z_epsilon_guess;
  vector[N_data] guess;
  vector[N_0] guess_0;
  vector[N_1] guess_1;

  for (i in 1:N_data) {
    guess[i] = mu_guess[item[i]] + epsilon_guess[participant[i]];
  }
  for (i in 1:N_0) {
    guess_0[i] = mu_guess[item_0[i]] + epsilon_guess[participant_0[i]];
  }
  for (i in 1:N_1) {
    guess_1[i] = mu_guess[item_1[i]] + epsilon_guess[participant_1[i]];
  }
}

model {
  sigma_epsilon_guess ~ exponential(1);
  z_epsilon_guess ~ std_normal();

  for (i in 1:N_data) {
    y[i] ~ normal(guess[i], sigma_e);
  }
  for (i in 1:N_0) {
    y_0[i] ~ normal(guess_0[i], sigma_e);
  }
  for (i in 1:N_1) {
    y_1[i] ~ normal(guess_1[i], sigma_e);
  } 
}

generated quantities {
  vector[N_data] ll;
  for (i in 1:N_data) {
    if (y[i] >= 0 && y[i] <= 1)
      ll[i] = normal_lpdf(y[i] | guess[i], sigma_e);
    else
      ll[i] = negative_infinity();
  }
}
```

This baseline model treats each item as having an inherent degree along the relevant scale, with participants providing noisy measurements of these degrees. The censoring approach handles the common issue of responses at the boundaries (0 and 1) of the slider scale.

## PDS-to-Stan

So what components of the above model are derived from PDS? To answer this, we need to define our PDS model of the norming task itself. Here it is:

```haskell
s1'        = termOf $ getSemantics @Adjectives 1 ["jo", "is", "a", "soccer player"]
q1'        = termOf $ getSemantics @Adjectives 0 ["how", "tall", "jo", "is"]
discourse' = ty tau $ assert s1' >>> ask q1'
scaleNormingExample = asTyped tau (betaDeltaNormal deltaRules . adjectivesRespond scaleNormingPrior) discourse'
```

This code:
1. Asserts that Jo is a soccer player (establishing context)
2. Asks "how tall is Jo?" using the degree-argument version of the adjective
3. Applies beta and delta reduction rules via `betaDeltaNormal`
4. Uses `scaleNormingPrior` to generate prior distributions
5. Applies `adjectivesRespond` to specify the response function

### Working through degree questions

Degree questions like "how tall is Jo?" use a special lexical entry for adjectives that exposes the degree argument. From [`Grammar.Lexica.SynSem.Adjectives`](https://juliangrove.github.io/pds/Grammar-Lexica-SynSem-Adjectives.html):

```haskell
-- Degree-question version of "tall"
"tall" -> [ SynSem {
    syn = (S :\: NP) :/: (S :/: NP :/: Deg),
    sem = ty tau (lam s (purePP (lam f (lam x (lam i 
      (f @@ (sCon "height" @@ i @@ x) @@ x @@ i))))) @@ s))
} ]
```

This entry takes a degree-to-proposition function and applies it to the entity's height. The question word "how" provides this function:

```haskell
"how" -> [ SynSem {
    syn = S :/: (S :\: Deg),
    sem = ty tau (purePP (lam p (lam i (Return (iota (lam d (p @@ d @@ i)))))))
} ]
```

The $\iota$ operator extracts the unique degree satisfying the predicate—implementing the semantics of degree questions.

### Delta rules and semantic computation

PDS applies delta rules to simplify these complex λ-terms. As discussed in [the implementation section](../pds-intro/implementation.html), delta rules enable different semantic computations while preserving semantic equivalence. The formalism is strongly normalizing and confluent, so the order of rule application doesn't affect the final result—a crucial property that ensures our semantic theory remains consistent.

Key delta rules for adjectives include:
- **Arithmetic operations**: Simplifying comparisons like $\ct{(≥)}$ when applied to constants
- **State/index extraction**: Rules for $\ct{height}$, $\ct{d\_tall}$, etc.
- **Beta reduction**: Standard λ-calculus reduction

These rules transform the complex compositional semantics into simpler forms suitable for compilation to Stan. The transformation preserves the semantic content while making it computationally tractable.

### From lambda terms to Stan parameters

The challenge is translating abstract semantic computations into Stan's parameter space. This translation embodies our linking hypothesis between semantic competence and performance. Key transformations:

1. **Degree extraction becomes parameter inference**:
   - $\iota(\lambda d[\ct{height}(i)(j) = d])$ → Infer parameter `height_jo`
   - The unique degree satisfying the equation becomes a parameter to estimate

2. **Functions become data structures**:
   - $\ct{height} : \iota \to e \to r$ → Array `height[person]`
   - Function application → Array indexing

3. **Propositions become probabilities**:
   - Truth values → Real numbers in [0,1]
   - Logical operations → Probabilistic operations

4. **The monad becomes Stan's target**:
   - The $\Do$-notation structures sequential computation:
   ```
   \begin{array}[t]{rl}
   \Do & x ← \ct{return}(5) \\
       & y ← \ct{normal}(x, 1) \\
       & \ct{return}(y)
   \end{array}
   ```
   - This becomes contributions to Stan's log probability

This translation embodies our linking hypothesis: semantic computations generate behavioral data through a noisy measurement process captured by `adjectivesRespond`.

### The PDS kernel model

The PDS system outputs the following kernel model:^[Actual PDS output: `model { w ~ normal(0.0, 1.0); target += normal_lpdf(y | w, sigma); }`]

```stan
model {
  // FIXED EFFECTS
  w ~ normal(0.0, 1.0);
  
  // LIKELIHOOD
  target += normal_lpdf(y | w, sigma);
}
```

This is the semantic core—it captures the essential degree-based semantics where `w` represents the degree on the height scale. But reality is complicated... we need random effects, the ability to model censored data, and proper indexing for multiple items and participants. This gap between the kernel model and a full statistical implementation represents ongoing research: how to get from here (PDS output) to here (actual implementation).

The full model with analyst augmentations looks like:

```stan {.line-numbers highlight="6,13"}
model {
  // PRIORS (analyst-added)
  sigma_epsilon_guess ~ exponential(1);
  sigma_e ~ beta(2, 10);
  
  // FIXED EFFECTS (PDS kernel)
  mu_guess ~ normal(0.0, 1.0);
  
  // RANDOM EFFECTS (analyst-added)
  z_epsilon_guess ~ std_normal();
  
  // LIKELIHOOD (PDS kernel with modifications)
  y[i] ~ normal(mu_guess[item[i]] + epsilon_guess[participant[i]], sigma_e);
}
```

Lines 6 and 13 (highlighted) show the kernel model from PDS. The unhighlighted portions add statistical machinery for real data: hierarchical priors, random effects, and indexed parameters. The kernel captures the core semantic computation—degrees on scales—while the augmentations handle the realities of experimental data.